{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Assignment 2\n",
    "\n",
    "## Instructions\n",
    "- Your submission should be the `.ipynb` file with your name,\n",
    "  like `YusufMesbah.ipynb`. it should include the answers to the questions in\n",
    "  markdown cells.\n",
    "- You are expected to follow the best practices for code writing and model\n",
    "training. Poor coding style will be penalized.\n",
    "- You are allowed to discuss ideas with your peers, but no sharing of code.\n",
    "Plagiarism in the code will result in failing. If you use code from the\n",
    "internet, cite it.\n",
    "- If the instructions seem vague, use common sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Task 1: ANN (30%)\n",
    "For this task, you are required to build a fully connect feed-forward ANN model\n",
    "for a multi-label regression problem.\n",
    "\n",
    "For the given data, you need do proper data preprocessing, design the ANN model,\n",
    "then fine-tune your model architecture (number of layers, number of neurons,\n",
    "activation function, learning rate, momentum, regularization).\n",
    "\n",
    "For evaluating your model, do $80/20$ train test split.\n",
    "\n",
    "### Data\n",
    "You will be working with the data in `Task 1.csv` for predicting students'\n",
    "scores in 3 different exams: math, reading and writing. The columns include:\n",
    " - gender\n",
    " - race\n",
    " - parental level of education\n",
    " - lunch meal plan at school\n",
    " - whether the student undertook the test preparation course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>race/ethnicity</th>\n",
       "      <th>parental level of education</th>\n",
       "      <th>lunch</th>\n",
       "      <th>test preparation course</th>\n",
       "      <th>math score</th>\n",
       "      <th>reading score</th>\n",
       "      <th>writing score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>group A</td>\n",
       "      <td>high school</td>\n",
       "      <td>standard</td>\n",
       "      <td>completed</td>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>group D</td>\n",
       "      <td>some high school</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>40</td>\n",
       "      <td>59</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>male</td>\n",
       "      <td>group E</td>\n",
       "      <td>some college</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>59</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male</td>\n",
       "      <td>group B</td>\n",
       "      <td>high school</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>77</td>\n",
       "      <td>78</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male</td>\n",
       "      <td>group E</td>\n",
       "      <td>associate's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>completed</td>\n",
       "      <td>78</td>\n",
       "      <td>73</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender race/ethnicity parental level of education         lunch  \\\n",
       "0    male        group A                 high school      standard   \n",
       "1  female        group D            some high school  free/reduced   \n",
       "2    male        group E                some college  free/reduced   \n",
       "3    male        group B                 high school      standard   \n",
       "4    male        group E          associate's degree      standard   \n",
       "\n",
       "  test preparation course  math score  reading score  writing score  \n",
       "0               completed          67             67             63  \n",
       "1                    none          40             59             55  \n",
       "2                    none          59             60             50  \n",
       "3                    none          77             78             68  \n",
       "4               completed          78             73             68  "
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read data set\n",
    "data = pd.read_csv('Task 1.csv')\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 ['male' 'female']\n",
      "2 ['standard' 'free/reduced']\n",
      "2 ['completed' 'none']\n",
      "5 ['group A' 'group B' 'group C' 'group D' 'group E']\n",
      "6 ['high school' 'some high school' 'some college' \"associate's degree\"\n",
      " \"bachelor's degree\" \"master's degree\"]\n",
      "\n",
      "Orginized parental level of education:  ['some high school', 'high school', 'some college', \"associate's degree\", \"bachelor's degree\", \"master's degree\"]\n"
     ]
    }
   ],
   "source": [
    "# Check count of distinct values in columns gender, lunch\n",
    "gender_col = data['gender'].unique()\n",
    "print(data['gender'].nunique(), gender_col)\n",
    "\n",
    "lunch_col = data['lunch'].unique()\n",
    "print(data['lunch'].nunique(), lunch_col)\n",
    "\n",
    "course_col = data['test preparation course'].unique()\n",
    "print(data['test preparation course'].nunique(), course_col)\n",
    "\n",
    "race_col = data['race/ethnicity'].unique()\n",
    "race_col.sort()\n",
    "print(data['race/ethnicity'].nunique(), race_col)\n",
    "\n",
    "level_col = data['parental level of education'].unique()\n",
    "print(data['parental level of education'].nunique(), level_col)\n",
    "\n",
    "# Array of orginized parental level of education\n",
    "level_col = ['some high school', 'high school', 'some college', 'associate\\'s degree', 'bachelor\\'s degree', 'master\\'s degree']\n",
    "\n",
    "print('\\nOrginized parental level of education: ', level_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "\n",
    "# Create encoder\n",
    "ordinalEncoder = OrdinalEncoder(categories=[race_col, level_col])\n",
    "oneHotEncoder = OneHotEncoder()\n",
    "\n",
    "ordinal_features = ['race/ethnicity', 'parental level of education']\n",
    "one_hot_features = ['gender', 'lunch', 'test preparation course']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode ordinal features\n",
    "new_oe_features = ordinalEncoder.fit_transform(data[ordinal_features])\n",
    "new_oe_cols = pd.DataFrame(new_oe_features, dtype=int, columns=ordinal_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dale\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Encode one hot features\n",
    "new_ohe_features = oneHotEncoder.fit_transform(data[one_hot_features])\n",
    "new_ohe_cols = pd.DataFrame(new_ohe_features.toarray(), dtype=int, columns=oneHotEncoder.get_feature_names(one_hot_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>math score</th>\n",
       "      <th>reading score</th>\n",
       "      <th>writing score</th>\n",
       "      <th>race/ethnicity</th>\n",
       "      <th>parental level of education</th>\n",
       "      <th>gender_female</th>\n",
       "      <th>gender_male</th>\n",
       "      <th>lunch_free/reduced</th>\n",
       "      <th>lunch_standard</th>\n",
       "      <th>test preparation course_completed</th>\n",
       "      <th>test preparation course_none</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40</td>\n",
       "      <td>59</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77</td>\n",
       "      <td>78</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>78</td>\n",
       "      <td>73</td>\n",
       "      <td>68</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   math score  reading score  writing score  race/ethnicity  \\\n",
       "0          67             67             63               0   \n",
       "1          40             59             55               3   \n",
       "2          59             60             50               4   \n",
       "3          77             78             68               1   \n",
       "4          78             73             68               4   \n",
       "\n",
       "   parental level of education  gender_female  gender_male  \\\n",
       "0                            1              0            1   \n",
       "1                            0              1            0   \n",
       "2                            2              0            1   \n",
       "3                            1              0            1   \n",
       "4                            3              0            1   \n",
       "\n",
       "   lunch_free/reduced  lunch_standard  test preparation course_completed  \\\n",
       "0                   0               1                                  1   \n",
       "1                   1               0                                  0   \n",
       "2                   1               0                                  0   \n",
       "3                   0               1                                  0   \n",
       "4                   0               1                                  1   \n",
       "\n",
       "   test preparation course_none  \n",
       "0                             0  \n",
       "1                             1  \n",
       "2                             1  \n",
       "3                             1  \n",
       "4                             0  "
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine all features\n",
    "new_data = pd.concat([data], axis=1)\n",
    "new_data = new_data.drop(columns=ordinal_features + one_hot_features )\n",
    "new_data = pd.concat([new_data, new_oe_cols, new_ohe_cols], axis=1)\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data to target and features\n",
    "X = new_data.iloc[:, 3:].values\n",
    "y = new_data.iloc[:, :3].values\n",
    "\n",
    "# Split data to training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale testing and training data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Create Data Frames with scaled data\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scale target data\n",
    "# target_scaler = StandardScaler()\n",
    "# target_scaler.fit(y_train)\n",
    "\n",
    "# # Create Data Frames with scaled data\n",
    "# y_train = target_scaler.transform(y_train)\n",
    "# y_test = target_scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super().__init__()\n",
    "        self.y = torch.tensor(y).float()\n",
    "        self.X = torch.tensor(X).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx, :], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "test_dataset = CustomDataset(X_test, y_test)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3])"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, label = next(iter(train_dataloader))\n",
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "ANN(\n",
      "  (hidden1): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (output): Linear(in_features=4, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Custom neural network class\n",
    "class ANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ANN, self).__init__()\n",
    "        self.hidden1 = nn.Linear(8, 4)\n",
    "        self.output = nn.Linear(4, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.hidden1(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Create neural network\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"cuda\" if use_cuda else \"cpu\")\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "model = ANN().to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "lr = 0.001\n",
    "momentum = 0.9\n",
    "seed = 1\n",
    "log_interval = 4\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compare first element in batch with the first target value \n",
    "def print_batch_element_error(batch, target):\n",
    "    # get value from tensor\n",
    "    batch = batch.cpu().data.numpy()\n",
    "    target = target.cpu().data.numpy()\n",
    "    predicted_v = batch[0]\n",
    "    target_v = target[0]\n",
    "    # predicted_v = target_scaler.inverse_transform(batch)\n",
    "    # predicted_v = predicted_v[0]\n",
    "    # target_v = target_scaler.inverse_transform(target)\n",
    "    # target_v = target_v[0]\n",
    "    error = np.abs(predicted_v - target_v)\n",
    "    print('Predicted: ', predicted_v, 'Target: ', target_v, 'Error: ', error, '\\n')\n",
    "\n",
    "\n",
    "def get_accuracy(output, target, threshold=2):\n",
    "    output = output.cpu().data.numpy()\n",
    "    target = target.cpu().data.numpy()\n",
    "    # output = target_scaler.inverse_transform(output)\n",
    "    # target = target_scaler.inverse_transform(target)\n",
    "    accuracy = 0\n",
    "    for i in range(len(output)):\n",
    "        predicted_v = output[i]\n",
    "        target_v = target[i]\n",
    "        error = np.abs(predicted_v - target_v)\n",
    "        for j in range(len(error)):\n",
    "            if error[j] <= threshold:\n",
    "                error[j] = 1\n",
    "            else:\n",
    "                error[j] = 0\n",
    "        accuracy += np.sum(error)/3\n",
    "    \n",
    "    return accuracy/len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "class EarlyStopping():\n",
    "    def __init__(self, tolerance=5, min_delta=0, mode='min'):\n",
    "        self.tolerance = tolerance\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.prev_metric = np.inf if mode == 'min' else -np.inf\n",
    "\n",
    "        self.operation = operator.gt if mode == 'min' else operator.lt\n",
    "\n",
    "    def __call__(self, metric):\n",
    "        delta = (metric - self.prev_metric)\n",
    "\n",
    "        if self.operation(delta, self.min_delta):\n",
    "            self.counter +=1\n",
    "        else:\n",
    "            self.counter = 0\n",
    "            self.prev_metric = metric\n",
    "\n",
    "        if self.counter >= self.tolerance:\n",
    "            self.early_stop = True\n",
    "        return self.early_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, loss_func, accuracy_func = None, acc_treshold = 2, task = 1):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    accuracy = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad() \n",
    "        output = model(data).squeeze()\n",
    "        # print('Output: ', output)\n",
    "        # print('Target: ', target)\n",
    "        # print(output.shape)\n",
    "        # print(target.shape)\n",
    "        loss = loss_func(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        if accuracy_func:\n",
    "            curr_accuracy = accuracy_func(output, target, acc_treshold)\n",
    "            accuracy += curr_accuracy\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                        100. * batch_idx / len(train_loader), loss.item()), end='')\n",
    "            if accuracy_func:\n",
    "                print('\\tAccuracy: {:.6f}'.format(curr_accuracy), end='')\n",
    "            print()\n",
    "\n",
    "    # Print average scores for epoch\n",
    "    if task == 1:\n",
    "        epoch_loss /= len(train_loader.dataset)\n",
    "    elif task == 2:\n",
    "        epoch_loss /= len(train_loader)\n",
    "\n",
    "    print('\\nTrain Epoch: Average Loss: {:.6f}'.format(epoch_loss), end='')\n",
    "    \n",
    "    if accuracy_func:\n",
    "        print('\\tAccuracy: {:.6f}'.format(accuracy/len(train_loader)), end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, loss_func, accuracy_func, acc_treshold = 2, task = 1):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data).squeeze()\n",
    "            test_loss += loss_func(output, target).item()\n",
    "            accuracy += accuracy_func(output, target, acc_treshold)\n",
    "\n",
    "    if task == 1:\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "    elif task == 2:\n",
    "        test_loss /= len(test_loader)\n",
    "    \n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f} Average accuracy: {:.4f}\\n'.format(\n",
    "        test_loss, accuracy / len(test_loader)))\n",
    "\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/800 (0%)]\tLoss: 4780.580078\tAccuracy: 0.000000\n",
      "Train Epoch: 1 [128/800 (16%)]\tLoss: 5859.499023\tAccuracy: 0.000000\n",
      "Train Epoch: 1 [256/800 (32%)]\tLoss: 4628.923340\tAccuracy: 0.000000\n",
      "Train Epoch: 1 [384/800 (48%)]\tLoss: 4451.016113\tAccuracy: 0.000000\n",
      "Train Epoch: 1 [512/800 (64%)]\tLoss: 3332.394043\tAccuracy: 0.000000\n",
      "Train Epoch: 1 [640/800 (80%)]\tLoss: 1704.239258\tAccuracy: 0.010417\n",
      "Train Epoch: 1 [768/800 (96%)]\tLoss: 1093.289795\tAccuracy: 0.031250\n",
      "\n",
      "Train Epoch: Average Loss: 112.839182\tAccuracy: 0.010417\n",
      "\n",
      "Test set: Average loss: 58.5771 Average accuracy: 0.0253\n",
      "\n",
      "Train Epoch: 2 [0/800 (0%)]\tLoss: 1646.183105\tAccuracy: 0.052083\n",
      "Train Epoch: 2 [128/800 (16%)]\tLoss: 209.868866\tAccuracy: 0.104167\n",
      "Train Epoch: 2 [256/800 (32%)]\tLoss: 746.114685\tAccuracy: 0.052083\n",
      "Train Epoch: 2 [384/800 (48%)]\tLoss: 625.274170\tAccuracy: 0.052083\n",
      "Train Epoch: 2 [512/800 (64%)]\tLoss: 229.813080\tAccuracy: 0.114583\n",
      "Train Epoch: 2 [640/800 (80%)]\tLoss: 221.668518\tAccuracy: 0.125000\n",
      "Train Epoch: 2 [768/800 (96%)]\tLoss: 279.110291\tAccuracy: 0.104167\n",
      "\n",
      "Train Epoch: Average Loss: 17.629777\tAccuracy: 0.070833\n",
      "\n",
      "Test set: Average loss: 10.3607 Average accuracy: 0.0893\n",
      "\n",
      "Train Epoch: 3 [0/800 (0%)]\tLoss: 299.990875\tAccuracy: 0.083333\n",
      "Train Epoch: 3 [128/800 (16%)]\tLoss: 245.720245\tAccuracy: 0.041667\n",
      "Train Epoch: 3 [256/800 (32%)]\tLoss: 142.344101\tAccuracy: 0.218750\n",
      "Train Epoch: 3 [384/800 (48%)]\tLoss: 231.308807\tAccuracy: 0.104167\n",
      "Train Epoch: 3 [512/800 (64%)]\tLoss: 173.054260\tAccuracy: 0.114583\n",
      "Train Epoch: 3 [640/800 (80%)]\tLoss: 146.244476\tAccuracy: 0.072917\n",
      "Train Epoch: 3 [768/800 (96%)]\tLoss: 181.249908\tAccuracy: 0.083333\n",
      "\n",
      "Train Epoch: Average Loss: 6.666994\tAccuracy: 0.109167\n",
      "\n",
      "Test set: Average loss: 8.9185 Average accuracy: 0.1057\n",
      "\n",
      "Train Epoch: 4 [0/800 (0%)]\tLoss: 130.545273\tAccuracy: 0.093750\n",
      "Train Epoch: 4 [128/800 (16%)]\tLoss: 156.650772\tAccuracy: 0.104167\n",
      "Train Epoch: 4 [256/800 (32%)]\tLoss: 238.903351\tAccuracy: 0.072917\n",
      "Train Epoch: 4 [384/800 (48%)]\tLoss: 119.867699\tAccuracy: 0.166667\n",
      "Train Epoch: 4 [512/800 (64%)]\tLoss: 194.331955\tAccuracy: 0.083333\n",
      "Train Epoch: 4 [640/800 (80%)]\tLoss: 193.897079\tAccuracy: 0.114583\n",
      "Train Epoch: 4 [768/800 (96%)]\tLoss: 119.744980\tAccuracy: 0.072917\n",
      "\n",
      "Train Epoch: Average Loss: 5.260499\tAccuracy: 0.121250\n",
      "\n",
      "Test set: Average loss: 6.8760 Average accuracy: 0.1101\n",
      "\n",
      "Train Epoch: 5 [0/800 (0%)]\tLoss: 198.834930\tAccuracy: 0.114583\n",
      "Train Epoch: 5 [128/800 (16%)]\tLoss: 160.515091\tAccuracy: 0.093750\n",
      "Train Epoch: 5 [256/800 (32%)]\tLoss: 173.368820\tAccuracy: 0.093750\n",
      "Train Epoch: 5 [384/800 (48%)]\tLoss: 229.944672\tAccuracy: 0.072917\n",
      "Train Epoch: 5 [512/800 (64%)]\tLoss: 163.299927\tAccuracy: 0.177083\n",
      "Train Epoch: 5 [640/800 (80%)]\tLoss: 194.658905\tAccuracy: 0.135417\n",
      "Train Epoch: 5 [768/800 (96%)]\tLoss: 125.026062\tAccuracy: 0.093750\n",
      "\n",
      "Train Epoch: Average Loss: 5.405345\tAccuracy: 0.126667\n",
      "\n",
      "Test set: Average loss: 6.8681 Average accuracy: 0.1116\n",
      "\n",
      "Train Epoch: 6 [0/800 (0%)]\tLoss: 126.628662\tAccuracy: 0.218750\n",
      "Train Epoch: 6 [128/800 (16%)]\tLoss: 100.365616\tAccuracy: 0.166667\n",
      "Train Epoch: 6 [256/800 (32%)]\tLoss: 101.096710\tAccuracy: 0.208333\n",
      "Train Epoch: 6 [384/800 (48%)]\tLoss: 216.777344\tAccuracy: 0.093750\n",
      "Train Epoch: 6 [512/800 (64%)]\tLoss: 169.563721\tAccuracy: 0.135417\n",
      "Train Epoch: 6 [640/800 (80%)]\tLoss: 148.284836\tAccuracy: 0.125000\n",
      "Train Epoch: 6 [768/800 (96%)]\tLoss: 174.152557\tAccuracy: 0.145833\n",
      "\n",
      "Train Epoch: Average Loss: 5.044439\tAccuracy: 0.120417\n",
      "\n",
      "Test set: Average loss: 6.6807 Average accuracy: 0.1235\n",
      "\n",
      "Train Epoch: 7 [0/800 (0%)]\tLoss: 195.622559\tAccuracy: 0.125000\n",
      "Train Epoch: 7 [128/800 (16%)]\tLoss: 202.999847\tAccuracy: 0.052083\n",
      "Train Epoch: 7 [256/800 (32%)]\tLoss: 216.117279\tAccuracy: 0.135417\n",
      "Train Epoch: 7 [384/800 (48%)]\tLoss: 157.723526\tAccuracy: 0.072917\n",
      "Train Epoch: 7 [512/800 (64%)]\tLoss: 136.582214\tAccuracy: 0.104167\n",
      "Train Epoch: 7 [640/800 (80%)]\tLoss: 151.593719\tAccuracy: 0.135417\n",
      "Train Epoch: 7 [768/800 (96%)]\tLoss: 199.510345\tAccuracy: 0.083333\n",
      "\n",
      "Train Epoch: Average Loss: 5.321265\tAccuracy: 0.109167\n",
      "\n",
      "Test set: Average loss: 6.8116 Average accuracy: 0.1027\n",
      "\n",
      "Train Epoch: 8 [0/800 (0%)]\tLoss: 115.498795\tAccuracy: 0.145833\n",
      "Train Epoch: 8 [128/800 (16%)]\tLoss: 165.809830\tAccuracy: 0.041667\n",
      "Train Epoch: 8 [256/800 (32%)]\tLoss: 170.804535\tAccuracy: 0.104167\n",
      "Train Epoch: 8 [384/800 (48%)]\tLoss: 207.226135\tAccuracy: 0.125000\n",
      "Train Epoch: 8 [512/800 (64%)]\tLoss: 266.024078\tAccuracy: 0.072917\n",
      "Train Epoch: 8 [640/800 (80%)]\tLoss: 175.944092\tAccuracy: 0.135417\n",
      "Train Epoch: 8 [768/800 (96%)]\tLoss: 133.116226\tAccuracy: 0.145833\n",
      "\n",
      "Train Epoch: Average Loss: 5.141749\tAccuracy: 0.115000\n",
      "\n",
      "Test set: Average loss: 7.0592 Average accuracy: 0.1429\n",
      "\n",
      "Train Epoch: 9 [0/800 (0%)]\tLoss: 149.403366\tAccuracy: 0.083333\n",
      "Train Epoch: 9 [128/800 (16%)]\tLoss: 191.164108\tAccuracy: 0.187500\n",
      "Train Epoch: 9 [256/800 (32%)]\tLoss: 158.578522\tAccuracy: 0.104167\n",
      "Train Epoch: 9 [384/800 (48%)]\tLoss: 165.002716\tAccuracy: 0.093750\n",
      "Train Epoch: 9 [512/800 (64%)]\tLoss: 126.731285\tAccuracy: 0.166667\n",
      "Train Epoch: 9 [640/800 (80%)]\tLoss: 170.081055\tAccuracy: 0.083333\n",
      "Train Epoch: 9 [768/800 (96%)]\tLoss: 169.676758\tAccuracy: 0.187500\n",
      "\n",
      "Train Epoch: Average Loss: 5.102285\tAccuracy: 0.130833\n",
      "\n",
      "Test set: Average loss: 8.3284 Average accuracy: 0.0997\n",
      "\n",
      "Train Epoch: 10 [0/800 (0%)]\tLoss: 145.673309\tAccuracy: 0.125000\n",
      "Train Epoch: 10 [128/800 (16%)]\tLoss: 178.821472\tAccuracy: 0.093750\n",
      "Train Epoch: 10 [256/800 (32%)]\tLoss: 207.537628\tAccuracy: 0.197917\n",
      "Train Epoch: 10 [384/800 (48%)]\tLoss: 177.278809\tAccuracy: 0.125000\n",
      "Train Epoch: 10 [512/800 (64%)]\tLoss: 201.114655\tAccuracy: 0.093750\n",
      "Train Epoch: 10 [640/800 (80%)]\tLoss: 126.100601\tAccuracy: 0.166667\n",
      "Train Epoch: 10 [768/800 (96%)]\tLoss: 132.524902\tAccuracy: 0.135417\n",
      "\n",
      "Train Epoch: Average Loss: 5.371126\tAccuracy: 0.105833\n",
      "\n",
      "Test set: Average loss: 8.0251 Average accuracy: 0.1190\n",
      "\n",
      "Train Epoch: 11 [0/800 (0%)]\tLoss: 144.825256\tAccuracy: 0.135417\n",
      "Train Epoch: 11 [128/800 (16%)]\tLoss: 163.672241\tAccuracy: 0.083333\n",
      "Train Epoch: 11 [256/800 (32%)]\tLoss: 149.051575\tAccuracy: 0.072917\n",
      "Train Epoch: 11 [384/800 (48%)]\tLoss: 190.871429\tAccuracy: 0.135417\n",
      "Train Epoch: 11 [512/800 (64%)]\tLoss: 213.229233\tAccuracy: 0.104167\n",
      "Train Epoch: 11 [640/800 (80%)]\tLoss: 134.504852\tAccuracy: 0.062500\n",
      "Train Epoch: 11 [768/800 (96%)]\tLoss: 235.858002\tAccuracy: 0.104167\n",
      "\n",
      "Train Epoch: Average Loss: 5.311164\tAccuracy: 0.114583\n",
      "\n",
      "Test set: Average loss: 6.5886 Average accuracy: 0.1205\n",
      "\n",
      "Train Epoch: 12 [0/800 (0%)]\tLoss: 203.555786\tAccuracy: 0.072917\n",
      "Train Epoch: 12 [128/800 (16%)]\tLoss: 137.497620\tAccuracy: 0.145833\n",
      "Train Epoch: 12 [256/800 (32%)]\tLoss: 159.935181\tAccuracy: 0.114583\n",
      "Train Epoch: 12 [384/800 (48%)]\tLoss: 176.526215\tAccuracy: 0.083333\n",
      "Train Epoch: 12 [512/800 (64%)]\tLoss: 135.836075\tAccuracy: 0.145833\n",
      "Train Epoch: 12 [640/800 (80%)]\tLoss: 163.009476\tAccuracy: 0.125000\n",
      "Train Epoch: 12 [768/800 (96%)]\tLoss: 154.244232\tAccuracy: 0.125000\n",
      "\n",
      "Train Epoch: Average Loss: 5.087497\tAccuracy: 0.123333\n",
      "\n",
      "Test set: Average loss: 6.8312 Average accuracy: 0.1265\n",
      "\n",
      "Train Epoch: 13 [0/800 (0%)]\tLoss: 132.042465\tAccuracy: 0.083333\n",
      "Train Epoch: 13 [128/800 (16%)]\tLoss: 154.292099\tAccuracy: 0.156250\n",
      "Train Epoch: 13 [256/800 (32%)]\tLoss: 132.581299\tAccuracy: 0.177083\n",
      "Train Epoch: 13 [384/800 (48%)]\tLoss: 210.838379\tAccuracy: 0.166667\n",
      "Train Epoch: 13 [512/800 (64%)]\tLoss: 203.359055\tAccuracy: 0.104167\n",
      "Train Epoch: 13 [640/800 (80%)]\tLoss: 150.685974\tAccuracy: 0.093750\n",
      "Train Epoch: 13 [768/800 (96%)]\tLoss: 278.783966\tAccuracy: 0.072917\n",
      "\n",
      "Train Epoch: Average Loss: 5.044245\tAccuracy: 0.117500\n",
      "\n",
      "Test set: Average loss: 6.9070 Average accuracy: 0.1146\n",
      "\n",
      "Train Epoch: 14 [0/800 (0%)]\tLoss: 107.571831\tAccuracy: 0.135417\n",
      "Train Epoch: 14 [128/800 (16%)]\tLoss: 210.784790\tAccuracy: 0.062500\n",
      "Train Epoch: 14 [256/800 (32%)]\tLoss: 169.928497\tAccuracy: 0.114583\n",
      "Train Epoch: 14 [384/800 (48%)]\tLoss: 197.973602\tAccuracy: 0.104167\n",
      "Train Epoch: 14 [512/800 (64%)]\tLoss: 149.295242\tAccuracy: 0.177083\n",
      "Train Epoch: 14 [640/800 (80%)]\tLoss: 193.591187\tAccuracy: 0.052083\n",
      "Train Epoch: 14 [768/800 (96%)]\tLoss: 174.249359\tAccuracy: 0.104167\n",
      "\n",
      "Train Epoch: Average Loss: 5.025326\tAccuracy: 0.110000\n",
      "\n",
      "Test set: Average loss: 7.1007 Average accuracy: 0.1161\n",
      "\n",
      "Train Epoch: 15 [0/800 (0%)]\tLoss: 153.012436\tAccuracy: 0.135417\n",
      "Train Epoch: 15 [128/800 (16%)]\tLoss: 173.027069\tAccuracy: 0.166667\n",
      "Train Epoch: 15 [256/800 (32%)]\tLoss: 126.328369\tAccuracy: 0.125000\n",
      "Train Epoch: 15 [384/800 (48%)]\tLoss: 118.345261\tAccuracy: 0.104167\n",
      "Train Epoch: 15 [512/800 (64%)]\tLoss: 100.582603\tAccuracy: 0.208333\n",
      "Train Epoch: 15 [640/800 (80%)]\tLoss: 198.522064\tAccuracy: 0.125000\n",
      "Train Epoch: 15 [768/800 (96%)]\tLoss: 163.856552\tAccuracy: 0.145833\n",
      "\n",
      "Train Epoch: Average Loss: 5.095920\tAccuracy: 0.118750\n",
      "\n",
      "Test set: Average loss: 6.9261 Average accuracy: 0.1101\n",
      "\n",
      "Train Epoch: 16 [0/800 (0%)]\tLoss: 119.238762\tAccuracy: 0.125000\n",
      "Train Epoch: 16 [128/800 (16%)]\tLoss: 223.635605\tAccuracy: 0.166667\n",
      "Train Epoch: 16 [256/800 (32%)]\tLoss: 121.591484\tAccuracy: 0.093750\n",
      "Train Epoch: 16 [384/800 (48%)]\tLoss: 139.413315\tAccuracy: 0.093750\n",
      "Train Epoch: 16 [512/800 (64%)]\tLoss: 170.126816\tAccuracy: 0.072917\n",
      "Train Epoch: 16 [640/800 (80%)]\tLoss: 171.296310\tAccuracy: 0.187500\n",
      "Train Epoch: 16 [768/800 (96%)]\tLoss: 140.302124\tAccuracy: 0.177083\n",
      "\n",
      "Train Epoch: Average Loss: 5.115637\tAccuracy: 0.122500\n",
      "\n",
      "Test set: Average loss: 6.5804 Average accuracy: 0.1339\n",
      "\n",
      "Train Epoch: 17 [0/800 (0%)]\tLoss: 153.880356\tAccuracy: 0.177083\n",
      "Train Epoch: 17 [128/800 (16%)]\tLoss: 120.097351\tAccuracy: 0.239583\n",
      "Train Epoch: 17 [256/800 (32%)]\tLoss: 192.119843\tAccuracy: 0.125000\n",
      "Train Epoch: 17 [384/800 (48%)]\tLoss: 184.563461\tAccuracy: 0.135417\n",
      "Train Epoch: 17 [512/800 (64%)]\tLoss: 161.487885\tAccuracy: 0.104167\n",
      "Train Epoch: 17 [640/800 (80%)]\tLoss: 221.418091\tAccuracy: 0.156250\n",
      "Train Epoch: 17 [768/800 (96%)]\tLoss: 213.243942\tAccuracy: 0.104167\n",
      "\n",
      "Train Epoch: Average Loss: 5.398583\tAccuracy: 0.125833\n",
      "\n",
      "Test set: Average loss: 6.7173 Average accuracy: 0.1101\n",
      "\n",
      "Train Epoch: 18 [0/800 (0%)]\tLoss: 212.613388\tAccuracy: 0.083333\n",
      "Train Epoch: 18 [128/800 (16%)]\tLoss: 149.126587\tAccuracy: 0.104167\n",
      "Train Epoch: 18 [256/800 (32%)]\tLoss: 128.986694\tAccuracy: 0.083333\n",
      "Train Epoch: 18 [384/800 (48%)]\tLoss: 128.033325\tAccuracy: 0.187500\n",
      "Train Epoch: 18 [512/800 (64%)]\tLoss: 174.937225\tAccuracy: 0.093750\n",
      "Train Epoch: 18 [640/800 (80%)]\tLoss: 147.360352\tAccuracy: 0.114583\n",
      "Train Epoch: 18 [768/800 (96%)]\tLoss: 138.972443\tAccuracy: 0.104167\n",
      "\n",
      "Train Epoch: Average Loss: 5.351329\tAccuracy: 0.113750\n",
      "\n",
      "Test set: Average loss: 7.2306 Average accuracy: 0.1190\n",
      "\n",
      "Train Epoch: 19 [0/800 (0%)]\tLoss: 107.349174\tAccuracy: 0.177083\n",
      "Train Epoch: 19 [128/800 (16%)]\tLoss: 178.316010\tAccuracy: 0.093750\n",
      "Train Epoch: 19 [256/800 (32%)]\tLoss: 146.703522\tAccuracy: 0.093750\n",
      "Train Epoch: 19 [384/800 (48%)]\tLoss: 131.528320\tAccuracy: 0.114583\n",
      "Train Epoch: 19 [512/800 (64%)]\tLoss: 161.635880\tAccuracy: 0.093750\n",
      "Train Epoch: 19 [640/800 (80%)]\tLoss: 157.504791\tAccuracy: 0.062500\n",
      "Train Epoch: 19 [768/800 (96%)]\tLoss: 119.640610\tAccuracy: 0.156250\n",
      "\n",
      "Train Epoch: Average Loss: 5.068130\tAccuracy: 0.117917\n",
      "\n",
      "Test set: Average loss: 6.4389 Average accuracy: 0.1354\n",
      "\n",
      "Train Epoch: 20 [0/800 (0%)]\tLoss: 126.295624\tAccuracy: 0.125000\n",
      "Train Epoch: 20 [128/800 (16%)]\tLoss: 154.254776\tAccuracy: 0.083333\n",
      "Train Epoch: 20 [256/800 (32%)]\tLoss: 180.747437\tAccuracy: 0.125000\n",
      "Train Epoch: 20 [384/800 (48%)]\tLoss: 158.406311\tAccuracy: 0.135417\n",
      "Train Epoch: 20 [512/800 (64%)]\tLoss: 164.954025\tAccuracy: 0.145833\n",
      "Train Epoch: 20 [640/800 (80%)]\tLoss: 160.725159\tAccuracy: 0.156250\n",
      "Train Epoch: 20 [768/800 (96%)]\tLoss: 204.463318\tAccuracy: 0.114583\n",
      "\n",
      "Train Epoch: Average Loss: 5.058627\tAccuracy: 0.122500\n",
      "\n",
      "Test set: Average loss: 6.6836 Average accuracy: 0.1086\n",
      "\n",
      "Train Epoch: 21 [0/800 (0%)]\tLoss: 121.505020\tAccuracy: 0.114583\n",
      "Train Epoch: 21 [128/800 (16%)]\tLoss: 154.677353\tAccuracy: 0.135417\n",
      "Train Epoch: 21 [256/800 (32%)]\tLoss: 131.692657\tAccuracy: 0.104167\n",
      "Train Epoch: 21 [384/800 (48%)]\tLoss: 185.067657\tAccuracy: 0.145833\n",
      "Train Epoch: 21 [512/800 (64%)]\tLoss: 202.846039\tAccuracy: 0.093750\n",
      "Train Epoch: 21 [640/800 (80%)]\tLoss: 162.973175\tAccuracy: 0.166667\n",
      "Train Epoch: 21 [768/800 (96%)]\tLoss: 164.327133\tAccuracy: 0.135417\n",
      "\n",
      "Train Epoch: Average Loss: 4.897277\tAccuracy: 0.123333\n",
      "\n",
      "Test set: Average loss: 6.8398 Average accuracy: 0.1220\n",
      "\n",
      "Train Epoch: 22 [0/800 (0%)]\tLoss: 139.847076\tAccuracy: 0.114583\n",
      "Train Epoch: 22 [128/800 (16%)]\tLoss: 138.965088\tAccuracy: 0.125000\n",
      "Train Epoch: 22 [256/800 (32%)]\tLoss: 143.422882\tAccuracy: 0.197917\n",
      "Train Epoch: 22 [384/800 (48%)]\tLoss: 138.434296\tAccuracy: 0.156250\n",
      "Train Epoch: 22 [512/800 (64%)]\tLoss: 215.571259\tAccuracy: 0.166667\n",
      "Train Epoch: 22 [640/800 (80%)]\tLoss: 183.333176\tAccuracy: 0.229167\n",
      "Train Epoch: 22 [768/800 (96%)]\tLoss: 183.222275\tAccuracy: 0.093750\n",
      "\n",
      "Train Epoch: Average Loss: 5.274681\tAccuracy: 0.127500\n",
      "\n",
      "Test set: Average loss: 6.5184 Average accuracy: 0.1086\n",
      "\n",
      "Train Epoch: 23 [0/800 (0%)]\tLoss: 170.071335\tAccuracy: 0.104167\n",
      "Train Epoch: 23 [128/800 (16%)]\tLoss: 152.804077\tAccuracy: 0.156250\n",
      "Train Epoch: 23 [256/800 (32%)]\tLoss: 158.230469\tAccuracy: 0.062500\n",
      "Train Epoch: 23 [384/800 (48%)]\tLoss: 169.114349\tAccuracy: 0.072917\n",
      "Train Epoch: 23 [512/800 (64%)]\tLoss: 191.718674\tAccuracy: 0.072917\n",
      "Train Epoch: 23 [640/800 (80%)]\tLoss: 195.751144\tAccuracy: 0.125000\n",
      "Train Epoch: 23 [768/800 (96%)]\tLoss: 181.663376\tAccuracy: 0.083333\n",
      "\n",
      "Train Epoch: Average Loss: 5.117447\tAccuracy: 0.111250\n",
      "\n",
      "Test set: Average loss: 6.9734 Average accuracy: 0.1161\n",
      "\n",
      "Train Epoch: 24 [0/800 (0%)]\tLoss: 139.784409\tAccuracy: 0.114583\n",
      "Train Epoch: 24 [128/800 (16%)]\tLoss: 128.199951\tAccuracy: 0.114583\n",
      "Train Epoch: 24 [256/800 (32%)]\tLoss: 172.745956\tAccuracy: 0.145833\n",
      "Train Epoch: 24 [384/800 (48%)]\tLoss: 183.627686\tAccuracy: 0.125000\n",
      "Train Epoch: 24 [512/800 (64%)]\tLoss: 185.284027\tAccuracy: 0.177083\n",
      "Train Epoch: 24 [640/800 (80%)]\tLoss: 156.909988\tAccuracy: 0.177083\n",
      "Train Epoch: 24 [768/800 (96%)]\tLoss: 164.188828\tAccuracy: 0.125000\n",
      "\n",
      "Train Epoch: Average Loss: 5.270596\tAccuracy: 0.121250\n",
      "\n",
      "Test set: Average loss: 6.7369 Average accuracy: 0.1265\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "early_stopping = EarlyStopping()\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_dataloader, optimizer, epoch, loss_fn, get_accuracy, 2)\n",
    "    test_loss = test(model, device, test_dataloader, loss_fn, get_accuracy, 2)\n",
    "\n",
    "    if early_stopping(test_loss):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Questions\n",
    "1. What preprocessing techniques did you use? Why?\n",
    "    - *Answer*\n",
    "2. Describe the fine-tuning process and how you reached your model architecture.\n",
    "    - *Answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Task 2: CNN (40%)\n",
    "For this task, you will be doing image classification:\n",
    "- First, adapt your best model from Task 1 to work on this task, and\n",
    "fit it on the new data. Then, evaluate its performance.\n",
    "- After that, build a CNN model for image classification.\n",
    "- Compare both models in terms of accuracy, number of parameters and speed of\n",
    "inference (the time the model takes to predict 50 samples).\n",
    "\n",
    "For the given data, you need to do proper data preprocessing and augmentation,\n",
    "data loaders.\n",
    "Then fine-tune your model architecture (number of layers, number of filters,\n",
    "activation function, learning rate, momentum, regularization).\n",
    "\n",
    "### Data\n",
    "You will be working with the data in `triple_mnist.zip` for predicting 3-digit\n",
    "numbers writen in the image. Each image contains 3 digits similar to the\n",
    "following example (whose label is `039`):\n",
    "\n",
    "![example](https://github.com/shaohua0116/MultiDigitMNIST/blob/master/asset/examples/039/0_039.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "test_path = './triple_mnist/triple_mnist/test/'\n",
    "train_path = './triple_mnist/triple_mnist/train/'\n",
    "val_path = './triple_mnist/triple_mnist/val/'\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(84),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_data = ImageFolder(root=train_path, transform=transform)\n",
    "\n",
    "test_data = ImageFolder(root=test_path, transform=transform)\n",
    "\n",
    "val_data = ImageFolder(root=val_path, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_data_targets(data):\n",
    "    classes = data.classes\n",
    "    for i in range(len(data.samples)):\n",
    "        path, index = data.samples[i]\n",
    "        cl = classes[index]\n",
    "        arr = np.zeros((3,), dtype=np.int64)\n",
    "        for j in range(len(cl)):\n",
    "            arr[j] = int(cl[j])\n",
    "        new_sample = (path, arr)\n",
    "        data.samples[i] = new_sample\n",
    "        data.targets[i] = arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_data_targets(train_data)\n",
    "fix_data_targets(test_data)\n",
    "fix_data_targets(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False) \n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 84, 84]) 7056 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x169b3dc4910>"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "in_channels = images.shape[1]\n",
    "images_shape = np.prod(images.shape[1:])\n",
    "\n",
    "print(images.shape, images_shape, in_channels)\n",
    "plt.imshow(np.transpose(images[0].cpu().detach().numpy(), (1, 2, 0)), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN_2(ANN):\n",
    "    def __init__(self):\n",
    "        super(ANN_2, self).__init__()\n",
    "        self.hidden1 = nn.Linear(images_shape, 2500)\n",
    "        self.output = nn.Linear(2500, 30)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, images_shape)\n",
    "        x = F.leaky_relu(self.hidden1(x))\n",
    "        x = self.output(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        # x = F.sigmoid(x)\n",
    "        x = x.view(-1, 3, 10)\n",
    "        x = x.transpose(1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN_2(\n",
      "  (hidden1): Linear(in_features=7056, out_features=2500, bias=True)\n",
      "  (output): Linear(in_features=2500, out_features=30, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "model = ANN_2().to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "lr = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "# loss_fn = nn.NLLLoss()\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task2_accuracy(output, target, treshold):\n",
    "    output = output.cpu().detach().numpy()\n",
    "    target = target.cpu().detach().numpy()\n",
    "\n",
    "    accuracy = 0\n",
    "\n",
    "    for i in range(len(output)):\n",
    "        out = np.transpose(output[i])\n",
    "        tar = target[i]\n",
    "\n",
    "        # print(out)\n",
    "        # print(tar)\n",
    "\n",
    "        num1 = out[0].argmax()\n",
    "        num2 = out[1].argmax()\n",
    "        num3 = out[2].argmax()\n",
    "\n",
    "        # print(num1, num2, num3)\n",
    "\n",
    "        if num1 == tar[0] and num2 == tar[1] and num3 == tar[2]:\n",
    "            accuracy += 1\n",
    "    \n",
    "    return accuracy / len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/64000 (0%)]\tLoss: 2.301415\n",
      "Train Epoch: 1 [640/64000 (1%)]\tLoss: 2.299937\n",
      "Train Epoch: 1 [1280/64000 (2%)]\tLoss: 2.306798\n",
      "Train Epoch: 1 [1920/64000 (3%)]\tLoss: 2.303297\n",
      "Train Epoch: 1 [2560/64000 (4%)]\tLoss: 2.300428\n",
      "Train Epoch: 1 [3200/64000 (5%)]\tLoss: 2.296822\n",
      "Train Epoch: 1 [3840/64000 (6%)]\tLoss: 2.294634\n",
      "Train Epoch: 1 [4480/64000 (7%)]\tLoss: 2.295011\n",
      "Train Epoch: 1 [5120/64000 (8%)]\tLoss: 2.286525\n",
      "Train Epoch: 1 [5760/64000 (9%)]\tLoss: 2.293885\n",
      "Train Epoch: 1 [6400/64000 (10%)]\tLoss: 2.302762\n",
      "Train Epoch: 1 [7040/64000 (11%)]\tLoss: 2.302186\n",
      "Train Epoch: 1 [7680/64000 (12%)]\tLoss: 2.291246\n",
      "Train Epoch: 1 [8320/64000 (13%)]\tLoss: 2.286270\n",
      "Train Epoch: 1 [8960/64000 (14%)]\tLoss: 2.289138\n",
      "Train Epoch: 1 [9600/64000 (15%)]\tLoss: 2.286181\n",
      "Train Epoch: 1 [10240/64000 (16%)]\tLoss: 2.285983\n",
      "Train Epoch: 1 [10880/64000 (17%)]\tLoss: 2.299311\n",
      "Train Epoch: 1 [11520/64000 (18%)]\tLoss: 2.291000\n",
      "Train Epoch: 1 [12160/64000 (19%)]\tLoss: 2.284649\n",
      "Train Epoch: 1 [12800/64000 (20%)]\tLoss: 2.288501\n",
      "Train Epoch: 1 [13440/64000 (21%)]\tLoss: 2.282487\n",
      "Train Epoch: 1 [14080/64000 (22%)]\tLoss: 2.278463\n",
      "Train Epoch: 1 [14720/64000 (23%)]\tLoss: 2.280036\n",
      "Train Epoch: 1 [15360/64000 (24%)]\tLoss: 2.276021\n",
      "Train Epoch: 1 [16000/64000 (25%)]\tLoss: 2.282393\n",
      "Train Epoch: 1 [16640/64000 (26%)]\tLoss: 2.275115\n",
      "Train Epoch: 1 [17280/64000 (27%)]\tLoss: 2.278478\n",
      "Train Epoch: 1 [17920/64000 (28%)]\tLoss: 2.277150\n",
      "Train Epoch: 1 [18560/64000 (29%)]\tLoss: 2.276987\n",
      "Train Epoch: 1 [19200/64000 (30%)]\tLoss: 2.273106\n",
      "Train Epoch: 1 [19840/64000 (31%)]\tLoss: 2.284350\n",
      "Train Epoch: 1 [20480/64000 (32%)]\tLoss: 2.263951\n",
      "Train Epoch: 1 [21120/64000 (33%)]\tLoss: 2.271418\n",
      "Train Epoch: 1 [21760/64000 (34%)]\tLoss: 2.280251\n",
      "Train Epoch: 1 [22400/64000 (35%)]\tLoss: 2.273487\n",
      "Train Epoch: 1 [23040/64000 (36%)]\tLoss: 2.279326\n",
      "Train Epoch: 1 [23680/64000 (37%)]\tLoss: 2.268229\n",
      "Train Epoch: 1 [24320/64000 (38%)]\tLoss: 2.259366\n",
      "Train Epoch: 1 [24960/64000 (39%)]\tLoss: 2.269450\n",
      "Train Epoch: 1 [25600/64000 (40%)]\tLoss: 2.278221\n",
      "Train Epoch: 1 [26240/64000 (41%)]\tLoss: 2.263020\n",
      "Train Epoch: 1 [26880/64000 (42%)]\tLoss: 2.257485\n",
      "Train Epoch: 1 [27520/64000 (43%)]\tLoss: 2.260419\n",
      "Train Epoch: 1 [28160/64000 (44%)]\tLoss: 2.259872\n",
      "Train Epoch: 1 [28800/64000 (45%)]\tLoss: 2.240235\n",
      "Train Epoch: 1 [29440/64000 (46%)]\tLoss: 2.258137\n",
      "Train Epoch: 1 [30080/64000 (47%)]\tLoss: 2.265354\n",
      "Train Epoch: 1 [30720/64000 (48%)]\tLoss: 2.247326\n",
      "Train Epoch: 1 [31360/64000 (49%)]\tLoss: 2.245504\n",
      "Train Epoch: 1 [32000/64000 (50%)]\tLoss: 2.260674\n",
      "Train Epoch: 1 [32640/64000 (51%)]\tLoss: 2.264647\n",
      "Train Epoch: 1 [33280/64000 (52%)]\tLoss: 2.249738\n",
      "Train Epoch: 1 [33920/64000 (53%)]\tLoss: 2.245102\n",
      "Train Epoch: 1 [34560/64000 (54%)]\tLoss: 2.239082\n",
      "Train Epoch: 1 [35200/64000 (55%)]\tLoss: 2.251187\n",
      "Train Epoch: 1 [35840/64000 (56%)]\tLoss: 2.235524\n",
      "Train Epoch: 1 [36480/64000 (57%)]\tLoss: 2.241112\n",
      "Train Epoch: 1 [37120/64000 (58%)]\tLoss: 2.248435\n",
      "Train Epoch: 1 [37760/64000 (59%)]\tLoss: 2.241689\n",
      "Train Epoch: 1 [38400/64000 (60%)]\tLoss: 2.243642\n",
      "Train Epoch: 1 [39040/64000 (61%)]\tLoss: 2.249793\n",
      "Train Epoch: 1 [39680/64000 (62%)]\tLoss: 2.234253\n",
      "Train Epoch: 1 [40320/64000 (63%)]\tLoss: 2.245184\n",
      "Train Epoch: 1 [40960/64000 (64%)]\tLoss: 2.239865\n",
      "Train Epoch: 1 [41600/64000 (65%)]\tLoss: 2.234129\n",
      "Train Epoch: 1 [42240/64000 (66%)]\tLoss: 2.232099\n",
      "Train Epoch: 1 [42880/64000 (67%)]\tLoss: 2.215834\n",
      "Train Epoch: 1 [43520/64000 (68%)]\tLoss: 2.236768\n",
      "Train Epoch: 1 [44160/64000 (69%)]\tLoss: 2.221602\n",
      "Train Epoch: 1 [44800/64000 (70%)]\tLoss: 2.233450\n",
      "Train Epoch: 1 [45440/64000 (71%)]\tLoss: 2.220193\n",
      "Train Epoch: 1 [46080/64000 (72%)]\tLoss: 2.221967\n",
      "Train Epoch: 1 [46720/64000 (73%)]\tLoss: 2.228462\n",
      "Train Epoch: 1 [47360/64000 (74%)]\tLoss: 2.220577\n",
      "Train Epoch: 1 [48000/64000 (75%)]\tLoss: 2.217793\n",
      "Train Epoch: 1 [48640/64000 (76%)]\tLoss: 2.216857\n",
      "Train Epoch: 1 [49280/64000 (77%)]\tLoss: 2.213725\n",
      "Train Epoch: 1 [49920/64000 (78%)]\tLoss: 2.215317\n",
      "Train Epoch: 1 [50560/64000 (79%)]\tLoss: 2.206614\n",
      "Train Epoch: 1 [51200/64000 (80%)]\tLoss: 2.226274\n",
      "Train Epoch: 1 [51840/64000 (81%)]\tLoss: 2.211581\n",
      "Train Epoch: 1 [52480/64000 (82%)]\tLoss: 2.213722\n",
      "Train Epoch: 1 [53120/64000 (83%)]\tLoss: 2.200544\n",
      "Train Epoch: 1 [53760/64000 (84%)]\tLoss: 2.210681\n",
      "Train Epoch: 1 [54400/64000 (85%)]\tLoss: 2.179577\n",
      "Train Epoch: 1 [55040/64000 (86%)]\tLoss: 2.190106\n",
      "Train Epoch: 1 [55680/64000 (87%)]\tLoss: 2.195574\n",
      "Train Epoch: 1 [56320/64000 (88%)]\tLoss: 2.208004\n",
      "Train Epoch: 1 [56960/64000 (89%)]\tLoss: 2.212508\n",
      "Train Epoch: 1 [57600/64000 (90%)]\tLoss: 2.192735\n",
      "Train Epoch: 1 [58240/64000 (91%)]\tLoss: 2.184141\n",
      "Train Epoch: 1 [58880/64000 (92%)]\tLoss: 2.185550\n",
      "Train Epoch: 1 [59520/64000 (93%)]\tLoss: 2.143428\n",
      "Train Epoch: 1 [60160/64000 (94%)]\tLoss: 2.199387\n",
      "Train Epoch: 1 [60800/64000 (95%)]\tLoss: 2.186057\n",
      "Train Epoch: 1 [61440/64000 (96%)]\tLoss: 2.177314\n",
      "Train Epoch: 1 [62080/64000 (97%)]\tLoss: 2.161528\n",
      "Train Epoch: 1 [62720/64000 (98%)]\tLoss: 2.184506\n",
      "Train Epoch: 1 [63360/64000 (99%)]\tLoss: 2.173715\n",
      "\n",
      "Train Epoch: Average Loss: 2.247077\n",
      "\n",
      "Test set: Average loss: 2.1840 Average accuracy: 0.0145\n",
      "\n",
      "Train Epoch: 2 [0/64000 (0%)]\tLoss: 2.131321\n",
      "Train Epoch: 2 [640/64000 (1%)]\tLoss: 2.184015\n",
      "Train Epoch: 2 [1280/64000 (2%)]\tLoss: 2.140672\n",
      "Train Epoch: 2 [1920/64000 (3%)]\tLoss: 2.148173\n",
      "Train Epoch: 2 [2560/64000 (4%)]\tLoss: 2.152097\n",
      "Train Epoch: 2 [3200/64000 (5%)]\tLoss: 2.174884\n",
      "Train Epoch: 2 [3840/64000 (6%)]\tLoss: 2.151884\n",
      "Train Epoch: 2 [4480/64000 (7%)]\tLoss: 2.155258\n",
      "Train Epoch: 2 [5120/64000 (8%)]\tLoss: 2.144487\n",
      "Train Epoch: 2 [5760/64000 (9%)]\tLoss: 2.139048\n",
      "Train Epoch: 2 [6400/64000 (10%)]\tLoss: 2.117864\n",
      "Train Epoch: 2 [7040/64000 (11%)]\tLoss: 2.153325\n",
      "Train Epoch: 2 [7680/64000 (12%)]\tLoss: 2.127056\n",
      "Train Epoch: 2 [8320/64000 (13%)]\tLoss: 2.104166\n",
      "Train Epoch: 2 [8960/64000 (14%)]\tLoss: 2.113420\n",
      "Train Epoch: 2 [9600/64000 (15%)]\tLoss: 2.112180\n",
      "Train Epoch: 2 [10240/64000 (16%)]\tLoss: 2.105475\n",
      "Train Epoch: 2 [10880/64000 (17%)]\tLoss: 2.123024\n",
      "Train Epoch: 2 [11520/64000 (18%)]\tLoss: 2.098470\n",
      "Train Epoch: 2 [12160/64000 (19%)]\tLoss: 2.084450\n",
      "Train Epoch: 2 [12800/64000 (20%)]\tLoss: 2.140602\n",
      "Train Epoch: 2 [13440/64000 (21%)]\tLoss: 2.086443\n",
      "Train Epoch: 2 [14080/64000 (22%)]\tLoss: 2.093320\n",
      "Train Epoch: 2 [14720/64000 (23%)]\tLoss: 2.067405\n",
      "Train Epoch: 2 [15360/64000 (24%)]\tLoss: 2.082884\n",
      "Train Epoch: 2 [16000/64000 (25%)]\tLoss: 2.143458\n",
      "Train Epoch: 2 [16640/64000 (26%)]\tLoss: 2.087566\n",
      "Train Epoch: 2 [17280/64000 (27%)]\tLoss: 2.104938\n",
      "Train Epoch: 2 [17920/64000 (28%)]\tLoss: 2.084337\n",
      "Train Epoch: 2 [18560/64000 (29%)]\tLoss: 2.080080\n",
      "Train Epoch: 2 [19200/64000 (30%)]\tLoss: 2.003697\n",
      "Train Epoch: 2 [19840/64000 (31%)]\tLoss: 2.059136\n",
      "Train Epoch: 2 [20480/64000 (32%)]\tLoss: 2.076072\n",
      "Train Epoch: 2 [21120/64000 (33%)]\tLoss: 2.074873\n",
      "Train Epoch: 2 [21760/64000 (34%)]\tLoss: 2.075315\n",
      "Train Epoch: 2 [22400/64000 (35%)]\tLoss: 2.046302\n",
      "Train Epoch: 2 [23040/64000 (36%)]\tLoss: 2.072682\n",
      "Train Epoch: 2 [23680/64000 (37%)]\tLoss: 2.093020\n",
      "Train Epoch: 2 [24320/64000 (38%)]\tLoss: 2.078431\n",
      "Train Epoch: 2 [24960/64000 (39%)]\tLoss: 2.044824\n",
      "Train Epoch: 2 [25600/64000 (40%)]\tLoss: 2.046635\n",
      "Train Epoch: 2 [26240/64000 (41%)]\tLoss: 2.028093\n",
      "Train Epoch: 2 [26880/64000 (42%)]\tLoss: 2.077191\n",
      "Train Epoch: 2 [27520/64000 (43%)]\tLoss: 2.095303\n",
      "Train Epoch: 2 [28160/64000 (44%)]\tLoss: 2.018987\n",
      "Train Epoch: 2 [28800/64000 (45%)]\tLoss: 2.033909\n",
      "Train Epoch: 2 [29440/64000 (46%)]\tLoss: 1.963426\n",
      "Train Epoch: 2 [30080/64000 (47%)]\tLoss: 2.075813\n",
      "Train Epoch: 2 [30720/64000 (48%)]\tLoss: 2.068927\n",
      "Train Epoch: 2 [31360/64000 (49%)]\tLoss: 1.995776\n",
      "Train Epoch: 2 [32000/64000 (50%)]\tLoss: 2.000352\n",
      "Train Epoch: 2 [32640/64000 (51%)]\tLoss: 1.998855\n",
      "Train Epoch: 2 [33280/64000 (52%)]\tLoss: 1.987785\n",
      "Train Epoch: 2 [33920/64000 (53%)]\tLoss: 2.027371\n",
      "Train Epoch: 2 [34560/64000 (54%)]\tLoss: 1.998144\n",
      "Train Epoch: 2 [35200/64000 (55%)]\tLoss: 2.031476\n",
      "Train Epoch: 2 [35840/64000 (56%)]\tLoss: 2.003045\n",
      "Train Epoch: 2 [36480/64000 (57%)]\tLoss: 2.036471\n",
      "Train Epoch: 2 [37120/64000 (58%)]\tLoss: 2.018224\n",
      "Train Epoch: 2 [37760/64000 (59%)]\tLoss: 2.021912\n",
      "Train Epoch: 2 [38400/64000 (60%)]\tLoss: 2.030574\n",
      "Train Epoch: 2 [39040/64000 (61%)]\tLoss: 2.013070\n",
      "Train Epoch: 2 [39680/64000 (62%)]\tLoss: 2.048207\n",
      "Train Epoch: 2 [40320/64000 (63%)]\tLoss: 2.038618\n",
      "Train Epoch: 2 [40960/64000 (64%)]\tLoss: 2.017520\n",
      "Train Epoch: 2 [41600/64000 (65%)]\tLoss: 2.052788\n",
      "Train Epoch: 2 [42240/64000 (66%)]\tLoss: 1.972216\n",
      "Train Epoch: 2 [42880/64000 (67%)]\tLoss: 1.951061\n",
      "Train Epoch: 2 [43520/64000 (68%)]\tLoss: 2.017168\n",
      "Train Epoch: 2 [44160/64000 (69%)]\tLoss: 2.012053\n",
      "Train Epoch: 2 [44800/64000 (70%)]\tLoss: 1.942955\n",
      "Train Epoch: 2 [45440/64000 (71%)]\tLoss: 2.059718\n",
      "Train Epoch: 2 [46080/64000 (72%)]\tLoss: 1.980094\n",
      "Train Epoch: 2 [46720/64000 (73%)]\tLoss: 1.958639\n",
      "Train Epoch: 2 [47360/64000 (74%)]\tLoss: 1.985413\n",
      "Train Epoch: 2 [48000/64000 (75%)]\tLoss: 1.997048\n",
      "Train Epoch: 2 [48640/64000 (76%)]\tLoss: 1.955040\n",
      "Train Epoch: 2 [49280/64000 (77%)]\tLoss: 1.943364\n",
      "Train Epoch: 2 [49920/64000 (78%)]\tLoss: 1.959185\n",
      "Train Epoch: 2 [50560/64000 (79%)]\tLoss: 2.014412\n",
      "Train Epoch: 2 [51200/64000 (80%)]\tLoss: 1.929449\n",
      "Train Epoch: 2 [51840/64000 (81%)]\tLoss: 1.939161\n",
      "Train Epoch: 2 [52480/64000 (82%)]\tLoss: 1.941913\n",
      "Train Epoch: 2 [53120/64000 (83%)]\tLoss: 1.954316\n",
      "Train Epoch: 2 [53760/64000 (84%)]\tLoss: 1.930647\n",
      "Train Epoch: 2 [54400/64000 (85%)]\tLoss: 1.973631\n",
      "Train Epoch: 2 [55040/64000 (86%)]\tLoss: 1.988811\n",
      "Train Epoch: 2 [55680/64000 (87%)]\tLoss: 1.959549\n",
      "Train Epoch: 2 [56320/64000 (88%)]\tLoss: 1.988599\n",
      "Train Epoch: 2 [56960/64000 (89%)]\tLoss: 1.984408\n",
      "Train Epoch: 2 [57600/64000 (90%)]\tLoss: 2.007896\n",
      "Train Epoch: 2 [58240/64000 (91%)]\tLoss: 1.915329\n",
      "Train Epoch: 2 [58880/64000 (92%)]\tLoss: 1.910849\n",
      "Train Epoch: 2 [59520/64000 (93%)]\tLoss: 1.947323\n",
      "Train Epoch: 2 [60160/64000 (94%)]\tLoss: 1.960181\n",
      "Train Epoch: 2 [60800/64000 (95%)]\tLoss: 1.927319\n",
      "Train Epoch: 2 [61440/64000 (96%)]\tLoss: 1.856419\n",
      "Train Epoch: 2 [62080/64000 (97%)]\tLoss: 1.861607\n",
      "Train Epoch: 2 [62720/64000 (98%)]\tLoss: 1.920184\n",
      "Train Epoch: 2 [63360/64000 (99%)]\tLoss: 1.947027\n",
      "\n",
      "Train Epoch: Average Loss: 2.031951\n",
      "\n",
      "Test set: Average loss: 1.9533 Average accuracy: 0.0202\n",
      "\n",
      "Train Epoch: 3 [0/64000 (0%)]\tLoss: 1.934990\n",
      "Train Epoch: 3 [640/64000 (1%)]\tLoss: 1.915050\n",
      "Train Epoch: 3 [1280/64000 (2%)]\tLoss: 1.983635\n",
      "Train Epoch: 3 [1920/64000 (3%)]\tLoss: 1.848070\n",
      "Train Epoch: 3 [2560/64000 (4%)]\tLoss: 1.895656\n",
      "Train Epoch: 3 [3200/64000 (5%)]\tLoss: 1.839027\n",
      "Train Epoch: 3 [3840/64000 (6%)]\tLoss: 1.885647\n",
      "Train Epoch: 3 [4480/64000 (7%)]\tLoss: 1.894128\n",
      "Train Epoch: 3 [5120/64000 (8%)]\tLoss: 1.881554\n",
      "Train Epoch: 3 [5760/64000 (9%)]\tLoss: 1.955701\n",
      "Train Epoch: 3 [6400/64000 (10%)]\tLoss: 1.818140\n",
      "Train Epoch: 3 [7040/64000 (11%)]\tLoss: 1.892951\n",
      "Train Epoch: 3 [7680/64000 (12%)]\tLoss: 1.861966\n",
      "Train Epoch: 3 [8320/64000 (13%)]\tLoss: 1.963320\n",
      "Train Epoch: 3 [8960/64000 (14%)]\tLoss: 1.883483\n",
      "Train Epoch: 3 [9600/64000 (15%)]\tLoss: 1.851622\n",
      "Train Epoch: 3 [10240/64000 (16%)]\tLoss: 1.869716\n",
      "Train Epoch: 3 [10880/64000 (17%)]\tLoss: 1.854158\n",
      "Train Epoch: 3 [11520/64000 (18%)]\tLoss: 1.943979\n",
      "Train Epoch: 3 [12160/64000 (19%)]\tLoss: 1.844088\n",
      "Train Epoch: 3 [12800/64000 (20%)]\tLoss: 1.899969\n",
      "Train Epoch: 3 [13440/64000 (21%)]\tLoss: 1.892319\n",
      "Train Epoch: 3 [14080/64000 (22%)]\tLoss: 1.892882\n",
      "Train Epoch: 3 [14720/64000 (23%)]\tLoss: 1.973833\n",
      "Train Epoch: 3 [15360/64000 (24%)]\tLoss: 1.952314\n",
      "Train Epoch: 3 [16000/64000 (25%)]\tLoss: 1.911168\n",
      "Train Epoch: 3 [16640/64000 (26%)]\tLoss: 1.872435\n",
      "Train Epoch: 3 [17280/64000 (27%)]\tLoss: 1.839602\n",
      "Train Epoch: 3 [17920/64000 (28%)]\tLoss: 1.859782\n",
      "Train Epoch: 3 [18560/64000 (29%)]\tLoss: 1.917173\n",
      "Train Epoch: 3 [19200/64000 (30%)]\tLoss: 1.839343\n",
      "Train Epoch: 3 [19840/64000 (31%)]\tLoss: 1.869533\n",
      "Train Epoch: 3 [20480/64000 (32%)]\tLoss: 1.836594\n",
      "Train Epoch: 3 [21120/64000 (33%)]\tLoss: 1.803538\n",
      "Train Epoch: 3 [21760/64000 (34%)]\tLoss: 1.818263\n",
      "Train Epoch: 3 [22400/64000 (35%)]\tLoss: 1.793837\n",
      "Train Epoch: 3 [23040/64000 (36%)]\tLoss: 1.854054\n",
      "Train Epoch: 3 [23680/64000 (37%)]\tLoss: 1.882408\n",
      "Train Epoch: 3 [24320/64000 (38%)]\tLoss: 1.917004\n",
      "Train Epoch: 3 [24960/64000 (39%)]\tLoss: 1.866887\n",
      "Train Epoch: 3 [25600/64000 (40%)]\tLoss: 1.853147\n",
      "Train Epoch: 3 [26240/64000 (41%)]\tLoss: 1.779144\n",
      "Train Epoch: 3 [26880/64000 (42%)]\tLoss: 1.678929\n",
      "Train Epoch: 3 [27520/64000 (43%)]\tLoss: 1.942804\n",
      "Train Epoch: 3 [28160/64000 (44%)]\tLoss: 1.805539\n",
      "Train Epoch: 3 [28800/64000 (45%)]\tLoss: 1.973030\n",
      "Train Epoch: 3 [29440/64000 (46%)]\tLoss: 1.923380\n",
      "Train Epoch: 3 [30080/64000 (47%)]\tLoss: 1.912197\n",
      "Train Epoch: 3 [30720/64000 (48%)]\tLoss: 1.925730\n",
      "Train Epoch: 3 [31360/64000 (49%)]\tLoss: 1.816307\n",
      "Train Epoch: 3 [32000/64000 (50%)]\tLoss: 1.917063\n",
      "Train Epoch: 3 [32640/64000 (51%)]\tLoss: 1.749959\n",
      "Train Epoch: 3 [33280/64000 (52%)]\tLoss: 1.756348\n",
      "Train Epoch: 3 [33920/64000 (53%)]\tLoss: 1.792433\n",
      "Train Epoch: 3 [34560/64000 (54%)]\tLoss: 1.863385\n",
      "Train Epoch: 3 [35200/64000 (55%)]\tLoss: 1.819974\n",
      "Train Epoch: 3 [35840/64000 (56%)]\tLoss: 1.807630\n",
      "Train Epoch: 3 [36480/64000 (57%)]\tLoss: 1.780156\n",
      "Train Epoch: 3 [37120/64000 (58%)]\tLoss: 1.875018\n",
      "Train Epoch: 3 [37760/64000 (59%)]\tLoss: 1.818524\n",
      "Train Epoch: 3 [38400/64000 (60%)]\tLoss: 1.825254\n",
      "Train Epoch: 3 [39040/64000 (61%)]\tLoss: 1.845910\n",
      "Train Epoch: 3 [39680/64000 (62%)]\tLoss: 1.803713\n",
      "Train Epoch: 3 [40320/64000 (63%)]\tLoss: 1.735581\n",
      "Train Epoch: 3 [40960/64000 (64%)]\tLoss: 1.856507\n",
      "Train Epoch: 3 [41600/64000 (65%)]\tLoss: 1.874023\n",
      "Train Epoch: 3 [42240/64000 (66%)]\tLoss: 1.756525\n",
      "Train Epoch: 3 [42880/64000 (67%)]\tLoss: 1.824020\n",
      "Train Epoch: 3 [43520/64000 (68%)]\tLoss: 1.820026\n",
      "Train Epoch: 3 [44160/64000 (69%)]\tLoss: 1.777381\n",
      "Train Epoch: 3 [44800/64000 (70%)]\tLoss: 1.795465\n",
      "Train Epoch: 3 [45440/64000 (71%)]\tLoss: 1.855840\n",
      "Train Epoch: 3 [46080/64000 (72%)]\tLoss: 1.792276\n",
      "Train Epoch: 3 [46720/64000 (73%)]\tLoss: 1.862842\n",
      "Train Epoch: 3 [47360/64000 (74%)]\tLoss: 1.797706\n",
      "Train Epoch: 3 [48000/64000 (75%)]\tLoss: 1.770745\n",
      "Train Epoch: 3 [48640/64000 (76%)]\tLoss: 1.841555\n",
      "Train Epoch: 3 [49280/64000 (77%)]\tLoss: 1.886192\n",
      "Train Epoch: 3 [49920/64000 (78%)]\tLoss: 1.794697\n",
      "Train Epoch: 3 [50560/64000 (79%)]\tLoss: 1.828807\n",
      "Train Epoch: 3 [51200/64000 (80%)]\tLoss: 1.754021\n",
      "Train Epoch: 3 [51840/64000 (81%)]\tLoss: 1.790875\n",
      "Train Epoch: 3 [52480/64000 (82%)]\tLoss: 1.865926\n",
      "Train Epoch: 3 [53120/64000 (83%)]\tLoss: 1.803596\n",
      "Train Epoch: 3 [53760/64000 (84%)]\tLoss: 1.750044\n",
      "Train Epoch: 3 [54400/64000 (85%)]\tLoss: 1.773879\n",
      "Train Epoch: 3 [55040/64000 (86%)]\tLoss: 1.837092\n",
      "Train Epoch: 3 [55680/64000 (87%)]\tLoss: 1.795913\n",
      "Train Epoch: 3 [56320/64000 (88%)]\tLoss: 1.798752\n",
      "Train Epoch: 3 [56960/64000 (89%)]\tLoss: 1.760627\n",
      "Train Epoch: 3 [57600/64000 (90%)]\tLoss: 1.758364\n",
      "Train Epoch: 3 [58240/64000 (91%)]\tLoss: 1.799727\n",
      "Train Epoch: 3 [58880/64000 (92%)]\tLoss: 1.802652\n",
      "Train Epoch: 3 [59520/64000 (93%)]\tLoss: 1.824726\n",
      "Train Epoch: 3 [60160/64000 (94%)]\tLoss: 1.686290\n",
      "Train Epoch: 3 [60800/64000 (95%)]\tLoss: 1.754404\n",
      "Train Epoch: 3 [61440/64000 (96%)]\tLoss: 1.741494\n",
      "Train Epoch: 3 [62080/64000 (97%)]\tLoss: 1.767219\n",
      "Train Epoch: 3 [62720/64000 (98%)]\tLoss: 1.814026\n",
      "Train Epoch: 3 [63360/64000 (99%)]\tLoss: 1.700218\n",
      "\n",
      "Train Epoch: Average Loss: 1.839748\n",
      "\n",
      "Test set: Average loss: 1.8317 Average accuracy: 0.0363\n",
      "\n",
      "Train Epoch: 4 [0/64000 (0%)]\tLoss: 1.720237\n",
      "Train Epoch: 4 [640/64000 (1%)]\tLoss: 1.817228\n",
      "Train Epoch: 4 [1280/64000 (2%)]\tLoss: 1.830825\n",
      "Train Epoch: 4 [1920/64000 (3%)]\tLoss: 1.746097\n",
      "Train Epoch: 4 [2560/64000 (4%)]\tLoss: 1.746846\n",
      "Train Epoch: 4 [3200/64000 (5%)]\tLoss: 1.620932\n",
      "Train Epoch: 4 [3840/64000 (6%)]\tLoss: 1.725071\n",
      "Train Epoch: 4 [4480/64000 (7%)]\tLoss: 1.841742\n",
      "Train Epoch: 4 [5120/64000 (8%)]\tLoss: 1.706139\n",
      "Train Epoch: 4 [5760/64000 (9%)]\tLoss: 1.880661\n",
      "Train Epoch: 4 [6400/64000 (10%)]\tLoss: 1.738834\n",
      "Train Epoch: 4 [7040/64000 (11%)]\tLoss: 1.724159\n",
      "Train Epoch: 4 [7680/64000 (12%)]\tLoss: 1.864018\n",
      "Train Epoch: 4 [8320/64000 (13%)]\tLoss: 1.691992\n",
      "Train Epoch: 4 [8960/64000 (14%)]\tLoss: 1.695371\n",
      "Train Epoch: 4 [9600/64000 (15%)]\tLoss: 1.693384\n",
      "Train Epoch: 4 [10240/64000 (16%)]\tLoss: 1.769063\n",
      "Train Epoch: 4 [10880/64000 (17%)]\tLoss: 1.904850\n",
      "Train Epoch: 4 [11520/64000 (18%)]\tLoss: 1.768822\n",
      "Train Epoch: 4 [12160/64000 (19%)]\tLoss: 1.770882\n",
      "Train Epoch: 4 [12800/64000 (20%)]\tLoss: 1.706297\n",
      "Train Epoch: 4 [13440/64000 (21%)]\tLoss: 1.722328\n",
      "Train Epoch: 4 [14080/64000 (22%)]\tLoss: 1.860880\n",
      "Train Epoch: 4 [14720/64000 (23%)]\tLoss: 1.681033\n",
      "Train Epoch: 4 [15360/64000 (24%)]\tLoss: 1.836559\n",
      "Train Epoch: 4 [16000/64000 (25%)]\tLoss: 1.674465\n",
      "Train Epoch: 4 [16640/64000 (26%)]\tLoss: 1.738715\n",
      "Train Epoch: 4 [17280/64000 (27%)]\tLoss: 1.754315\n",
      "Train Epoch: 4 [17920/64000 (28%)]\tLoss: 1.836820\n",
      "Train Epoch: 4 [18560/64000 (29%)]\tLoss: 1.763640\n",
      "Train Epoch: 4 [19200/64000 (30%)]\tLoss: 1.690277\n",
      "Train Epoch: 4 [19840/64000 (31%)]\tLoss: 1.740598\n",
      "Train Epoch: 4 [20480/64000 (32%)]\tLoss: 1.735878\n",
      "Train Epoch: 4 [21120/64000 (33%)]\tLoss: 1.777075\n",
      "Train Epoch: 4 [21760/64000 (34%)]\tLoss: 1.750277\n",
      "Train Epoch: 4 [22400/64000 (35%)]\tLoss: 1.665943\n",
      "Train Epoch: 4 [23040/64000 (36%)]\tLoss: 1.875837\n",
      "Train Epoch: 4 [23680/64000 (37%)]\tLoss: 1.662282\n",
      "Train Epoch: 4 [24320/64000 (38%)]\tLoss: 1.768388\n",
      "Train Epoch: 4 [24960/64000 (39%)]\tLoss: 1.703266\n",
      "Train Epoch: 4 [25600/64000 (40%)]\tLoss: 1.746964\n",
      "Train Epoch: 4 [26240/64000 (41%)]\tLoss: 1.764328\n",
      "Train Epoch: 4 [26880/64000 (42%)]\tLoss: 1.780117\n",
      "Train Epoch: 4 [27520/64000 (43%)]\tLoss: 1.670165\n",
      "Train Epoch: 4 [28160/64000 (44%)]\tLoss: 1.734888\n",
      "Train Epoch: 4 [28800/64000 (45%)]\tLoss: 1.734037\n",
      "Train Epoch: 4 [29440/64000 (46%)]\tLoss: 1.747966\n",
      "Train Epoch: 4 [30080/64000 (47%)]\tLoss: 1.795989\n",
      "Train Epoch: 4 [30720/64000 (48%)]\tLoss: 1.760237\n",
      "Train Epoch: 4 [31360/64000 (49%)]\tLoss: 1.723036\n",
      "Train Epoch: 4 [32000/64000 (50%)]\tLoss: 1.703184\n",
      "Train Epoch: 4 [32640/64000 (51%)]\tLoss: 1.658573\n",
      "Train Epoch: 4 [33280/64000 (52%)]\tLoss: 1.776913\n",
      "Train Epoch: 4 [33920/64000 (53%)]\tLoss: 1.777602\n",
      "Train Epoch: 4 [34560/64000 (54%)]\tLoss: 1.723754\n",
      "Train Epoch: 4 [35200/64000 (55%)]\tLoss: 1.737121\n",
      "Train Epoch: 4 [35840/64000 (56%)]\tLoss: 1.735307\n",
      "Train Epoch: 4 [36480/64000 (57%)]\tLoss: 1.651280\n",
      "Train Epoch: 4 [37120/64000 (58%)]\tLoss: 1.739207\n",
      "Train Epoch: 4 [37760/64000 (59%)]\tLoss: 1.856490\n",
      "Train Epoch: 4 [38400/64000 (60%)]\tLoss: 1.707963\n",
      "Train Epoch: 4 [39040/64000 (61%)]\tLoss: 1.772678\n",
      "Train Epoch: 4 [39680/64000 (62%)]\tLoss: 1.738019\n",
      "Train Epoch: 4 [40320/64000 (63%)]\tLoss: 1.789680\n",
      "Train Epoch: 4 [40960/64000 (64%)]\tLoss: 1.683904\n",
      "Train Epoch: 4 [41600/64000 (65%)]\tLoss: 1.751717\n",
      "Train Epoch: 4 [42240/64000 (66%)]\tLoss: 1.787874\n",
      "Train Epoch: 4 [42880/64000 (67%)]\tLoss: 1.759183\n",
      "Train Epoch: 4 [43520/64000 (68%)]\tLoss: 1.655141\n",
      "Train Epoch: 4 [44160/64000 (69%)]\tLoss: 1.728031\n",
      "Train Epoch: 4 [44800/64000 (70%)]\tLoss: 1.711755\n",
      "Train Epoch: 4 [45440/64000 (71%)]\tLoss: 1.802143\n",
      "Train Epoch: 4 [46080/64000 (72%)]\tLoss: 1.748538\n",
      "Train Epoch: 4 [46720/64000 (73%)]\tLoss: 1.784719\n",
      "Train Epoch: 4 [47360/64000 (74%)]\tLoss: 1.796183\n",
      "Train Epoch: 4 [48000/64000 (75%)]\tLoss: 1.767094\n",
      "Train Epoch: 4 [48640/64000 (76%)]\tLoss: 1.667327\n",
      "Train Epoch: 4 [49280/64000 (77%)]\tLoss: 1.717534\n",
      "Train Epoch: 4 [49920/64000 (78%)]\tLoss: 1.711935\n",
      "Train Epoch: 4 [50560/64000 (79%)]\tLoss: 1.683003\n",
      "Train Epoch: 4 [51200/64000 (80%)]\tLoss: 1.779984\n",
      "Train Epoch: 4 [51840/64000 (81%)]\tLoss: 1.679433\n",
      "Train Epoch: 4 [52480/64000 (82%)]\tLoss: 1.800270\n",
      "Train Epoch: 4 [53120/64000 (83%)]\tLoss: 1.739608\n",
      "Train Epoch: 4 [53760/64000 (84%)]\tLoss: 1.761547\n",
      "Train Epoch: 4 [54400/64000 (85%)]\tLoss: 1.797106\n",
      "Train Epoch: 4 [55040/64000 (86%)]\tLoss: 1.721151\n",
      "Train Epoch: 4 [55680/64000 (87%)]\tLoss: 1.819187\n",
      "Train Epoch: 4 [56320/64000 (88%)]\tLoss: 1.640925\n",
      "Train Epoch: 4 [56960/64000 (89%)]\tLoss: 1.680659\n",
      "Train Epoch: 4 [57600/64000 (90%)]\tLoss: 1.780633\n",
      "Train Epoch: 4 [58240/64000 (91%)]\tLoss: 1.790599\n",
      "Train Epoch: 4 [58880/64000 (92%)]\tLoss: 1.716224\n",
      "Train Epoch: 4 [59520/64000 (93%)]\tLoss: 1.662911\n",
      "Train Epoch: 4 [60160/64000 (94%)]\tLoss: 1.732369\n",
      "Train Epoch: 4 [60800/64000 (95%)]\tLoss: 1.750447\n",
      "Train Epoch: 4 [61440/64000 (96%)]\tLoss: 1.753224\n",
      "Train Epoch: 4 [62080/64000 (97%)]\tLoss: 1.706808\n",
      "Train Epoch: 4 [62720/64000 (98%)]\tLoss: 1.712825\n",
      "Train Epoch: 4 [63360/64000 (99%)]\tLoss: 1.706810\n",
      "\n",
      "Train Epoch: Average Loss: 1.742029\n",
      "\n",
      "Test set: Average loss: 1.7793 Average accuracy: 0.0397\n",
      "\n",
      "Train Epoch: 5 [0/64000 (0%)]\tLoss: 1.690195\n",
      "Train Epoch: 5 [640/64000 (1%)]\tLoss: 1.627758\n",
      "Train Epoch: 5 [1280/64000 (2%)]\tLoss: 1.768549\n",
      "Train Epoch: 5 [1920/64000 (3%)]\tLoss: 1.676247\n",
      "Train Epoch: 5 [2560/64000 (4%)]\tLoss: 1.684424\n",
      "Train Epoch: 5 [3200/64000 (5%)]\tLoss: 1.675947\n",
      "Train Epoch: 5 [3840/64000 (6%)]\tLoss: 1.761234\n",
      "Train Epoch: 5 [4480/64000 (7%)]\tLoss: 1.729496\n",
      "Train Epoch: 5 [5120/64000 (8%)]\tLoss: 1.637396\n",
      "Train Epoch: 5 [5760/64000 (9%)]\tLoss: 1.825375\n",
      "Train Epoch: 5 [6400/64000 (10%)]\tLoss: 1.674267\n",
      "Train Epoch: 5 [7040/64000 (11%)]\tLoss: 1.779200\n",
      "Train Epoch: 5 [7680/64000 (12%)]\tLoss: 1.660707\n",
      "Train Epoch: 5 [8320/64000 (13%)]\tLoss: 1.660799\n",
      "Train Epoch: 5 [8960/64000 (14%)]\tLoss: 1.690679\n",
      "Train Epoch: 5 [9600/64000 (15%)]\tLoss: 1.680895\n",
      "Train Epoch: 5 [10240/64000 (16%)]\tLoss: 1.745234\n",
      "Train Epoch: 5 [10880/64000 (17%)]\tLoss: 1.709637\n",
      "Train Epoch: 5 [11520/64000 (18%)]\tLoss: 1.649772\n",
      "Train Epoch: 5 [12160/64000 (19%)]\tLoss: 1.680360\n",
      "Train Epoch: 5 [12800/64000 (20%)]\tLoss: 1.665218\n",
      "Train Epoch: 5 [13440/64000 (21%)]\tLoss: 1.566585\n",
      "Train Epoch: 5 [14080/64000 (22%)]\tLoss: 1.689323\n",
      "Train Epoch: 5 [14720/64000 (23%)]\tLoss: 1.745463\n",
      "Train Epoch: 5 [15360/64000 (24%)]\tLoss: 1.672697\n",
      "Train Epoch: 5 [16000/64000 (25%)]\tLoss: 1.703327\n",
      "Train Epoch: 5 [16640/64000 (26%)]\tLoss: 1.726499\n",
      "Train Epoch: 5 [17280/64000 (27%)]\tLoss: 1.725675\n",
      "Train Epoch: 5 [17920/64000 (28%)]\tLoss: 1.668816\n",
      "Train Epoch: 5 [18560/64000 (29%)]\tLoss: 1.717950\n",
      "Train Epoch: 5 [19200/64000 (30%)]\tLoss: 1.631463\n",
      "Train Epoch: 5 [19840/64000 (31%)]\tLoss: 1.619758\n",
      "Train Epoch: 5 [20480/64000 (32%)]\tLoss: 1.641362\n",
      "Train Epoch: 5 [21120/64000 (33%)]\tLoss: 1.659735\n",
      "Train Epoch: 5 [21760/64000 (34%)]\tLoss: 1.582492\n",
      "Train Epoch: 5 [22400/64000 (35%)]\tLoss: 1.722623\n",
      "Train Epoch: 5 [23040/64000 (36%)]\tLoss: 1.717535\n",
      "Train Epoch: 5 [23680/64000 (37%)]\tLoss: 1.741844\n",
      "Train Epoch: 5 [24320/64000 (38%)]\tLoss: 1.717900\n",
      "Train Epoch: 5 [24960/64000 (39%)]\tLoss: 1.757547\n",
      "Train Epoch: 5 [25600/64000 (40%)]\tLoss: 1.670173\n",
      "Train Epoch: 5 [26240/64000 (41%)]\tLoss: 1.715460\n",
      "Train Epoch: 5 [26880/64000 (42%)]\tLoss: 1.812747\n",
      "Train Epoch: 5 [27520/64000 (43%)]\tLoss: 1.694411\n",
      "Train Epoch: 5 [28160/64000 (44%)]\tLoss: 1.744422\n",
      "Train Epoch: 5 [28800/64000 (45%)]\tLoss: 1.690598\n",
      "Train Epoch: 5 [29440/64000 (46%)]\tLoss: 1.663575\n",
      "Train Epoch: 5 [30080/64000 (47%)]\tLoss: 1.778131\n",
      "Train Epoch: 5 [30720/64000 (48%)]\tLoss: 1.770643\n",
      "Train Epoch: 5 [31360/64000 (49%)]\tLoss: 1.697252\n",
      "Train Epoch: 5 [32000/64000 (50%)]\tLoss: 1.724776\n",
      "Train Epoch: 5 [32640/64000 (51%)]\tLoss: 1.774984\n",
      "Train Epoch: 5 [33280/64000 (52%)]\tLoss: 1.636654\n",
      "Train Epoch: 5 [33920/64000 (53%)]\tLoss: 1.694295\n",
      "Train Epoch: 5 [34560/64000 (54%)]\tLoss: 1.691181\n",
      "Train Epoch: 5 [35200/64000 (55%)]\tLoss: 1.622523\n",
      "Train Epoch: 5 [35840/64000 (56%)]\tLoss: 1.733581\n",
      "Train Epoch: 5 [36480/64000 (57%)]\tLoss: 1.521466\n",
      "Train Epoch: 5 [37120/64000 (58%)]\tLoss: 1.681278\n",
      "Train Epoch: 5 [37760/64000 (59%)]\tLoss: 1.726562\n",
      "Train Epoch: 5 [38400/64000 (60%)]\tLoss: 1.732623\n",
      "Train Epoch: 5 [39040/64000 (61%)]\tLoss: 1.533535\n",
      "Train Epoch: 5 [39680/64000 (62%)]\tLoss: 1.656127\n",
      "Train Epoch: 5 [40320/64000 (63%)]\tLoss: 1.697153\n",
      "Train Epoch: 5 [40960/64000 (64%)]\tLoss: 1.605303\n",
      "Train Epoch: 5 [41600/64000 (65%)]\tLoss: 1.722504\n",
      "Train Epoch: 5 [42240/64000 (66%)]\tLoss: 1.718572\n",
      "Train Epoch: 5 [42880/64000 (67%)]\tLoss: 1.595562\n",
      "Train Epoch: 5 [43520/64000 (68%)]\tLoss: 1.659740\n",
      "Train Epoch: 5 [44160/64000 (69%)]\tLoss: 1.604763\n",
      "Train Epoch: 5 [44800/64000 (70%)]\tLoss: 1.741372\n",
      "Train Epoch: 5 [45440/64000 (71%)]\tLoss: 1.626945\n",
      "Train Epoch: 5 [46080/64000 (72%)]\tLoss: 1.559158\n",
      "Train Epoch: 5 [46720/64000 (73%)]\tLoss: 1.690675\n",
      "Train Epoch: 5 [47360/64000 (74%)]\tLoss: 1.782167\n",
      "Train Epoch: 5 [48000/64000 (75%)]\tLoss: 1.750256\n",
      "Train Epoch: 5 [48640/64000 (76%)]\tLoss: 1.693891\n",
      "Train Epoch: 5 [49280/64000 (77%)]\tLoss: 1.796147\n",
      "Train Epoch: 5 [49920/64000 (78%)]\tLoss: 1.777124\n",
      "Train Epoch: 5 [50560/64000 (79%)]\tLoss: 1.643802\n",
      "Train Epoch: 5 [51200/64000 (80%)]\tLoss: 1.809493\n",
      "Train Epoch: 5 [51840/64000 (81%)]\tLoss: 1.689007\n",
      "Train Epoch: 5 [52480/64000 (82%)]\tLoss: 1.597160\n",
      "Train Epoch: 5 [53120/64000 (83%)]\tLoss: 1.605058\n",
      "Train Epoch: 5 [53760/64000 (84%)]\tLoss: 1.645387\n",
      "Train Epoch: 5 [54400/64000 (85%)]\tLoss: 1.655401\n",
      "Train Epoch: 5 [55040/64000 (86%)]\tLoss: 1.795053\n",
      "Train Epoch: 5 [55680/64000 (87%)]\tLoss: 1.651490\n",
      "Train Epoch: 5 [56320/64000 (88%)]\tLoss: 1.636566\n",
      "Train Epoch: 5 [56960/64000 (89%)]\tLoss: 1.639253\n",
      "Train Epoch: 5 [57600/64000 (90%)]\tLoss: 1.701188\n",
      "Train Epoch: 5 [58240/64000 (91%)]\tLoss: 1.565921\n",
      "Train Epoch: 5 [58880/64000 (92%)]\tLoss: 1.660576\n",
      "Train Epoch: 5 [59520/64000 (93%)]\tLoss: 1.657578\n",
      "Train Epoch: 5 [60160/64000 (94%)]\tLoss: 1.606009\n",
      "Train Epoch: 5 [60800/64000 (95%)]\tLoss: 1.629723\n",
      "Train Epoch: 5 [61440/64000 (96%)]\tLoss: 1.672575\n",
      "Train Epoch: 5 [62080/64000 (97%)]\tLoss: 1.626066\n",
      "Train Epoch: 5 [62720/64000 (98%)]\tLoss: 1.686419\n",
      "Train Epoch: 5 [63360/64000 (99%)]\tLoss: 1.586805\n",
      "\n",
      "Train Epoch: Average Loss: 1.686999\n",
      "\n",
      "Test set: Average loss: 1.7312 Average accuracy: 0.0512\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "early_stopping = EarlyStopping()\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch, loss_fn, None, 1, 2)\n",
    "    test_loss = test(model, device, test_loader, loss_fn, task2_accuracy, 1, 2)\n",
    "\n",
    "    if early_stopping(test_loss):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # Input image 84 x 84\n",
    "        self.conv1 = nn.Conv2d(in_channels, 8, 3)  # output 82 x 82\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # output 41 x 41\n",
    "        self.conv1_bn = nn.BatchNorm2d(8)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(8, 16, 4)  # output 38 x 38\n",
    "        self.conv2_bn = nn.BatchNorm2d(16)  # output 19 x 19\n",
    "\n",
    "        self.conv3 = nn.Conv2d(16, 4, 3) # output 17 x 17\n",
    "        self.conv3_bn = nn.BatchNorm2d(4)\n",
    "\n",
    "        self.fc1 = nn.Linear(17 * 17 * 4, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.out = nn.Linear(64, 30)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "\n",
    "        # print(x.shape)\n",
    "\n",
    "        x = self.conv1_bn(x)\n",
    "\n",
    "        # print(x.shape)\n",
    "\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "        # print(x.shape)\n",
    "\n",
    "        x = self.conv2_bn(x)\n",
    "\n",
    "        # print(x.shape)\n",
    "\n",
    "        x = F.relu(self.conv3(x))\n",
    "\n",
    "        # print(x.shape)\n",
    "\n",
    "        x = self.conv3_bn(x)\n",
    "\n",
    "        # print(x.shape)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        # print(x.shape)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        x = self.out(x)\n",
    "\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        # x = F.sigmoid(x)\n",
    "        x = x.view(-1, 3, 10)\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv1_bn): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(8, 16, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (conv2_bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv3_bn): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=1156, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (out): Linear(in_features=64, out_features=30, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "model = CNN().to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/64000 (0%)]\tLoss: 2.289592\n",
      "Train Epoch: 1 [640/64000 (1%)]\tLoss: 2.312856\n",
      "Train Epoch: 1 [1280/64000 (2%)]\tLoss: 2.311376\n",
      "Train Epoch: 1 [1920/64000 (3%)]\tLoss: 2.311517\n",
      "Train Epoch: 1 [2560/64000 (4%)]\tLoss: 2.303910\n",
      "Train Epoch: 1 [3200/64000 (5%)]\tLoss: 2.313715\n",
      "Train Epoch: 1 [3840/64000 (6%)]\tLoss: 2.309962\n",
      "Train Epoch: 1 [4480/64000 (7%)]\tLoss: 2.297141\n",
      "Train Epoch: 1 [5120/64000 (8%)]\tLoss: 2.311703\n",
      "Train Epoch: 1 [5760/64000 (9%)]\tLoss: 2.302983\n",
      "Train Epoch: 1 [6400/64000 (10%)]\tLoss: 2.297581\n",
      "Train Epoch: 1 [7040/64000 (11%)]\tLoss: 2.309102\n",
      "Train Epoch: 1 [7680/64000 (12%)]\tLoss: 2.294192\n",
      "Train Epoch: 1 [8320/64000 (13%)]\tLoss: 2.299702\n",
      "Train Epoch: 1 [8960/64000 (14%)]\tLoss: 2.311949\n",
      "Train Epoch: 1 [9600/64000 (15%)]\tLoss: 2.289080\n",
      "Train Epoch: 1 [10240/64000 (16%)]\tLoss: 2.297824\n",
      "Train Epoch: 1 [10880/64000 (17%)]\tLoss: 2.290418\n",
      "Train Epoch: 1 [11520/64000 (18%)]\tLoss: 2.312258\n",
      "Train Epoch: 1 [12160/64000 (19%)]\tLoss: 2.295887\n",
      "Train Epoch: 1 [12800/64000 (20%)]\tLoss: 2.289554\n",
      "Train Epoch: 1 [13440/64000 (21%)]\tLoss: 2.288683\n",
      "Train Epoch: 1 [14080/64000 (22%)]\tLoss: 2.280642\n",
      "Train Epoch: 1 [14720/64000 (23%)]\tLoss: 2.295796\n",
      "Train Epoch: 1 [15360/64000 (24%)]\tLoss: 2.290686\n",
      "Train Epoch: 1 [16000/64000 (25%)]\tLoss: 2.281044\n",
      "Train Epoch: 1 [16640/64000 (26%)]\tLoss: 2.295860\n",
      "Train Epoch: 1 [17280/64000 (27%)]\tLoss: 2.282943\n",
      "Train Epoch: 1 [17920/64000 (28%)]\tLoss: 2.272927\n",
      "Train Epoch: 1 [18560/64000 (29%)]\tLoss: 2.283391\n",
      "Train Epoch: 1 [19200/64000 (30%)]\tLoss: 2.269802\n",
      "Train Epoch: 1 [19840/64000 (31%)]\tLoss: 2.291647\n",
      "Train Epoch: 1 [20480/64000 (32%)]\tLoss: 2.292047\n",
      "Train Epoch: 1 [21120/64000 (33%)]\tLoss: 2.276718\n",
      "Train Epoch: 1 [21760/64000 (34%)]\tLoss: 2.283604\n",
      "Train Epoch: 1 [22400/64000 (35%)]\tLoss: 2.267678\n",
      "Train Epoch: 1 [23040/64000 (36%)]\tLoss: 2.276330\n",
      "Train Epoch: 1 [23680/64000 (37%)]\tLoss: 2.264705\n",
      "Train Epoch: 1 [24320/64000 (38%)]\tLoss: 2.265105\n",
      "Train Epoch: 1 [24960/64000 (39%)]\tLoss: 2.249008\n",
      "Train Epoch: 1 [25600/64000 (40%)]\tLoss: 2.268574\n",
      "Train Epoch: 1 [26240/64000 (41%)]\tLoss: 2.259335\n",
      "Train Epoch: 1 [26880/64000 (42%)]\tLoss: 2.253701\n",
      "Train Epoch: 1 [27520/64000 (43%)]\tLoss: 2.241279\n",
      "Train Epoch: 1 [28160/64000 (44%)]\tLoss: 2.252394\n",
      "Train Epoch: 1 [28800/64000 (45%)]\tLoss: 2.226378\n",
      "Train Epoch: 1 [29440/64000 (46%)]\tLoss: 2.229729\n",
      "Train Epoch: 1 [30080/64000 (47%)]\tLoss: 2.223941\n",
      "Train Epoch: 1 [30720/64000 (48%)]\tLoss: 2.182226\n",
      "Train Epoch: 1 [31360/64000 (49%)]\tLoss: 2.199189\n",
      "Train Epoch: 1 [32000/64000 (50%)]\tLoss: 2.194943\n",
      "Train Epoch: 1 [32640/64000 (51%)]\tLoss: 2.150191\n",
      "Train Epoch: 1 [33280/64000 (52%)]\tLoss: 2.171823\n",
      "Train Epoch: 1 [33920/64000 (53%)]\tLoss: 2.161218\n",
      "Train Epoch: 1 [34560/64000 (54%)]\tLoss: 2.180814\n",
      "Train Epoch: 1 [35200/64000 (55%)]\tLoss: 2.170393\n",
      "Train Epoch: 1 [35840/64000 (56%)]\tLoss: 2.133774\n",
      "Train Epoch: 1 [36480/64000 (57%)]\tLoss: 2.125630\n",
      "Train Epoch: 1 [37120/64000 (58%)]\tLoss: 2.111883\n",
      "Train Epoch: 1 [37760/64000 (59%)]\tLoss: 2.084018\n",
      "Train Epoch: 1 [38400/64000 (60%)]\tLoss: 2.071422\n",
      "Train Epoch: 1 [39040/64000 (61%)]\tLoss: 2.036653\n",
      "Train Epoch: 1 [39680/64000 (62%)]\tLoss: 2.042846\n",
      "Train Epoch: 1 [40320/64000 (63%)]\tLoss: 2.050722\n",
      "Train Epoch: 1 [40960/64000 (64%)]\tLoss: 1.948989\n",
      "Train Epoch: 1 [41600/64000 (65%)]\tLoss: 2.008764\n",
      "Train Epoch: 1 [42240/64000 (66%)]\tLoss: 2.002944\n",
      "Train Epoch: 1 [42880/64000 (67%)]\tLoss: 1.885973\n",
      "Train Epoch: 1 [43520/64000 (68%)]\tLoss: 1.925830\n",
      "Train Epoch: 1 [44160/64000 (69%)]\tLoss: 1.934839\n",
      "Train Epoch: 1 [44800/64000 (70%)]\tLoss: 1.921103\n",
      "Train Epoch: 1 [45440/64000 (71%)]\tLoss: 1.939666\n",
      "Train Epoch: 1 [46080/64000 (72%)]\tLoss: 1.830955\n",
      "Train Epoch: 1 [46720/64000 (73%)]\tLoss: 1.887471\n",
      "Train Epoch: 1 [47360/64000 (74%)]\tLoss: 1.775823\n",
      "Train Epoch: 1 [48000/64000 (75%)]\tLoss: 1.813040\n",
      "Train Epoch: 1 [48640/64000 (76%)]\tLoss: 1.777577\n",
      "Train Epoch: 1 [49280/64000 (77%)]\tLoss: 1.725481\n",
      "Train Epoch: 1 [49920/64000 (78%)]\tLoss: 1.734774\n",
      "Train Epoch: 1 [50560/64000 (79%)]\tLoss: 1.647576\n",
      "Train Epoch: 1 [51200/64000 (80%)]\tLoss: 1.668920\n",
      "Train Epoch: 1 [51840/64000 (81%)]\tLoss: 1.608628\n",
      "Train Epoch: 1 [52480/64000 (82%)]\tLoss: 1.543985\n",
      "Train Epoch: 1 [53120/64000 (83%)]\tLoss: 1.613791\n",
      "Train Epoch: 1 [53760/64000 (84%)]\tLoss: 1.513813\n",
      "Train Epoch: 1 [54400/64000 (85%)]\tLoss: 1.489631\n",
      "Train Epoch: 1 [55040/64000 (86%)]\tLoss: 1.441575\n",
      "Train Epoch: 1 [55680/64000 (87%)]\tLoss: 1.487191\n",
      "Train Epoch: 1 [56320/64000 (88%)]\tLoss: 1.380740\n",
      "Train Epoch: 1 [56960/64000 (89%)]\tLoss: 1.380938\n",
      "Train Epoch: 1 [57600/64000 (90%)]\tLoss: 1.414455\n",
      "Train Epoch: 1 [58240/64000 (91%)]\tLoss: 1.334579\n",
      "Train Epoch: 1 [58880/64000 (92%)]\tLoss: 1.281739\n",
      "Train Epoch: 1 [59520/64000 (93%)]\tLoss: 1.298969\n",
      "Train Epoch: 1 [60160/64000 (94%)]\tLoss: 1.242726\n",
      "Train Epoch: 1 [60800/64000 (95%)]\tLoss: 1.234887\n",
      "Train Epoch: 1 [61440/64000 (96%)]\tLoss: 1.238203\n",
      "Train Epoch: 1 [62080/64000 (97%)]\tLoss: 1.183572\n",
      "Train Epoch: 1 [62720/64000 (98%)]\tLoss: 1.101431\n",
      "Train Epoch: 1 [63360/64000 (99%)]\tLoss: 1.177217\n",
      "\n",
      "Train Epoch: Average Loss: 2.003782\n",
      "\n",
      "Test set: Average loss: 1.1412 Average accuracy: 0.1857\n",
      "\n",
      "Train Epoch: 2 [0/64000 (0%)]\tLoss: 1.134093\n",
      "Train Epoch: 2 [640/64000 (1%)]\tLoss: 1.101826\n",
      "Train Epoch: 2 [1280/64000 (2%)]\tLoss: 0.940402\n",
      "Train Epoch: 2 [1920/64000 (3%)]\tLoss: 1.039512\n",
      "Train Epoch: 2 [2560/64000 (4%)]\tLoss: 1.008822\n",
      "Train Epoch: 2 [3200/64000 (5%)]\tLoss: 0.978504\n",
      "Train Epoch: 2 [3840/64000 (6%)]\tLoss: 0.980656\n",
      "Train Epoch: 2 [4480/64000 (7%)]\tLoss: 0.984070\n",
      "Train Epoch: 2 [5120/64000 (8%)]\tLoss: 1.034192\n",
      "Train Epoch: 2 [5760/64000 (9%)]\tLoss: 0.906084\n",
      "Train Epoch: 2 [6400/64000 (10%)]\tLoss: 0.899275\n",
      "Train Epoch: 2 [7040/64000 (11%)]\tLoss: 0.848375\n",
      "Train Epoch: 2 [7680/64000 (12%)]\tLoss: 0.839114\n",
      "Train Epoch: 2 [8320/64000 (13%)]\tLoss: 0.770630\n",
      "Train Epoch: 2 [8960/64000 (14%)]\tLoss: 0.778177\n",
      "Train Epoch: 2 [9600/64000 (15%)]\tLoss: 0.805289\n",
      "Train Epoch: 2 [10240/64000 (16%)]\tLoss: 0.766090\n",
      "Train Epoch: 2 [10880/64000 (17%)]\tLoss: 0.792719\n",
      "Train Epoch: 2 [11520/64000 (18%)]\tLoss: 0.772283\n",
      "Train Epoch: 2 [12160/64000 (19%)]\tLoss: 0.784380\n",
      "Train Epoch: 2 [12800/64000 (20%)]\tLoss: 0.646357\n",
      "Train Epoch: 2 [13440/64000 (21%)]\tLoss: 0.721387\n",
      "Train Epoch: 2 [14080/64000 (22%)]\tLoss: 0.654935\n",
      "Train Epoch: 2 [14720/64000 (23%)]\tLoss: 0.631270\n",
      "Train Epoch: 2 [15360/64000 (24%)]\tLoss: 0.627963\n",
      "Train Epoch: 2 [16000/64000 (25%)]\tLoss: 0.606526\n",
      "Train Epoch: 2 [16640/64000 (26%)]\tLoss: 0.576396\n",
      "Train Epoch: 2 [17280/64000 (27%)]\tLoss: 0.562039\n",
      "Train Epoch: 2 [17920/64000 (28%)]\tLoss: 0.656887\n",
      "Train Epoch: 2 [18560/64000 (29%)]\tLoss: 0.643496\n",
      "Train Epoch: 2 [19200/64000 (30%)]\tLoss: 0.514824\n",
      "Train Epoch: 2 [19840/64000 (31%)]\tLoss: 0.588024\n",
      "Train Epoch: 2 [20480/64000 (32%)]\tLoss: 0.501511\n",
      "Train Epoch: 2 [21120/64000 (33%)]\tLoss: 0.611246\n",
      "Train Epoch: 2 [21760/64000 (34%)]\tLoss: 0.685272\n",
      "Train Epoch: 2 [22400/64000 (35%)]\tLoss: 0.481059\n",
      "Train Epoch: 2 [23040/64000 (36%)]\tLoss: 0.522111\n",
      "Train Epoch: 2 [23680/64000 (37%)]\tLoss: 0.565740\n",
      "Train Epoch: 2 [24320/64000 (38%)]\tLoss: 0.598652\n",
      "Train Epoch: 2 [24960/64000 (39%)]\tLoss: 0.485584\n",
      "Train Epoch: 2 [25600/64000 (40%)]\tLoss: 0.654189\n",
      "Train Epoch: 2 [26240/64000 (41%)]\tLoss: 0.471714\n",
      "Train Epoch: 2 [26880/64000 (42%)]\tLoss: 0.477488\n",
      "Train Epoch: 2 [27520/64000 (43%)]\tLoss: 0.502993\n",
      "Train Epoch: 2 [28160/64000 (44%)]\tLoss: 0.430571\n",
      "Train Epoch: 2 [28800/64000 (45%)]\tLoss: 0.384060\n",
      "Train Epoch: 2 [29440/64000 (46%)]\tLoss: 0.385709\n",
      "Train Epoch: 2 [30080/64000 (47%)]\tLoss: 0.392821\n",
      "Train Epoch: 2 [30720/64000 (48%)]\tLoss: 0.427168\n",
      "Train Epoch: 2 [31360/64000 (49%)]\tLoss: 0.464877\n",
      "Train Epoch: 2 [32000/64000 (50%)]\tLoss: 0.421424\n",
      "Train Epoch: 2 [32640/64000 (51%)]\tLoss: 0.422904\n",
      "Train Epoch: 2 [33280/64000 (52%)]\tLoss: 0.482227\n",
      "Train Epoch: 2 [33920/64000 (53%)]\tLoss: 0.435590\n",
      "Train Epoch: 2 [34560/64000 (54%)]\tLoss: 0.447106\n",
      "Train Epoch: 2 [35200/64000 (55%)]\tLoss: 0.376401\n",
      "Train Epoch: 2 [35840/64000 (56%)]\tLoss: 0.474910\n",
      "Train Epoch: 2 [36480/64000 (57%)]\tLoss: 0.359741\n",
      "Train Epoch: 2 [37120/64000 (58%)]\tLoss: 0.324826\n",
      "Train Epoch: 2 [37760/64000 (59%)]\tLoss: 0.453680\n",
      "Train Epoch: 2 [38400/64000 (60%)]\tLoss: 0.324858\n",
      "Train Epoch: 2 [39040/64000 (61%)]\tLoss: 0.301921\n",
      "Train Epoch: 2 [39680/64000 (62%)]\tLoss: 0.393192\n",
      "Train Epoch: 2 [40320/64000 (63%)]\tLoss: 0.287494\n",
      "Train Epoch: 2 [40960/64000 (64%)]\tLoss: 0.376235\n",
      "Train Epoch: 2 [41600/64000 (65%)]\tLoss: 0.447356\n",
      "Train Epoch: 2 [42240/64000 (66%)]\tLoss: 0.240009\n",
      "Train Epoch: 2 [42880/64000 (67%)]\tLoss: 0.394883\n",
      "Train Epoch: 2 [43520/64000 (68%)]\tLoss: 0.483587\n",
      "Train Epoch: 2 [44160/64000 (69%)]\tLoss: 0.331939\n",
      "Train Epoch: 2 [44800/64000 (70%)]\tLoss: 0.296273\n",
      "Train Epoch: 2 [45440/64000 (71%)]\tLoss: 0.264351\n",
      "Train Epoch: 2 [46080/64000 (72%)]\tLoss: 0.359021\n",
      "Train Epoch: 2 [46720/64000 (73%)]\tLoss: 0.398629\n",
      "Train Epoch: 2 [47360/64000 (74%)]\tLoss: 0.369105\n",
      "Train Epoch: 2 [48000/64000 (75%)]\tLoss: 0.229385\n",
      "Train Epoch: 2 [48640/64000 (76%)]\tLoss: 0.328400\n",
      "Train Epoch: 2 [49280/64000 (77%)]\tLoss: 0.271323\n",
      "Train Epoch: 2 [49920/64000 (78%)]\tLoss: 0.284173\n",
      "Train Epoch: 2 [50560/64000 (79%)]\tLoss: 0.274595\n",
      "Train Epoch: 2 [51200/64000 (80%)]\tLoss: 0.267110\n",
      "Train Epoch: 2 [51840/64000 (81%)]\tLoss: 0.327567\n",
      "Train Epoch: 2 [52480/64000 (82%)]\tLoss: 0.318247\n",
      "Train Epoch: 2 [53120/64000 (83%)]\tLoss: 0.305315\n",
      "Train Epoch: 2 [53760/64000 (84%)]\tLoss: 0.271598\n",
      "Train Epoch: 2 [54400/64000 (85%)]\tLoss: 0.309251\n",
      "Train Epoch: 2 [55040/64000 (86%)]\tLoss: 0.223163\n",
      "Train Epoch: 2 [55680/64000 (87%)]\tLoss: 0.336323\n",
      "Train Epoch: 2 [56320/64000 (88%)]\tLoss: 0.263938\n",
      "Train Epoch: 2 [56960/64000 (89%)]\tLoss: 0.278955\n",
      "Train Epoch: 2 [57600/64000 (90%)]\tLoss: 0.188317\n",
      "Train Epoch: 2 [58240/64000 (91%)]\tLoss: 0.353329\n",
      "Train Epoch: 2 [58880/64000 (92%)]\tLoss: 0.211925\n",
      "Train Epoch: 2 [59520/64000 (93%)]\tLoss: 0.231431\n",
      "Train Epoch: 2 [60160/64000 (94%)]\tLoss: 0.329048\n",
      "Train Epoch: 2 [60800/64000 (95%)]\tLoss: 0.220125\n",
      "Train Epoch: 2 [61440/64000 (96%)]\tLoss: 0.260288\n",
      "Train Epoch: 2 [62080/64000 (97%)]\tLoss: 0.285883\n",
      "Train Epoch: 2 [62720/64000 (98%)]\tLoss: 0.198664\n",
      "Train Epoch: 2 [63360/64000 (99%)]\tLoss: 0.271382\n",
      "\n",
      "Train Epoch: Average Loss: 0.510255\n",
      "\n",
      "Test set: Average loss: 0.4052 Average accuracy: 0.6485\n",
      "\n",
      "Train Epoch: 3 [0/64000 (0%)]\tLoss: 0.235456\n",
      "Train Epoch: 3 [640/64000 (1%)]\tLoss: 0.207395\n",
      "Train Epoch: 3 [1280/64000 (2%)]\tLoss: 0.217533\n",
      "Train Epoch: 3 [1920/64000 (3%)]\tLoss: 0.174188\n",
      "Train Epoch: 3 [2560/64000 (4%)]\tLoss: 0.214853\n",
      "Train Epoch: 3 [3200/64000 (5%)]\tLoss: 0.262083\n",
      "Train Epoch: 3 [3840/64000 (6%)]\tLoss: 0.196567\n",
      "Train Epoch: 3 [4480/64000 (7%)]\tLoss: 0.232013\n",
      "Train Epoch: 3 [5120/64000 (8%)]\tLoss: 0.158617\n",
      "Train Epoch: 3 [5760/64000 (9%)]\tLoss: 0.199233\n",
      "Train Epoch: 3 [6400/64000 (10%)]\tLoss: 0.338982\n",
      "Train Epoch: 3 [7040/64000 (11%)]\tLoss: 0.199379\n",
      "Train Epoch: 3 [7680/64000 (12%)]\tLoss: 0.224991\n",
      "Train Epoch: 3 [8320/64000 (13%)]\tLoss: 0.225015\n",
      "Train Epoch: 3 [8960/64000 (14%)]\tLoss: 0.268769\n",
      "Train Epoch: 3 [9600/64000 (15%)]\tLoss: 0.209530\n",
      "Train Epoch: 3 [10240/64000 (16%)]\tLoss: 0.135311\n",
      "Train Epoch: 3 [10880/64000 (17%)]\tLoss: 0.150803\n",
      "Train Epoch: 3 [11520/64000 (18%)]\tLoss: 0.263465\n",
      "Train Epoch: 3 [12160/64000 (19%)]\tLoss: 0.248058\n",
      "Train Epoch: 3 [12800/64000 (20%)]\tLoss: 0.249450\n",
      "Train Epoch: 3 [13440/64000 (21%)]\tLoss: 0.181012\n",
      "Train Epoch: 3 [14080/64000 (22%)]\tLoss: 0.259523\n",
      "Train Epoch: 3 [14720/64000 (23%)]\tLoss: 0.171014\n",
      "Train Epoch: 3 [15360/64000 (24%)]\tLoss: 0.195522\n",
      "Train Epoch: 3 [16000/64000 (25%)]\tLoss: 0.215642\n",
      "Train Epoch: 3 [16640/64000 (26%)]\tLoss: 0.239024\n",
      "Train Epoch: 3 [17280/64000 (27%)]\tLoss: 0.242955\n",
      "Train Epoch: 3 [17920/64000 (28%)]\tLoss: 0.167156\n",
      "Train Epoch: 3 [18560/64000 (29%)]\tLoss: 0.142467\n",
      "Train Epoch: 3 [19200/64000 (30%)]\tLoss: 0.213508\n",
      "Train Epoch: 3 [19840/64000 (31%)]\tLoss: 0.234362\n",
      "Train Epoch: 3 [20480/64000 (32%)]\tLoss: 0.175187\n",
      "Train Epoch: 3 [21120/64000 (33%)]\tLoss: 0.257432\n",
      "Train Epoch: 3 [21760/64000 (34%)]\tLoss: 0.196534\n",
      "Train Epoch: 3 [22400/64000 (35%)]\tLoss: 0.191995\n",
      "Train Epoch: 3 [23040/64000 (36%)]\tLoss: 0.233309\n",
      "Train Epoch: 3 [23680/64000 (37%)]\tLoss: 0.199066\n",
      "Train Epoch: 3 [24320/64000 (38%)]\tLoss: 0.194269\n",
      "Train Epoch: 3 [24960/64000 (39%)]\tLoss: 0.203822\n",
      "Train Epoch: 3 [25600/64000 (40%)]\tLoss: 0.211515\n",
      "Train Epoch: 3 [26240/64000 (41%)]\tLoss: 0.159144\n",
      "Train Epoch: 3 [26880/64000 (42%)]\tLoss: 0.173483\n",
      "Train Epoch: 3 [27520/64000 (43%)]\tLoss: 0.167040\n",
      "Train Epoch: 3 [28160/64000 (44%)]\tLoss: 0.165156\n",
      "Train Epoch: 3 [28800/64000 (45%)]\tLoss: 0.161984\n",
      "Train Epoch: 3 [29440/64000 (46%)]\tLoss: 0.208974\n",
      "Train Epoch: 3 [30080/64000 (47%)]\tLoss: 0.134983\n",
      "Train Epoch: 3 [30720/64000 (48%)]\tLoss: 0.138035\n",
      "Train Epoch: 3 [31360/64000 (49%)]\tLoss: 0.135188\n",
      "Train Epoch: 3 [32000/64000 (50%)]\tLoss: 0.167771\n",
      "Train Epoch: 3 [32640/64000 (51%)]\tLoss: 0.204097\n",
      "Train Epoch: 3 [33280/64000 (52%)]\tLoss: 0.275148\n",
      "Train Epoch: 3 [33920/64000 (53%)]\tLoss: 0.155147\n",
      "Train Epoch: 3 [34560/64000 (54%)]\tLoss: 0.175342\n",
      "Train Epoch: 3 [35200/64000 (55%)]\tLoss: 0.152692\n",
      "Train Epoch: 3 [35840/64000 (56%)]\tLoss: 0.228676\n",
      "Train Epoch: 3 [36480/64000 (57%)]\tLoss: 0.185331\n",
      "Train Epoch: 3 [37120/64000 (58%)]\tLoss: 0.195868\n",
      "Train Epoch: 3 [37760/64000 (59%)]\tLoss: 0.206094\n",
      "Train Epoch: 3 [38400/64000 (60%)]\tLoss: 0.231002\n",
      "Train Epoch: 3 [39040/64000 (61%)]\tLoss: 0.211377\n",
      "Train Epoch: 3 [39680/64000 (62%)]\tLoss: 0.215528\n",
      "Train Epoch: 3 [40320/64000 (63%)]\tLoss: 0.311289\n",
      "Train Epoch: 3 [40960/64000 (64%)]\tLoss: 0.209583\n",
      "Train Epoch: 3 [41600/64000 (65%)]\tLoss: 0.236811\n",
      "Train Epoch: 3 [42240/64000 (66%)]\tLoss: 0.157044\n",
      "Train Epoch: 3 [42880/64000 (67%)]\tLoss: 0.277839\n",
      "Train Epoch: 3 [43520/64000 (68%)]\tLoss: 0.161662\n",
      "Train Epoch: 3 [44160/64000 (69%)]\tLoss: 0.202551\n",
      "Train Epoch: 3 [44800/64000 (70%)]\tLoss: 0.162891\n",
      "Train Epoch: 3 [45440/64000 (71%)]\tLoss: 0.215339\n",
      "Train Epoch: 3 [46080/64000 (72%)]\tLoss: 0.175471\n",
      "Train Epoch: 3 [46720/64000 (73%)]\tLoss: 0.176511\n",
      "Train Epoch: 3 [47360/64000 (74%)]\tLoss: 0.171501\n",
      "Train Epoch: 3 [48000/64000 (75%)]\tLoss: 0.115260\n",
      "Train Epoch: 3 [48640/64000 (76%)]\tLoss: 0.106231\n",
      "Train Epoch: 3 [49280/64000 (77%)]\tLoss: 0.110885\n",
      "Train Epoch: 3 [49920/64000 (78%)]\tLoss: 0.174975\n",
      "Train Epoch: 3 [50560/64000 (79%)]\tLoss: 0.192457\n",
      "Train Epoch: 3 [51200/64000 (80%)]\tLoss: 0.200980\n",
      "Train Epoch: 3 [51840/64000 (81%)]\tLoss: 0.153136\n",
      "Train Epoch: 3 [52480/64000 (82%)]\tLoss: 0.128843\n",
      "Train Epoch: 3 [53120/64000 (83%)]\tLoss: 0.200636\n",
      "Train Epoch: 3 [53760/64000 (84%)]\tLoss: 0.181607\n",
      "Train Epoch: 3 [54400/64000 (85%)]\tLoss: 0.219464\n",
      "Train Epoch: 3 [55040/64000 (86%)]\tLoss: 0.179387\n",
      "Train Epoch: 3 [55680/64000 (87%)]\tLoss: 0.126160\n",
      "Train Epoch: 3 [56320/64000 (88%)]\tLoss: 0.158740\n",
      "Train Epoch: 3 [56960/64000 (89%)]\tLoss: 0.210780\n",
      "Train Epoch: 3 [57600/64000 (90%)]\tLoss: 0.151191\n",
      "Train Epoch: 3 [58240/64000 (91%)]\tLoss: 0.130279\n",
      "Train Epoch: 3 [58880/64000 (92%)]\tLoss: 0.218103\n",
      "Train Epoch: 3 [59520/64000 (93%)]\tLoss: 0.178834\n",
      "Train Epoch: 3 [60160/64000 (94%)]\tLoss: 0.212154\n",
      "Train Epoch: 3 [60800/64000 (95%)]\tLoss: 0.176613\n",
      "Train Epoch: 3 [61440/64000 (96%)]\tLoss: 0.232363\n",
      "Train Epoch: 3 [62080/64000 (97%)]\tLoss: 0.187130\n",
      "Train Epoch: 3 [62720/64000 (98%)]\tLoss: 0.181905\n",
      "Train Epoch: 3 [63360/64000 (99%)]\tLoss: 0.132160\n",
      "\n",
      "Train Epoch: Average Loss: 0.198547\n",
      "\n",
      "Test set: Average loss: 0.3241 Average accuracy: 0.7251\n",
      "\n",
      "Train Epoch: 4 [0/64000 (0%)]\tLoss: 0.193103\n",
      "Train Epoch: 4 [640/64000 (1%)]\tLoss: 0.205390\n",
      "Train Epoch: 4 [1280/64000 (2%)]\tLoss: 0.128593\n",
      "Train Epoch: 4 [1920/64000 (3%)]\tLoss: 0.176079\n",
      "Train Epoch: 4 [2560/64000 (4%)]\tLoss: 0.142366\n",
      "Train Epoch: 4 [3200/64000 (5%)]\tLoss: 0.124688\n",
      "Train Epoch: 4 [3840/64000 (6%)]\tLoss: 0.133722\n",
      "Train Epoch: 4 [4480/64000 (7%)]\tLoss: 0.118267\n",
      "Train Epoch: 4 [5120/64000 (8%)]\tLoss: 0.166405\n",
      "Train Epoch: 4 [5760/64000 (9%)]\tLoss: 0.136598\n",
      "Train Epoch: 4 [6400/64000 (10%)]\tLoss: 0.078700\n",
      "Train Epoch: 4 [7040/64000 (11%)]\tLoss: 0.110999\n",
      "Train Epoch: 4 [7680/64000 (12%)]\tLoss: 0.154187\n",
      "Train Epoch: 4 [8320/64000 (13%)]\tLoss: 0.135918\n",
      "Train Epoch: 4 [8960/64000 (14%)]\tLoss: 0.179808\n",
      "Train Epoch: 4 [9600/64000 (15%)]\tLoss: 0.163561\n",
      "Train Epoch: 4 [10240/64000 (16%)]\tLoss: 0.177508\n",
      "Train Epoch: 4 [10880/64000 (17%)]\tLoss: 0.110087\n",
      "Train Epoch: 4 [11520/64000 (18%)]\tLoss: 0.166406\n",
      "Train Epoch: 4 [12160/64000 (19%)]\tLoss: 0.150981\n",
      "Train Epoch: 4 [12800/64000 (20%)]\tLoss: 0.133855\n",
      "Train Epoch: 4 [13440/64000 (21%)]\tLoss: 0.079848\n",
      "Train Epoch: 4 [14080/64000 (22%)]\tLoss: 0.132129\n",
      "Train Epoch: 4 [14720/64000 (23%)]\tLoss: 0.171130\n",
      "Train Epoch: 4 [15360/64000 (24%)]\tLoss: 0.109607\n",
      "Train Epoch: 4 [16000/64000 (25%)]\tLoss: 0.098921\n",
      "Train Epoch: 4 [16640/64000 (26%)]\tLoss: 0.139991\n",
      "Train Epoch: 4 [17280/64000 (27%)]\tLoss: 0.104606\n",
      "Train Epoch: 4 [17920/64000 (28%)]\tLoss: 0.261633\n",
      "Train Epoch: 4 [18560/64000 (29%)]\tLoss: 0.205674\n",
      "Train Epoch: 4 [19200/64000 (30%)]\tLoss: 0.119295\n",
      "Train Epoch: 4 [19840/64000 (31%)]\tLoss: 0.124201\n",
      "Train Epoch: 4 [20480/64000 (32%)]\tLoss: 0.173229\n",
      "Train Epoch: 4 [21120/64000 (33%)]\tLoss: 0.127133\n",
      "Train Epoch: 4 [21760/64000 (34%)]\tLoss: 0.109783\n",
      "Train Epoch: 4 [22400/64000 (35%)]\tLoss: 0.112978\n",
      "Train Epoch: 4 [23040/64000 (36%)]\tLoss: 0.236975\n",
      "Train Epoch: 4 [23680/64000 (37%)]\tLoss: 0.156115\n",
      "Train Epoch: 4 [24320/64000 (38%)]\tLoss: 0.148975\n",
      "Train Epoch: 4 [24960/64000 (39%)]\tLoss: 0.136452\n",
      "Train Epoch: 4 [25600/64000 (40%)]\tLoss: 0.155157\n",
      "Train Epoch: 4 [26240/64000 (41%)]\tLoss: 0.127477\n",
      "Train Epoch: 4 [26880/64000 (42%)]\tLoss: 0.081056\n",
      "Train Epoch: 4 [27520/64000 (43%)]\tLoss: 0.096346\n",
      "Train Epoch: 4 [28160/64000 (44%)]\tLoss: 0.106581\n",
      "Train Epoch: 4 [28800/64000 (45%)]\tLoss: 0.148654\n",
      "Train Epoch: 4 [29440/64000 (46%)]\tLoss: 0.150874\n",
      "Train Epoch: 4 [30080/64000 (47%)]\tLoss: 0.184037\n",
      "Train Epoch: 4 [30720/64000 (48%)]\tLoss: 0.103396\n",
      "Train Epoch: 4 [31360/64000 (49%)]\tLoss: 0.181726\n",
      "Train Epoch: 4 [32000/64000 (50%)]\tLoss: 0.158231\n",
      "Train Epoch: 4 [32640/64000 (51%)]\tLoss: 0.139662\n",
      "Train Epoch: 4 [33280/64000 (52%)]\tLoss: 0.103738\n",
      "Train Epoch: 4 [33920/64000 (53%)]\tLoss: 0.116840\n",
      "Train Epoch: 4 [34560/64000 (54%)]\tLoss: 0.074063\n",
      "Train Epoch: 4 [35200/64000 (55%)]\tLoss: 0.173385\n",
      "Train Epoch: 4 [35840/64000 (56%)]\tLoss: 0.152168\n",
      "Train Epoch: 4 [36480/64000 (57%)]\tLoss: 0.083543\n",
      "Train Epoch: 4 [37120/64000 (58%)]\tLoss: 0.119313\n",
      "Train Epoch: 4 [37760/64000 (59%)]\tLoss: 0.124816\n",
      "Train Epoch: 4 [38400/64000 (60%)]\tLoss: 0.164708\n",
      "Train Epoch: 4 [39040/64000 (61%)]\tLoss: 0.082297\n",
      "Train Epoch: 4 [39680/64000 (62%)]\tLoss: 0.179456\n",
      "Train Epoch: 4 [40320/64000 (63%)]\tLoss: 0.104116\n",
      "Train Epoch: 4 [40960/64000 (64%)]\tLoss: 0.130971\n",
      "Train Epoch: 4 [41600/64000 (65%)]\tLoss: 0.100544\n",
      "Train Epoch: 4 [42240/64000 (66%)]\tLoss: 0.119422\n",
      "Train Epoch: 4 [42880/64000 (67%)]\tLoss: 0.147316\n",
      "Train Epoch: 4 [43520/64000 (68%)]\tLoss: 0.082120\n",
      "Train Epoch: 4 [44160/64000 (69%)]\tLoss: 0.095664\n",
      "Train Epoch: 4 [44800/64000 (70%)]\tLoss: 0.068040\n",
      "Train Epoch: 4 [45440/64000 (71%)]\tLoss: 0.121865\n",
      "Train Epoch: 4 [46080/64000 (72%)]\tLoss: 0.097830\n",
      "Train Epoch: 4 [46720/64000 (73%)]\tLoss: 0.137210\n",
      "Train Epoch: 4 [47360/64000 (74%)]\tLoss: 0.143276\n",
      "Train Epoch: 4 [48000/64000 (75%)]\tLoss: 0.111144\n",
      "Train Epoch: 4 [48640/64000 (76%)]\tLoss: 0.144420\n",
      "Train Epoch: 4 [49280/64000 (77%)]\tLoss: 0.127754\n",
      "Train Epoch: 4 [49920/64000 (78%)]\tLoss: 0.102976\n",
      "Train Epoch: 4 [50560/64000 (79%)]\tLoss: 0.194660\n",
      "Train Epoch: 4 [51200/64000 (80%)]\tLoss: 0.106808\n",
      "Train Epoch: 4 [51840/64000 (81%)]\tLoss: 0.104577\n",
      "Train Epoch: 4 [52480/64000 (82%)]\tLoss: 0.081183\n",
      "Train Epoch: 4 [53120/64000 (83%)]\tLoss: 0.147429\n",
      "Train Epoch: 4 [53760/64000 (84%)]\tLoss: 0.235676\n",
      "Train Epoch: 4 [54400/64000 (85%)]\tLoss: 0.195532\n",
      "Train Epoch: 4 [55040/64000 (86%)]\tLoss: 0.093144\n",
      "Train Epoch: 4 [55680/64000 (87%)]\tLoss: 0.166470\n",
      "Train Epoch: 4 [56320/64000 (88%)]\tLoss: 0.145668\n",
      "Train Epoch: 4 [56960/64000 (89%)]\tLoss: 0.115115\n",
      "Train Epoch: 4 [57600/64000 (90%)]\tLoss: 0.089317\n",
      "Train Epoch: 4 [58240/64000 (91%)]\tLoss: 0.123607\n",
      "Train Epoch: 4 [58880/64000 (92%)]\tLoss: 0.083991\n",
      "Train Epoch: 4 [59520/64000 (93%)]\tLoss: 0.099732\n",
      "Train Epoch: 4 [60160/64000 (94%)]\tLoss: 0.112466\n",
      "Train Epoch: 4 [60800/64000 (95%)]\tLoss: 0.185430\n",
      "Train Epoch: 4 [61440/64000 (96%)]\tLoss: 0.111830\n",
      "Train Epoch: 4 [62080/64000 (97%)]\tLoss: 0.164451\n",
      "Train Epoch: 4 [62720/64000 (98%)]\tLoss: 0.173072\n",
      "Train Epoch: 4 [63360/64000 (99%)]\tLoss: 0.136012\n",
      "\n",
      "Train Epoch: Average Loss: 0.141257\n",
      "\n",
      "Test set: Average loss: 0.1577 Average accuracy: 0.8619\n",
      "\n",
      "Train Epoch: 5 [0/64000 (0%)]\tLoss: 0.188791\n",
      "Train Epoch: 5 [640/64000 (1%)]\tLoss: 0.082391\n",
      "Train Epoch: 5 [1280/64000 (2%)]\tLoss: 0.121937\n",
      "Train Epoch: 5 [1920/64000 (3%)]\tLoss: 0.087212\n",
      "Train Epoch: 5 [2560/64000 (4%)]\tLoss: 0.120006\n",
      "Train Epoch: 5 [3200/64000 (5%)]\tLoss: 0.074934\n",
      "Train Epoch: 5 [3840/64000 (6%)]\tLoss: 0.094431\n",
      "Train Epoch: 5 [4480/64000 (7%)]\tLoss: 0.111847\n",
      "Train Epoch: 5 [5120/64000 (8%)]\tLoss: 0.105617\n",
      "Train Epoch: 5 [5760/64000 (9%)]\tLoss: 0.066305\n",
      "Train Epoch: 5 [6400/64000 (10%)]\tLoss: 0.086271\n",
      "Train Epoch: 5 [7040/64000 (11%)]\tLoss: 0.115406\n",
      "Train Epoch: 5 [7680/64000 (12%)]\tLoss: 0.080630\n",
      "Train Epoch: 5 [8320/64000 (13%)]\tLoss: 0.110964\n",
      "Train Epoch: 5 [8960/64000 (14%)]\tLoss: 0.188040\n",
      "Train Epoch: 5 [9600/64000 (15%)]\tLoss: 0.153903\n",
      "Train Epoch: 5 [10240/64000 (16%)]\tLoss: 0.125079\n",
      "Train Epoch: 5 [10880/64000 (17%)]\tLoss: 0.077824\n",
      "Train Epoch: 5 [11520/64000 (18%)]\tLoss: 0.156280\n",
      "Train Epoch: 5 [12160/64000 (19%)]\tLoss: 0.060515\n",
      "Train Epoch: 5 [12800/64000 (20%)]\tLoss: 0.117184\n",
      "Train Epoch: 5 [13440/64000 (21%)]\tLoss: 0.088670\n",
      "Train Epoch: 5 [14080/64000 (22%)]\tLoss: 0.090598\n",
      "Train Epoch: 5 [14720/64000 (23%)]\tLoss: 0.067739\n",
      "Train Epoch: 5 [15360/64000 (24%)]\tLoss: 0.169382\n",
      "Train Epoch: 5 [16000/64000 (25%)]\tLoss: 0.115574\n",
      "Train Epoch: 5 [16640/64000 (26%)]\tLoss: 0.206748\n",
      "Train Epoch: 5 [17280/64000 (27%)]\tLoss: 0.108989\n",
      "Train Epoch: 5 [17920/64000 (28%)]\tLoss: 0.148831\n",
      "Train Epoch: 5 [18560/64000 (29%)]\tLoss: 0.077420\n",
      "Train Epoch: 5 [19200/64000 (30%)]\tLoss: 0.132810\n",
      "Train Epoch: 5 [19840/64000 (31%)]\tLoss: 0.115550\n",
      "Train Epoch: 5 [20480/64000 (32%)]\tLoss: 0.076300\n",
      "Train Epoch: 5 [21120/64000 (33%)]\tLoss: 0.154162\n",
      "Train Epoch: 5 [21760/64000 (34%)]\tLoss: 0.069649\n",
      "Train Epoch: 5 [22400/64000 (35%)]\tLoss: 0.077669\n",
      "Train Epoch: 5 [23040/64000 (36%)]\tLoss: 0.106363\n",
      "Train Epoch: 5 [23680/64000 (37%)]\tLoss: 0.133232\n",
      "Train Epoch: 5 [24320/64000 (38%)]\tLoss: 0.142645\n",
      "Train Epoch: 5 [24960/64000 (39%)]\tLoss: 0.080137\n",
      "Train Epoch: 5 [25600/64000 (40%)]\tLoss: 0.106244\n",
      "Train Epoch: 5 [26240/64000 (41%)]\tLoss: 0.139860\n",
      "Train Epoch: 5 [26880/64000 (42%)]\tLoss: 0.079288\n",
      "Train Epoch: 5 [27520/64000 (43%)]\tLoss: 0.154344\n",
      "Train Epoch: 5 [28160/64000 (44%)]\tLoss: 0.096527\n",
      "Train Epoch: 5 [28800/64000 (45%)]\tLoss: 0.068642\n",
      "Train Epoch: 5 [29440/64000 (46%)]\tLoss: 0.084216\n",
      "Train Epoch: 5 [30080/64000 (47%)]\tLoss: 0.037857\n",
      "Train Epoch: 5 [30720/64000 (48%)]\tLoss: 0.184391\n",
      "Train Epoch: 5 [31360/64000 (49%)]\tLoss: 0.123712\n",
      "Train Epoch: 5 [32000/64000 (50%)]\tLoss: 0.152882\n",
      "Train Epoch: 5 [32640/64000 (51%)]\tLoss: 0.079772\n",
      "Train Epoch: 5 [33280/64000 (52%)]\tLoss: 0.052461\n",
      "Train Epoch: 5 [33920/64000 (53%)]\tLoss: 0.104075\n",
      "Train Epoch: 5 [34560/64000 (54%)]\tLoss: 0.142301\n",
      "Train Epoch: 5 [35200/64000 (55%)]\tLoss: 0.149435\n",
      "Train Epoch: 5 [35840/64000 (56%)]\tLoss: 0.143959\n",
      "Train Epoch: 5 [36480/64000 (57%)]\tLoss: 0.088832\n",
      "Train Epoch: 5 [37120/64000 (58%)]\tLoss: 0.111554\n",
      "Train Epoch: 5 [37760/64000 (59%)]\tLoss: 0.105461\n",
      "Train Epoch: 5 [38400/64000 (60%)]\tLoss: 0.153385\n",
      "Train Epoch: 5 [39040/64000 (61%)]\tLoss: 0.129505\n",
      "Train Epoch: 5 [39680/64000 (62%)]\tLoss: 0.118281\n",
      "Train Epoch: 5 [40320/64000 (63%)]\tLoss: 0.079142\n",
      "Train Epoch: 5 [40960/64000 (64%)]\tLoss: 0.123770\n",
      "Train Epoch: 5 [41600/64000 (65%)]\tLoss: 0.108669\n",
      "Train Epoch: 5 [42240/64000 (66%)]\tLoss: 0.106890\n",
      "Train Epoch: 5 [42880/64000 (67%)]\tLoss: 0.088470\n",
      "Train Epoch: 5 [43520/64000 (68%)]\tLoss: 0.085693\n",
      "Train Epoch: 5 [44160/64000 (69%)]\tLoss: 0.243196\n",
      "Train Epoch: 5 [44800/64000 (70%)]\tLoss: 0.084844\n",
      "Train Epoch: 5 [45440/64000 (71%)]\tLoss: 0.072005\n",
      "Train Epoch: 5 [46080/64000 (72%)]\tLoss: 0.095089\n",
      "Train Epoch: 5 [46720/64000 (73%)]\tLoss: 0.052718\n",
      "Train Epoch: 5 [47360/64000 (74%)]\tLoss: 0.081421\n",
      "Train Epoch: 5 [48000/64000 (75%)]\tLoss: 0.081466\n",
      "Train Epoch: 5 [48640/64000 (76%)]\tLoss: 0.073189\n",
      "Train Epoch: 5 [49280/64000 (77%)]\tLoss: 0.093424\n",
      "Train Epoch: 5 [49920/64000 (78%)]\tLoss: 0.125364\n",
      "Train Epoch: 5 [50560/64000 (79%)]\tLoss: 0.093285\n",
      "Train Epoch: 5 [51200/64000 (80%)]\tLoss: 0.114965\n",
      "Train Epoch: 5 [51840/64000 (81%)]\tLoss: 0.127542\n",
      "Train Epoch: 5 [52480/64000 (82%)]\tLoss: 0.175441\n",
      "Train Epoch: 5 [53120/64000 (83%)]\tLoss: 0.113891\n",
      "Train Epoch: 5 [53760/64000 (84%)]\tLoss: 0.145595\n",
      "Train Epoch: 5 [54400/64000 (85%)]\tLoss: 0.060937\n",
      "Train Epoch: 5 [55040/64000 (86%)]\tLoss: 0.109166\n",
      "Train Epoch: 5 [55680/64000 (87%)]\tLoss: 0.136243\n",
      "Train Epoch: 5 [56320/64000 (88%)]\tLoss: 0.152230\n",
      "Train Epoch: 5 [56960/64000 (89%)]\tLoss: 0.140960\n",
      "Train Epoch: 5 [57600/64000 (90%)]\tLoss: 0.127852\n",
      "Train Epoch: 5 [58240/64000 (91%)]\tLoss: 0.074460\n",
      "Train Epoch: 5 [58880/64000 (92%)]\tLoss: 0.082661\n",
      "Train Epoch: 5 [59520/64000 (93%)]\tLoss: 0.084759\n",
      "Train Epoch: 5 [60160/64000 (94%)]\tLoss: 0.093284\n",
      "Train Epoch: 5 [60800/64000 (95%)]\tLoss: 0.067592\n",
      "Train Epoch: 5 [61440/64000 (96%)]\tLoss: 0.263742\n",
      "Train Epoch: 5 [62080/64000 (97%)]\tLoss: 0.095633\n",
      "Train Epoch: 5 [62720/64000 (98%)]\tLoss: 0.106950\n",
      "Train Epoch: 5 [63360/64000 (99%)]\tLoss: 0.155035\n",
      "\n",
      "Train Epoch: Average Loss: 0.113552\n",
      "\n",
      "Test set: Average loss: 0.1505 Average accuracy: 0.8681\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "early_stopping = EarlyStopping()\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch, loss_fn, None, 1, 2)\n",
    "    test_loss = test(model, device, test_loader, loss_fn, task2_accuracy, 1, 2)\n",
    "\n",
    "    if early_stopping(test_loss):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Questions\n",
    "1. What preprocessing techniques did you use? Why?\n",
    "    - *Answer*\n",
    "2. What data augmentation techniques did you use?\n",
    "    - *Answer*\n",
    "3. Describe the fine-tuning process and how you reached your final CNN model.\n",
    "    - *Answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Task 3: Decision Trees and Ensemble Learning (15%)\n",
    "\n",
    "For the `loan_data.csv` data, predict if the bank should give a loan or not.\n",
    "You need to do the following:\n",
    "- Fine-tune a decision tree on the data\n",
    "- Fine-tune a random forest on the data\n",
    "- Compare their performance\n",
    "- Visualize your DT and one of the trees from the RF\n",
    "\n",
    "For evaluating your models, do $80/20$ train test split.\n",
    "\n",
    "### Data\n",
    "- `credit.policy`: Whether the customer meets the credit underwriting criteria.\n",
    "- `purpose`: The purpose of the loan.\n",
    "- `int.rate`: The interest rate of the loan.\n",
    "- `installment`: The monthly installments owed by the borrower if the loan is funded.\n",
    "- `log.annual.inc`: The natural logarithm of the self-reported annual income of the borrower.\n",
    "- `dti`: The debt-to-income ratio of the borrower.\n",
    "- `fico`: The FICO credit score of the borrower.\n",
    "- `days.with.cr.line`: The number of days the borrower has had a credit line.\n",
    "- `revol.bal`: The borrower's revolving balance.\n",
    "- `revol.util`: The borrower's revolving line utilization rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Questions\n",
    "1. How did the DT compare to the RF in performance? Why?\n",
    "    - *Answer*\n",
    "2. After fine-tuning, how does the max depth in DT compare to RF? Why?\n",
    "    - *Answer*\n",
    "3. What is ensemble learning? What are its pros and cons?\n",
    "    - *Answer*\n",
    "4. Briefly explain 2 types of boosting methods and 2 types of bagging methods.\n",
    "Which of these categories does RF fall under?\n",
    "    - *Answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Task 4: Domain Gap (15%)\n",
    "\n",
    "Evaluate your CNN model from task 2 on SVHN data without retraining your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Questions\n",
    "1. How did your model perform? Why is it better/worse?\n",
    "    - *Answer*\n",
    "2. What is domain gap in the context of ML?\n",
    "    - *Answer*\n",
    "3. Suggest two ways through which the problem of domain gap can be tackled.\n",
    "    - *Answer*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
