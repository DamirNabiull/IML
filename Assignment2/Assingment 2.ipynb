{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Damir Nabiullin\n",
    "d.nabiullin@innopolis.university"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Assignment 2\n",
    "\n",
    "## Instructions\n",
    "- Your submission should be the `.ipynb` file with your name,\n",
    "  like `YusufMesbah.ipynb`. it should include the answers to the questions in\n",
    "  markdown cells.\n",
    "- You are expected to follow the best practices for code writing and model\n",
    "training. Poor coding style will be penalized.\n",
    "- You are allowed to discuss ideas with your peers, but no sharing of code.\n",
    "Plagiarism in the code will result in failing. If you use code from the\n",
    "internet, cite it.\n",
    "- If the instructions seem vague, use common sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dale\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports for all tasks\n",
    "import os\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import operator\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Task 1: ANN (30%)\n",
    "For this task, you are required to build a fully connect feed-forward ANN model\n",
    "for a multi-label regression problem.\n",
    "\n",
    "For the given data, you need do proper data preprocessing, design the ANN model,\n",
    "then fine-tune your model architecture (number of layers, number of neurons,\n",
    "activation function, learning rate, momentum, regularization).\n",
    "\n",
    "For evaluating your model, do $80/20$ train test split.\n",
    "\n",
    "### Data\n",
    "You will be working with the data in `Task 1.csv` for predicting students'\n",
    "scores in 3 different exams: math, reading and writing. The columns include:\n",
    " - gender\n",
    " - race\n",
    " - parental level of education\n",
    " - lunch meal plan at school\n",
    " - whether the student undertook the test preparation course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 1 - SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>race/ethnicity</th>\n",
       "      <th>parental level of education</th>\n",
       "      <th>lunch</th>\n",
       "      <th>test preparation course</th>\n",
       "      <th>math score</th>\n",
       "      <th>reading score</th>\n",
       "      <th>writing score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>group A</td>\n",
       "      <td>high school</td>\n",
       "      <td>standard</td>\n",
       "      <td>completed</td>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>group D</td>\n",
       "      <td>some high school</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>40</td>\n",
       "      <td>59</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>male</td>\n",
       "      <td>group E</td>\n",
       "      <td>some college</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>59</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male</td>\n",
       "      <td>group B</td>\n",
       "      <td>high school</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>77</td>\n",
       "      <td>78</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male</td>\n",
       "      <td>group E</td>\n",
       "      <td>associate's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>completed</td>\n",
       "      <td>78</td>\n",
       "      <td>73</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender race/ethnicity parental level of education         lunch  \\\n",
       "0    male        group A                 high school      standard   \n",
       "1  female        group D            some high school  free/reduced   \n",
       "2    male        group E                some college  free/reduced   \n",
       "3    male        group B                 high school      standard   \n",
       "4    male        group E          associate's degree      standard   \n",
       "\n",
       "  test preparation course  math score  reading score  writing score  \n",
       "0               completed          67             67             63  \n",
       "1                    none          40             59             55  \n",
       "2                    none          59             60             50  \n",
       "3                    none          77             78             68  \n",
       "4               completed          78             73             68  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data set\n",
    "data = pd.read_csv('Task 1.csv')\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check columns and prepare them for encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 ['male' 'female']\n",
      "2 ['standard' 'free/reduced']\n",
      "2 ['completed' 'none']\n",
      "5 ['group A' 'group B' 'group C' 'group D' 'group E']\n",
      "6 ['high school' 'some high school' 'some college' \"associate's degree\"\n",
      " \"bachelor's degree\" \"master's degree\"]\n",
      "\n",
      "Orginized parental level of education:  ['some high school', 'high school', 'some college', \"associate's degree\", \"bachelor's degree\", \"master's degree\"]\n"
     ]
    }
   ],
   "source": [
    "# Check count of distinct values in columns gender, lunch\n",
    "gender_col = data['gender'].unique()\n",
    "print(data['gender'].nunique(), gender_col)\n",
    "\n",
    "lunch_col = data['lunch'].unique()\n",
    "print(data['lunch'].nunique(), lunch_col)\n",
    "\n",
    "course_col = data['test preparation course'].unique()\n",
    "print(data['test preparation course'].nunique(), course_col)\n",
    "\n",
    "race_col = data['race/ethnicity'].unique()\n",
    "race_col.sort()\n",
    "print(data['race/ethnicity'].nunique(), race_col)\n",
    "\n",
    "level_col = data['parental level of education'].unique()\n",
    "print(data['parental level of education'].nunique(), level_col)\n",
    "\n",
    "# Array of orginized parental level of education\n",
    "level_col = ['some high school', 'high school', 'some college', 'associate\\'s degree', 'bachelor\\'s degree', 'master\\'s degree']\n",
    "\n",
    "print('\\nOrginized parental level of education: ', level_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create encoder\n",
    "ordinalEncoder = OrdinalEncoder(categories=[race_col, level_col])\n",
    "oneHotEncoder = OneHotEncoder()\n",
    "\n",
    "ordinal_features = ['race/ethnicity', 'parental level of education']\n",
    "one_hot_features = ['gender', 'lunch', 'test preparation course']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode ordinal features\n",
    "new_oe_features = ordinalEncoder.fit_transform(data[ordinal_features])\n",
    "new_oe_cols = pd.DataFrame(new_oe_features, dtype=int, columns=ordinal_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dale\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Encode one hot features\n",
    "new_ohe_features = oneHotEncoder.fit_transform(data[one_hot_features])\n",
    "new_ohe_cols = pd.DataFrame(new_ohe_features.toarray(), dtype=int, columns=oneHotEncoder.get_feature_names(one_hot_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine new columns into new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>math score</th>\n",
       "      <th>reading score</th>\n",
       "      <th>writing score</th>\n",
       "      <th>race/ethnicity</th>\n",
       "      <th>parental level of education</th>\n",
       "      <th>gender_female</th>\n",
       "      <th>gender_male</th>\n",
       "      <th>lunch_free/reduced</th>\n",
       "      <th>lunch_standard</th>\n",
       "      <th>test preparation course_completed</th>\n",
       "      <th>test preparation course_none</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40</td>\n",
       "      <td>59</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77</td>\n",
       "      <td>78</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>78</td>\n",
       "      <td>73</td>\n",
       "      <td>68</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   math score  reading score  writing score  race/ethnicity  \\\n",
       "0          67             67             63               0   \n",
       "1          40             59             55               3   \n",
       "2          59             60             50               4   \n",
       "3          77             78             68               1   \n",
       "4          78             73             68               4   \n",
       "\n",
       "   parental level of education  gender_female  gender_male  \\\n",
       "0                            1              0            1   \n",
       "1                            0              1            0   \n",
       "2                            2              0            1   \n",
       "3                            1              0            1   \n",
       "4                            3              0            1   \n",
       "\n",
       "   lunch_free/reduced  lunch_standard  test preparation course_completed  \\\n",
       "0                   0               1                                  1   \n",
       "1                   1               0                                  0   \n",
       "2                   1               0                                  0   \n",
       "3                   0               1                                  0   \n",
       "4                   0               1                                  1   \n",
       "\n",
       "   test preparation course_none  \n",
       "0                             0  \n",
       "1                             1  \n",
       "2                             1  \n",
       "3                             1  \n",
       "4                             0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine all features\n",
    "new_data = pd.concat([data], axis=1)\n",
    "new_data = new_data.drop(columns=ordinal_features + one_hot_features )\n",
    "new_data = pd.concat([new_data, new_oe_cols, new_ohe_cols], axis=1)\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data to target and features\n",
    "X = new_data.iloc[:, 3:].values\n",
    "y = new_data.iloc[:, :3].values\n",
    "\n",
    "# Split data to training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale testing and training data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Create Data Frames with scaled data\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scale target data\n",
    "# target_scaler = StandardScaler()\n",
    "# target_scaler.fit(y_train)\n",
    "\n",
    "# # Create Data Frames with scaled data\n",
    "# y_train = target_scaler.transform(y_train)\n",
    "# y_test = target_scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create custom dataset for pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super().__init__()\n",
    "        self.y = torch.tensor(y).float()\n",
    "        self.X = torch.tensor(X).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx, :], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "test_dataset = CustomDataset(X_test, y_test)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, label = next(iter(train_dataloader))\n",
    "label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ANN model and prepare device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"cuda\" if use_cuda else \"cpu\")\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN(\n",
      "  (hidden1): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (output): Linear(in_features=4, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Custom neural network class\n",
    "class ANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ANN, self).__init__()\n",
    "        self.hidden1 = nn.Linear(8, 4)\n",
    "        self.output = nn.Linear(4, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.hidden1(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Create neural network\n",
    "model = ANN().to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "lr = 0.001\n",
    "momentum = 0.9\n",
    "seed = 1\n",
    "log_interval = 4\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create functions for accuracy and debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compare first element in batch with the first target value \n",
    "def print_batch_element_error(batch, target):\n",
    "    # get value from tensor\n",
    "    batch = batch.cpu().data.numpy()\n",
    "    target = target.cpu().data.numpy()\n",
    "    # get first element from batch\n",
    "    predicted_v = batch[0]\n",
    "    target_v = target[0]\n",
    "    # print error\n",
    "    error = np.abs(predicted_v - target_v)\n",
    "    print('Predicted: ', predicted_v, 'Target: ', target_v, 'Error: ', error, '\\n')\n",
    "\n",
    "\n",
    "# Custom accuracy function\n",
    "def task1_accuracy(output, target, threshold=2):\n",
    "    # get value from tensor\n",
    "    output = output.cpu().data.numpy()\n",
    "    target = target.cpu().data.numpy()\n",
    "    accuracy = 0\n",
    "    for i in range(len(output)):\n",
    "        predicted_v = output[i]\n",
    "        target_v = target[i]\n",
    "        error = np.abs(predicted_v - target_v)\n",
    "        for j in range(len(error)):\n",
    "            if error[j] <= threshold:\n",
    "                error[j] = 1\n",
    "            else:\n",
    "                error[j] = 0\n",
    "        accuracy += np.sum(error)/3\n",
    "    \n",
    "    return accuracy/len(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on https://stackoverflow.com/questions/71998978/early-stopping-in-pytorch\n",
    "# And Lab_10\n",
    "class EarlyStopping():\n",
    "    def __init__(self, tolerance=5, min_delta=0, mode='min'):\n",
    "        # Set initial values\n",
    "        self.tolerance = tolerance\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.prev_metric = np.inf if mode == 'min' else -np.inf\n",
    "\n",
    "        self.operation = operator.gt if mode == 'min' else operator.lt\n",
    "\n",
    "    def __call__(self, metric):\n",
    "        # Calculate difference between current metric and previous metric\n",
    "        delta = (metric - self.prev_metric)\n",
    "\n",
    "        # Check if difference is greater than minimum delta\n",
    "        if self.operation(delta, self.min_delta):\n",
    "            self.counter +=1\n",
    "        else:\n",
    "            self.counter = 0\n",
    "            self.prev_metric = metric\n",
    "\n",
    "        # Check if counter is greater than tolerance\n",
    "        if self.counter >= self.tolerance:\n",
    "            self.early_stop = True\n",
    "        return self.early_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, loss_func, accuracy_func = None, task = 1):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    accuracy = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad() \n",
    "        output = model(data).squeeze()\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_func(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        if accuracy_func:\n",
    "            curr_accuracy = accuracy_func(output, target)\n",
    "            accuracy += curr_accuracy\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                        100. * batch_idx / len(train_loader), loss.item()), end='')\n",
    "            # Print accuracy if accuracy function is provided\n",
    "            if accuracy_func:\n",
    "                print('\\tAccuracy: {:.6f}'.format(curr_accuracy), end='')\n",
    "            print()\n",
    "\n",
    "    # Calculate average loss\n",
    "    if task == 1:\n",
    "        epoch_loss /= len(train_loader.dataset)\n",
    "    elif task == 2:\n",
    "        epoch_loss /= len(train_loader)\n",
    "\n",
    "    # Print average scores for epoch\n",
    "    print('\\nTrain Epoch: Average Loss: {:.6f}'.format(epoch_loss), \n",
    "          end='')\n",
    "    \n",
    "    if accuracy_func:\n",
    "        print('\\tAverage Accuracy: {:.6f}'.format(accuracy/len(train_loader)), \n",
    "               end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to test/validate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, loss_func, accuracy_func, task = 1, set_name = 'Test'):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            # Get data and target\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data).squeeze()\n",
    "            # Calculate loss\n",
    "            test_loss += loss_func(output, target).item()\n",
    "            # Calculate accuracy\n",
    "            accuracy += accuracy_func(output, target)\n",
    "\n",
    "    # Calculate average loss\n",
    "    if task == 1:\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "    elif task == 2:\n",
    "        test_loss /= len(test_loader)\n",
    "    \n",
    "    # Print average scores\n",
    "    print('\\n{} set: Average loss: {:.4f} Average accuracy: {:.4f}\\n'.format(\n",
    "           set_name, \n",
    "           test_loss, \n",
    "           accuracy / len(test_loader)))\n",
    "\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test ANN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/800 (0%)]\tLoss: 4626.200684\tAccuracy: 0.000000\n",
      "Train Epoch: 1 [128/800 (16%)]\tLoss: 4277.919922\tAccuracy: 0.000000\n",
      "Train Epoch: 1 [256/800 (32%)]\tLoss: 5008.912109\tAccuracy: 0.000000\n",
      "Train Epoch: 1 [384/800 (48%)]\tLoss: 4048.454102\tAccuracy: 0.000000\n",
      "Train Epoch: 1 [512/800 (64%)]\tLoss: 3181.630371\tAccuracy: 0.000000\n",
      "Train Epoch: 1 [640/800 (80%)]\tLoss: 1120.749023\tAccuracy: 0.041667\n",
      "Train Epoch: 1 [768/800 (96%)]\tLoss: 739.155457\tAccuracy: 0.052083\n",
      "\n",
      "Train Epoch: Average Loss: 108.989784\tAverage Accuracy: 0.011667\n",
      "\n",
      "Test set: Average loss: 21.6852 Average accuracy: 0.0580\n",
      "\n",
      "Train Epoch: 2 [0/800 (0%)]\tLoss: 812.648071\tAccuracy: 0.072917\n",
      "Train Epoch: 2 [128/800 (16%)]\tLoss: 403.911713\tAccuracy: 0.093750\n",
      "Train Epoch: 2 [256/800 (32%)]\tLoss: 481.531921\tAccuracy: 0.083333\n",
      "Train Epoch: 2 [384/800 (48%)]\tLoss: 294.836304\tAccuracy: 0.083333\n",
      "Train Epoch: 2 [512/800 (64%)]\tLoss: 238.249634\tAccuracy: 0.114583\n",
      "Train Epoch: 2 [640/800 (80%)]\tLoss: 131.975983\tAccuracy: 0.083333\n",
      "Train Epoch: 2 [768/800 (96%)]\tLoss: 319.493011\tAccuracy: 0.104167\n",
      "\n",
      "Train Epoch: Average Loss: 12.759089\tAverage Accuracy: 0.092083\n",
      "\n",
      "Test set: Average loss: 8.1157 Average accuracy: 0.0893\n",
      "\n",
      "Train Epoch: 3 [0/800 (0%)]\tLoss: 272.646667\tAccuracy: 0.125000\n",
      "Train Epoch: 3 [128/800 (16%)]\tLoss: 182.985580\tAccuracy: 0.093750\n",
      "Train Epoch: 3 [256/800 (32%)]\tLoss: 195.523361\tAccuracy: 0.041667\n",
      "Train Epoch: 3 [384/800 (48%)]\tLoss: 109.601036\tAccuracy: 0.177083\n",
      "Train Epoch: 3 [512/800 (64%)]\tLoss: 201.802704\tAccuracy: 0.031250\n",
      "Train Epoch: 3 [640/800 (80%)]\tLoss: 158.421875\tAccuracy: 0.104167\n",
      "Train Epoch: 3 [768/800 (96%)]\tLoss: 230.059326\tAccuracy: 0.072917\n",
      "\n",
      "Train Epoch: Average Loss: 5.982783\tAverage Accuracy: 0.107500\n",
      "\n",
      "Test set: Average loss: 5.7450 Average accuracy: 0.1190\n",
      "\n",
      "Train Epoch: 4 [0/800 (0%)]\tLoss: 138.985397\tAccuracy: 0.125000\n",
      "Train Epoch: 4 [128/800 (16%)]\tLoss: 234.059860\tAccuracy: 0.145833\n",
      "Train Epoch: 4 [256/800 (32%)]\tLoss: 142.269104\tAccuracy: 0.135417\n",
      "Train Epoch: 4 [384/800 (48%)]\tLoss: 231.858551\tAccuracy: 0.114583\n",
      "Train Epoch: 4 [512/800 (64%)]\tLoss: 174.980957\tAccuracy: 0.083333\n",
      "Train Epoch: 4 [640/800 (80%)]\tLoss: 174.961914\tAccuracy: 0.166667\n",
      "Train Epoch: 4 [768/800 (96%)]\tLoss: 179.216156\tAccuracy: 0.156250\n",
      "\n",
      "Train Epoch: Average Loss: 5.453570\tAverage Accuracy: 0.119167\n",
      "\n",
      "Test set: Average loss: 5.7125 Average accuracy: 0.1116\n",
      "\n",
      "Train Epoch: 5 [0/800 (0%)]\tLoss: 149.460312\tAccuracy: 0.104167\n",
      "Train Epoch: 5 [128/800 (16%)]\tLoss: 112.635788\tAccuracy: 0.104167\n",
      "Train Epoch: 5 [256/800 (32%)]\tLoss: 137.216354\tAccuracy: 0.156250\n",
      "Train Epoch: 5 [384/800 (48%)]\tLoss: 229.369125\tAccuracy: 0.125000\n",
      "Train Epoch: 5 [512/800 (64%)]\tLoss: 200.312805\tAccuracy: 0.104167\n",
      "Train Epoch: 5 [640/800 (80%)]\tLoss: 147.853989\tAccuracy: 0.135417\n",
      "Train Epoch: 5 [768/800 (96%)]\tLoss: 191.259567\tAccuracy: 0.135417\n",
      "\n",
      "Train Epoch: Average Loss: 5.382754\tAverage Accuracy: 0.115417\n",
      "\n",
      "Test set: Average loss: 5.6408 Average accuracy: 0.1235\n",
      "\n",
      "Train Epoch: 6 [0/800 (0%)]\tLoss: 138.484238\tAccuracy: 0.104167\n",
      "Train Epoch: 6 [128/800 (16%)]\tLoss: 133.538635\tAccuracy: 0.156250\n",
      "Train Epoch: 6 [256/800 (32%)]\tLoss: 158.593048\tAccuracy: 0.135417\n",
      "Train Epoch: 6 [384/800 (48%)]\tLoss: 207.984543\tAccuracy: 0.083333\n",
      "Train Epoch: 6 [512/800 (64%)]\tLoss: 191.459229\tAccuracy: 0.177083\n",
      "Train Epoch: 6 [640/800 (80%)]\tLoss: 180.367416\tAccuracy: 0.072917\n",
      "Train Epoch: 6 [768/800 (96%)]\tLoss: 158.241562\tAccuracy: 0.114583\n",
      "\n",
      "Train Epoch: Average Loss: 5.177928\tAverage Accuracy: 0.125000\n",
      "\n",
      "Test set: Average loss: 6.0235 Average accuracy: 0.1310\n",
      "\n",
      "Train Epoch: 7 [0/800 (0%)]\tLoss: 165.665802\tAccuracy: 0.083333\n",
      "Train Epoch: 7 [128/800 (16%)]\tLoss: 197.076569\tAccuracy: 0.114583\n",
      "Train Epoch: 7 [256/800 (32%)]\tLoss: 209.649994\tAccuracy: 0.083333\n",
      "Train Epoch: 7 [384/800 (48%)]\tLoss: 197.881378\tAccuracy: 0.083333\n",
      "Train Epoch: 7 [512/800 (64%)]\tLoss: 260.002289\tAccuracy: 0.135417\n",
      "Train Epoch: 7 [640/800 (80%)]\tLoss: 161.412598\tAccuracy: 0.135417\n",
      "Train Epoch: 7 [768/800 (96%)]\tLoss: 202.155762\tAccuracy: 0.093750\n",
      "\n",
      "Train Epoch: Average Loss: 5.446837\tAverage Accuracy: 0.108333\n",
      "\n",
      "Test set: Average loss: 5.7806 Average accuracy: 0.1235\n",
      "\n",
      "Train Epoch: 8 [0/800 (0%)]\tLoss: 156.605225\tAccuracy: 0.104167\n",
      "Train Epoch: 8 [128/800 (16%)]\tLoss: 140.315887\tAccuracy: 0.062500\n",
      "Train Epoch: 8 [256/800 (32%)]\tLoss: 207.310608\tAccuracy: 0.177083\n",
      "Train Epoch: 8 [384/800 (48%)]\tLoss: 165.344635\tAccuracy: 0.135417\n",
      "Train Epoch: 8 [512/800 (64%)]\tLoss: 189.474823\tAccuracy: 0.125000\n",
      "Train Epoch: 8 [640/800 (80%)]\tLoss: 259.138550\tAccuracy: 0.062500\n",
      "Train Epoch: 8 [768/800 (96%)]\tLoss: 164.143524\tAccuracy: 0.145833\n",
      "\n",
      "Train Epoch: Average Loss: 5.225759\tAverage Accuracy: 0.114583\n",
      "\n",
      "Test set: Average loss: 6.3439 Average accuracy: 0.0997\n",
      "\n",
      "Train Epoch: 9 [0/800 (0%)]\tLoss: 122.437828\tAccuracy: 0.072917\n",
      "Train Epoch: 9 [128/800 (16%)]\tLoss: 179.000839\tAccuracy: 0.104167\n",
      "Train Epoch: 9 [256/800 (32%)]\tLoss: 206.012924\tAccuracy: 0.145833\n",
      "Train Epoch: 9 [384/800 (48%)]\tLoss: 190.458618\tAccuracy: 0.114583\n",
      "Train Epoch: 9 [512/800 (64%)]\tLoss: 138.599304\tAccuracy: 0.156250\n",
      "Train Epoch: 9 [640/800 (80%)]\tLoss: 170.568176\tAccuracy: 0.145833\n",
      "Train Epoch: 9 [768/800 (96%)]\tLoss: 162.653564\tAccuracy: 0.135417\n",
      "\n",
      "Train Epoch: Average Loss: 5.444669\tAverage Accuracy: 0.115833\n",
      "\n",
      "Test set: Average loss: 6.3669 Average accuracy: 0.0818\n",
      "\n",
      "Train Epoch: 10 [0/800 (0%)]\tLoss: 227.484421\tAccuracy: 0.104167\n",
      "Train Epoch: 10 [128/800 (16%)]\tLoss: 141.865967\tAccuracy: 0.125000\n",
      "Train Epoch: 10 [256/800 (32%)]\tLoss: 105.400742\tAccuracy: 0.187500\n",
      "Train Epoch: 10 [384/800 (48%)]\tLoss: 191.785812\tAccuracy: 0.093750\n",
      "Train Epoch: 10 [512/800 (64%)]\tLoss: 179.605576\tAccuracy: 0.072917\n",
      "Train Epoch: 10 [640/800 (80%)]\tLoss: 162.889191\tAccuracy: 0.114583\n",
      "Train Epoch: 10 [768/800 (96%)]\tLoss: 151.914886\tAccuracy: 0.072917\n",
      "\n",
      "Train Epoch: Average Loss: 5.153566\tAverage Accuracy: 0.125833\n",
      "\n",
      "Test set: Average loss: 5.7667 Average accuracy: 0.1220\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), \n",
    "                      lr=lr, \n",
    "                      momentum=momentum)\n",
    "                      \n",
    "early_stopping = EarlyStopping()\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # Train\n",
    "    train(model, \n",
    "          device, \n",
    "          train_dataloader, \n",
    "          optimizer, \n",
    "          epoch, \n",
    "          loss_fn, \n",
    "          task1_accuracy)\n",
    "\n",
    "    # Test\n",
    "    test_loss = test(model, \n",
    "                     device, \n",
    "                     test_dataloader, \n",
    "                     loss_fn, \n",
    "                     task1_accuracy)\n",
    "\n",
    "    if early_stopping(test_loss):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Questions\n",
    "1. What preprocessing techniques did you use? Why?\n",
    "    - *Answer*\n",
    "2. Describe the fine-tuning process and how you reached your model architecture.\n",
    "    - *Answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Task 2: CNN (40%)\n",
    "For this task, you will be doing image classification:\n",
    "- First, adapt your best model from Task 1 to work on this task, and\n",
    "fit it on the new data. Then, evaluate its performance.\n",
    "- After that, build a CNN model for image classification.\n",
    "- Compare both models in terms of accuracy, number of parameters and speed of\n",
    "inference (the time the model takes to predict 50 samples).\n",
    "\n",
    "For the given data, you need to do proper data preprocessing and augmentation,\n",
    "data loaders.\n",
    "Then fine-tune your model architecture (number of layers, number of filters,\n",
    "activation function, learning rate, momentum, regularization).\n",
    "\n",
    "### Data\n",
    "You will be working with the data in `triple_mnist.zip` for predicting 3-digit\n",
    "numbers writen in the image. Each image contains 3 digits similar to the\n",
    "following example (whose label is `039`):\n",
    "\n",
    "![example](https://github.com/shaohua0116/MultiDigitMNIST/blob/master/asset/examples/039/0_039.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 2 - SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing and augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task I used such data preprocessing methods and augmentation:\n",
    "\n",
    "1) Resize - I used it to don't care about the initial size of image and work with concrete one.\n",
    "\n",
    "2) Grayscale - I used it to work with one input channel, as our image is consist of black and white colors. In this case I decided that RGB is unnecessary.\n",
    "\n",
    "3) ToTensor - transform PIL to Tensor\n",
    "\n",
    "4) Moreover, I apply RandomRotation for training set. This can be useful in cases where the handwriting is slightly slanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to data\n",
    "test_path = './triple_mnist/triple_mnist/test/'\n",
    "train_path = './triple_mnist/triple_mnist/train/'\n",
    "val_path = './triple_mnist/triple_mnist/val/'\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Define train transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((84, 84)),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.RandomRotation(degrees=(-7, 7)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Define test transforms\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((84, 84)),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load train, test and validation datasets\n",
    "train_data = ImageFolder(root=train_path, \n",
    "                         transform=test_transform)\n",
    "\n",
    "augumented_train_data = ImageFolder(root=train_path,\n",
    "                                    transform=train_transform)\n",
    "\n",
    "test_data = ImageFolder(root=test_path, \n",
    "                        transform=test_transform)\n",
    "                        \n",
    "val_data = ImageFolder(root=val_path, \n",
    "                       transform=test_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part I decide to fix labels (targets column) in the dataset. ImageFolder readed it in this case: (path_to_image, index_of_class). I changed it to this way: (path_to_image, [num1, num2, num3]).\n",
    "\n",
    "I made it to predict each digit separately and improve time perfomance. I think that this way is better than predict 1 out of 1000 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fix target values in dataset\n",
    "def fix_data_targets(data):\n",
    "    classes = data.classes\n",
    "    for i in range(len(data.samples)):\n",
    "        path, index = data.samples[i]\n",
    "        cl = classes[index]\n",
    "        arr = np.zeros((3,), dtype=np.int64)\n",
    "        for j in range(len(cl)):\n",
    "            arr[j] = int(cl[j])\n",
    "        new_sample = (path, arr)\n",
    "        data.samples[i] = new_sample\n",
    "        data.targets[i] = arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ConcatDataset' object has no attribute 'classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Apply fix_data_targets to all datasets\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m fix_data_targets(train_data)\n\u001b[0;32m      3\u001b[0m fix_data_targets(test_data)\n\u001b[0;32m      4\u001b[0m fix_data_targets(val_data)\n",
      "Cell \u001b[1;32mIn [23], line 3\u001b[0m, in \u001b[0;36mfix_data_targets\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfix_data_targets\u001b[39m(data):\n\u001b[1;32m----> 3\u001b[0m     classes \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mclasses\n\u001b[0;32m      4\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(data\u001b[39m.\u001b[39msamples)):\n\u001b[0;32m      5\u001b[0m         path, index \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39msamples[i]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ConcatDataset' object has no attribute 'classes'"
     ]
    }
   ],
   "source": [
    "# Apply fix_data_targets to all datasets\n",
    "fix_data_targets(train_data)\n",
    "fix_data_targets(test_data)\n",
    "fix_data_targets(val_data)\n",
    "fix_data_targets(augumented_train_data)\n",
    "\n",
    "train_data.samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show one sample from dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part I get number of input channels and show first image from batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "# in_channels is created for future use in CNN\n",
    "in_channels = images.shape[1]\n",
    "images_shape = np.prod(images.shape[1:])\n",
    "\n",
    "# Plot imahe and print shape\n",
    "print(images.shape, images_shape, in_channels)\n",
    "plt.imshow(np.transpose(images[0].cpu().detach().numpy(), (1, 2, 0)), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare ANN from the first task for new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN_2(ANN):\n",
    "    def __init__(self):\n",
    "        super(ANN_2, self).__init__()\n",
    "        self.hidden1 = nn.Linear(images_shape, 64)\n",
    "        self.output = nn.Linear(64, 30)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Reshape input\n",
    "        x = x.view(-1, images_shape)\n",
    "\n",
    "        # Apply first hidden layer\n",
    "        x = F.leaky_relu(self.hidden1(x))\n",
    "\n",
    "        # Apply output layer\n",
    "        x = self.output(x)\n",
    "        \n",
    "        # Apply softmax\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "\n",
    "        # Reshape output\n",
    "        x = x.view(-1, 3, 10)\n",
    "        # Transpose output for loss function\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_model = ANN_2().to(device)\n",
    "print(ann_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update epochs number and loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task I decide to check ANN model without changing learning rate and momentum. As we asked to change only input and output layers.\n",
    "\n",
    "Moreover, I tried 2 different loss functions: NLLLoss and CrossEntropyLoss. As for me, in this case NLLLoss was better and model trained a little bit better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "log_interval = 20\n",
    "\n",
    "loss_fn = nn.NLLLoss()\n",
    "# loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a custom function for accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task2_accuracy(output, target):\n",
    "    # Convert Test data to numpy\n",
    "    output = output.cpu().detach().numpy()\n",
    "    target = target.cpu().detach().numpy()\n",
    "\n",
    "    accuracy = 0\n",
    "\n",
    "    # Itterate through each batch element\n",
    "    for i in range(len(output)):\n",
    "        out = np.transpose(output[i])\n",
    "        tar = target[i]\n",
    "\n",
    "        num1 = out[0].argmax()\n",
    "        num2 = out[1].argmax()\n",
    "        num3 = out[2].argmax()\n",
    "\n",
    "        # print(num1, num2, num3)\n",
    "\n",
    "        if num1 == tar[0] and num2 == tar[1] and num3 == tar[2]:\n",
    "            accuracy += 1\n",
    "    \n",
    "    return accuracy / len(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, validate and test ANN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(ann_model.parameters(), lr=lr, momentum=momentum)\n",
    "early_stopping = EarlyStopping()\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(ann_model, device, train_loader, optimizer, epoch, loss_fn, None, 2)\n",
    "    val_loss = test(ann_model, device, val_loader, loss_fn, task2_accuracy, 2, 'Validation')\n",
    "\n",
    "    if early_stopping(val_loss):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "test_loss = test(ann_model, device, test_loader, loss_fn, task2_accuracy, 2)\n",
    "\n",
    "print('ANN test seconds: {:.4f}'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For CNN model I tried different functions and number of neurones. I tried:\n",
    "\n",
    "1) Don't use pool between normalization and convolution -> caused overfitting.\n",
    "\n",
    "2) To use 2 convolution layers instead of 3 -> caused overfitting.\n",
    "\n",
    "3) To use 2 linear layers instead of 3 linear layers ->  2 layers showed better result than 3 layers.\n",
    "\n",
    "4) To use 4 linear layers instead of 3 linear layers -> caused underfitting.\n",
    "\n",
    "5) To use dropout of 15% on the first linear layer -> caused underfitting.\n",
    "\n",
    "6) To use small number of output channels (2-3-4) and big numbers (10-16-22) -> with big numbers model worked better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # Input image 84 x 84\n",
    "        self.conv1 = nn.Conv2d(in_channels, 10, 3)  # output 82 x 82\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # pool function with kernel size 2 and stride 2\n",
    "        self.conv1_bn = nn.BatchNorm2d(10) # batch normalization for 41 x 41 x 6\n",
    "\n",
    "        self.conv2 = nn.Conv2d(10, 16, 4)  # output 38 x 38\n",
    "        self.conv2_bn = nn.BatchNorm2d(16)  # batch normalization for 19 x 19 x 8\n",
    "\n",
    "        self.conv3 = nn.Conv2d(16, 22, 3) # output 17 x 17\n",
    "        self.conv3_bn = nn.BatchNorm2d(22) # batch normalization for 17 x 17 x 10\n",
    "\n",
    "        # self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        self.linear1 = nn.Linear(17 * 17 * 22, 128)\n",
    "        self.out = nn.Linear(128, 30)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply first convolutional layer\n",
    "        x = F.relu(self.conv1(x))\n",
    "        # Apply pool function\n",
    "        x = self.pool(x)\n",
    "        # Apply first batch normalization\n",
    "        x = self.conv1_bn(x)\n",
    "\n",
    "        # Apply second convolutional layer\n",
    "        x = F.relu(self.conv2(x))\n",
    "        # Apply pool function\n",
    "        x = self.pool(x)\n",
    "        # Apply second batch normalization\n",
    "        x = self.conv2_bn(x)\n",
    "        \n",
    "        # Apply third convolutional layer\n",
    "        x = F.relu(self.conv3(x))\n",
    "        # Apply batch normalization\n",
    "        x = self.conv3_bn(x)\n",
    "\n",
    "        # Flatten the output\n",
    "        x = torch.flatten(x, 1)\n",
    "        # Apply first linear layer\n",
    "        x = F.relu(self.linear1(x))\n",
    "        \n",
    "        # # Apply dropout\n",
    "        # x = self.dropout(x)\n",
    "\n",
    "        # Apply output layer\n",
    "        x = self.out(x)\n",
    "\n",
    "        # Apply softmax function\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        # Reshape the output\n",
    "        x = x.view(-1, 3, 10)\n",
    "        # Transpose output for loss function\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I found that learning rate and momentum from first task were bad for CNN, when tested. Therefore, I decided to change it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "momentum = 0.5\n",
    "\n",
    "cnn_model = CNN().to(device)\n",
    "print(cnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, validate and test CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(cnn_model.parameters(), lr=lr, momentum=momentum)\n",
    "early_stopping = EarlyStopping()\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(cnn_model, device, train_loader, optimizer, epoch, loss_fn, None, 2)\n",
    "    val_loss = test(cnn_model, device, val_loader, loss_fn, task2_accuracy, 2, 'Validation')\n",
    "\n",
    "    if early_stopping(val_loss):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "test_loss = test(cnn_model, device, test_loader, loss_fn, task2_accuracy, 2)\n",
    "\n",
    "print('CNN test seconds: {:.4f}'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of ANN model is less than 5%. \n",
    "\n",
    "While accuracy of CNN model is close to 90%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of parameters in each model\n",
    "ann_params = sum(param.numel() for param in ann_model.parameters())\n",
    "cnn_params = sum(param.numel() for param in cnn_model.parameters())\n",
    "\n",
    "# Print the number of parameters in each model\n",
    "print('ANN Parameters: ', ann_params)\n",
    "print('CNN Parameters: ', cnn_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANN model have 2 times less parameters than CNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare test time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANN model takes about 7 seconds to predict values on test set.\n",
    "\n",
    "While CNN model takes about 10 seconds to predict values on test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task I decided to predict each digit of number. Therefore, I have output layer as 30 x 1 in models. Each 10 values - probabilities of digit. Therefore, I split this list into 3 list with 10 values and take the index of maxprobability in each list. Then cobmine and get predicted integer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What preprocessing techniques did you use? Why?\n",
    "    - In this task I used Resize and Grayscale. I decided to use such preprocessing techniques to ensure that all images in dataset will be 84x84 pixels. Moreover, grayscale is used to change RGB to gray map. I think, that 1 channel input in this task is better than 3 channels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What data augmentation techniques did you use?\n",
    "    - In this task I used RandomRotation. I tought, that some rotations can improve perfomance in case of different handwritings. Because different handwritings have different slopes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Describe the fine-tuning process and how you reached your final CNN model.\n",
    "    - For CNN model I tried different functions and number of neurones. I tried:\n",
    "        1) Don't use pool between normalization and convolution -> caused overfitting.\n",
    "        2) To use 2 convolution layers instead of 3 -> caused overfitting.\n",
    "        3) To use 2 linear layers instead of 3 linear layers ->  2 layers showed better result than 3 layers.\n",
    "        4) To use 4 linear layers instead of 3 linear layers -> caused underfitting.\n",
    "        5) To use dropout of 15% on the first linear layer -> caused underfitting.\n",
    "        6) To use small number of output channels (2-3-4) and big numbers (10-16-22) -> with big numbers model worked better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Task 3: Decision Trees and Ensemble Learning (15%)\n",
    "\n",
    "For the `loan_data.csv` data, predict if the bank should give a loan or not.\n",
    "You need to do the following:\n",
    "- Fine-tune a decision tree on the data\n",
    "- Fine-tune a random forest on the data\n",
    "- Compare their performance\n",
    "- Visualize your DT and one of the trees from the RF\n",
    "\n",
    "For evaluating your models, do $80/20$ train test split.\n",
    "\n",
    "### Data\n",
    "- `credit.policy`: Whether the customer meets the credit underwriting criteria.\n",
    "- `purpose`: The purpose of the loan.\n",
    "- `int.rate`: The interest rate of the loan.\n",
    "- `installment`: The monthly installments owed by the borrower if the loan is funded.\n",
    "- `log.annual.inc`: The natural logarithm of the self-reported annual income of the borrower.\n",
    "- `dti`: The debt-to-income ratio of the borrower.\n",
    "- `fico`: The FICO credit score of the borrower.\n",
    "- `days.with.cr.line`: The number of days the borrower has had a credit line.\n",
    "- `revol.bal`: The borrower's revolving balance.\n",
    "- `revol.util`: The borrower's revolving line utilization rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Questions\n",
    "1. How did the DT compare to the RF in performance? Why?\n",
    "    - *Answer*\n",
    "2. After fine-tuning, how does the max depth in DT compare to RF? Why?\n",
    "    - *Answer*\n",
    "3. What is ensemble learning? What are its pros and cons?\n",
    "    - *Answer*\n",
    "4. Briefly explain 2 types of boosting methods and 2 types of bagging methods.\n",
    "Which of these categories does RF fall under?\n",
    "    - *Answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Task 4: Domain Gap (15%)\n",
    "\n",
    "Evaluate your CNN model from task 2 on SVHN data without retraining your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = []\n",
    "targets = []\n",
    " \n",
    "# get the path/directory\n",
    "folder_dir = './svhn/svhn/'\n",
    "for image in os.listdir(folder_dir):\n",
    "    # Combine the path and the image name\n",
    "    image_path = os.path.join(folder_dir, image)\n",
    "    # get the image name without the extension\n",
    "    image_name = os.path.splitext(image)[0]\n",
    "    arr = np.zeros(3, dtype=np.int64)\n",
    "    arr[0] = int(image_name[0])\n",
    "    arr[1] = int(image_name[1])\n",
    "    arr[2] = int(image_name[2])\n",
    "    image_paths.append(image_path)\n",
    "    targets.append(arr)\n",
    "\n",
    "print(image_paths, end='\\n\\n')\n",
    "print(*targets, end='\\n\\n')\n",
    "print(len(image_paths), len(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/vision/main/_modules/torchvision/datasets/folder.html\n",
    "\n",
    "def pil_loader(path: str) -> Image.Image:\n",
    "    with open(path, \"rb\") as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class\n",
    "class CustomSVHNDataset(Dataset):\n",
    "    def __init__(self, paths, targets, transform):\n",
    "        super().__init__()\n",
    "        self.paths = paths\n",
    "        self.targets = torch.tensor(targets).long()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = pil_loader(self.paths[idx])\n",
    "        image = self.transform(image)\n",
    "        target = self.targets[idx]\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.NLLLoss()\n",
    "batch_size = 4\n",
    "\n",
    "# Define train transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((84, 84)),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dataset = CustomSVHNDataset(image_paths, targets, transform)\n",
    "\n",
    "data_dataloader = DataLoader(data_dataset, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(data_dataloader))\n",
    "\n",
    "# in_channels is created for future use in CNN\n",
    "in_channels = images.shape[1]\n",
    "images_shape = np.prod(images.shape[1:])\n",
    "\n",
    "# Plot imahe and print shape\n",
    "print(images.shape, images_shape, in_channels)\n",
    "\n",
    "for i in range(len(images)):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    plt.imshow(images[i].squeeze(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(cnn_model, device, data_dataloader, loss_fn, task2_accuracy, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Questions\n",
    "1. How did your model perform? Why is it better/worse?\n",
    "    - *Answer*\n",
    "2. What is domain gap in the context of ML?\n",
    "    - *Answer*\n",
    "3. Suggest two ways through which the problem of domain gap can be tackled.\n",
    "    - *Answer*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
