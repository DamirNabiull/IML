{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab-7: ANN in Pytorch\n",
    "\n",
    "In this lab, you will practice simple deep learning model in Pytorch.\n",
    "\n",
    "\n",
    "## Objectives:\n",
    "1. Theoretical issues\n",
    "2. Get starting in Pytorch\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Theoretical issues\n",
    "Ordinary fully connected neural nets consists of Dense layers, activations, and output layer.\n",
    "\n",
    "1. What's the difference between deep learning and normal machine learning?\n",
    "<span style=\"color:blue\"> 1- Adding more data for traditional ML algorithms, won't improve\n",
    "\t  the performance and the learning curve will saturate (plateau), but for DL, you can get better performance.\n",
    "\t<br/>2- There's no feature extraction step in a lot of cases like in CNN\n",
    "\t<br/>3- It need high computational power in training.\n",
    "2. How does a neural network with no hidden layers and one output neuron compare to a logistic/linear regression?\n",
    "<span style=\"color:blue\"> There's no difference. NN with no hidden layers is perceptron which has the same architecture of logistic/linear regression. If the activation function in the output layer is linear and loss is mse then it is linear regression. If the activation function in the output layer is sigmoid and the loss is the logloss then it is logistic regression.\n",
    "3. How does a neural network with multiple hidden layers but with linear activation and one output neuron compared to logistic/linear regression?\n",
    "<span style=\"color:blue\"> Also, there's no difference. Take as an example this image:\n",
    "![alt text](https://ujwlkarn.files.wordpress.com/2016/08/screen-shot-2016-08-09-at-4-19-50-am.png?w=996&h=736)\n",
    "Derive the output neuron in this simple network, given the activation function is this linear function: f(x) = x, You will find it is a linear combination of the input variables.\n",
    "4. Can the perceptron find a non-linear decision boundary?\n",
    "<span style=\"color:blue\"> No, as long as the the logits (logOdds which is the input to the sigmoid function) is a linear combination of the input variables then it can only find a linear decision boundary.\n",
    "5. In multi-hidden layers network, what's the need of non-linear activation function?\n",
    "<span style=\"color:blue\"> To capture the non linear patterns in the relation between the input and the output.\n",
    "6. Is random weight assignment better than assigning same weights to the units in the hidden layer.\n",
    "<span style=\"color:blue\"> Yes, as assigning the same weight to the unit won't learn anything from the error signal propagated from the output layer as the error signal value depends on the value of the weight itels, so the hidden layer before the output layer all of the units will get the same value of the error and they will change to the same value as well and same for all units in the network.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch: Getting started \n",
    "\n",
    "### Feed Forward Neural Network\n",
    "An artificial neural network wherein connections between the nodes do not form a cycle.\n",
    "It consists of : \n",
    "\n",
    "- Input Layer  \n",
    "- Hidden Layer(s)\n",
    "- Output Layer\n",
    "\n",
    "![alt text](https://images.deepai.org/django-summernote/2019-06-06/5c17d9c2-0ad4-474c-be8d-d6ae9b094e74.png)\n",
    "\n",
    "\n",
    "The neural network contents units, typically called \"neurons\".  Each unit has some number of weighted inputs. These weighted inputs are summed together (a linear combination) then passed through an activation function to get the unit's output.\n",
    "\n",
    "<center>\n",
    "<img src=\"./assets/simple_neuron.png\" alt=\"drawing\" style=\"width:400px;\"/> \n",
    "</center>\n",
    "\n",
    "Mathematically this equivalent to:\n",
    "<center>\n",
    "\n",
    "$ \n",
    " y = f(w_1x_1 + w_2x_2 + b)\n",
    "$\n",
    "\n",
    "$ \n",
    " y = f\\left(\\sum_{i=1}^{N} w_ix_i + b\\right)\n",
    "$\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Tensors\n",
    "\n",
    "It turns out neural network computations are just a bunch of linear algebra operations on tensors, a generalization of matrices. A vector is a 1-dimensional tensor, a matrix is a 2-dimensional tensor, an array with three indices is a 3-dimensional tensor (RGB color images for example). The fundamental data structure for neural networks are tensors and PyTorch (as well as pretty much every other deep learning framework) is built around tensors.\n",
    " \n",
    "\n",
    "<img src=\"./assets/tensor_examples.svg\" width=\"600px\">\n",
    "\n",
    "Just like Numpy arrays, Pytorch tensors can be added, multiplied, subtracted, etc.\n",
    "\n",
    "#### A simple neuron with tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "def activation(x):\n",
    "    \"\"\" Sigmoid activation function \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "    \"\"\"\n",
    "    return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate data we need to compute the output of the neuron. We have `5` input features, just random for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate some data\n",
    "torch.manual_seed(7) # Set the random seed so things are predictable\n",
    "\n",
    "# Features are 5 random normal variables\n",
    "features = torch.randn((1, 5))\n",
    "\n",
    "# True weights for our data, random normal variables again\n",
    "weights = torch.randn_like(features)\n",
    "\n",
    "# and a true bias term\n",
    "bias = torch.randn((1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Task**: Calculate the output of the network with input features `features`, weights `weights`, and bias `bias`. Similar to Numpy, PyTorch has a [`torch.sum()`](https://pytorch.org/docs/stable/torch.html#torch.sum) function, as well as a `.sum()` method on tensors, for taking sums. Use the function `activation` defined above as the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1595]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the output here\n",
    "y = activation(torch.sum(features * weights) + bias)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do the multiplication and sum in the same operation using a matrix multiplication. In general, you'll want to use matrix multiplications since they are more efficient and accelerated using modern libraries and high-performance computing on GPUs.\n",
    "\n",
    "We can perform the same operation using matrice multiplication. Useuse [`torch.mm()`](https://pytorch.org/docs/stable/torch.html#torch.mm) or [`torch.matmul()`](https://pytorch.org/docs/stable/torch.html#torch.matmul) which is somewhat more complicated and supports broadcasting. If we try to do it with `features` and `weights` as they are.\n",
    "\n",
    "**Note** When doing matrice multiplication consider reshaping matrix in correct shape to avoid error. \n",
    "\n",
    "There are a few options here: [`weights.reshape()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.reshape), [`weights.resize_()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.resize_), and [`weights.view()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view).\n",
    "\n",
    "* `weights.reshape(a, b)` will return a new tensor with the same data as `weights` with size `(a, b)` sometimes, and sometimes a clone, as in it copies the data to another part of memory.\n",
    "* `weights.resize_(a, b)` returns the same tensor with a different shape. However, if the new shape results in fewer elements than the original tensor, some elements will be removed from the tensor (but not from memory). If the new shape results in more elements than the original tensor, new elements will be uninitialized in memory. Here I should note that the underscore at the end of the method denotes that this method is performed **in-place**. Here is a great forum thread to [read more about in-place operations](https://discuss.pytorch.org/t/what-is-in-place-operation/16244) in PyTorch.\n",
    "* `weights.view(a, b)` will return a new tensor with the same data as `weights` with size `(a, b)`.\n",
    "\n",
    "> **Task**:  Compute the output of the single unit network using matrix multiplicatiom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1595]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your solution here\n",
    "y = activation(torch.mm(features, weights.view(5,1)) + bias)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi layer network\n",
    "\n",
    "We saw how to compute the output of single unit network. The power of neural networks come when multiple units into layers. \n",
    "The output of one layer of neurons becomes the input for the next layer. With multiple input units and output units, we now need to express the weights as a matrix.\n",
    "\n",
    "<img src='./assets/multilayer_diagram_weights.png' width=450px>\n",
    "\n",
    "The first layer shown on the bottom here are the inputs, understandably called the **input layer**. The middle layer is called the **hidden layer**, and the final layer (on the right) is the **output layer**. We can express this network mathematically with matrices again and use matrix multiplication to get linear combinations for each unit in one operation. For example, the hidden layer ($h_1$ and $h_2$ here) can be calculated \n",
    "\n",
    "$$\n",
    "\\vec{h} = [h_1 \\, h_2] = \n",
    "\\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots \\, x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_{11} & w_{12} \\\\\n",
    "           w_{21} &w_{22} \\\\\n",
    "           \\vdots &\\vdots \\\\\n",
    "           w_{n1} &w_{n2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The output for this small network is found by treating the hidden layer as inputs for the output unit. The network output is expressed simply\n",
    "\n",
    "$$\n",
    "y =  f_2 \\! \\left(\\, f_1 \\! \\left(\\vec{x} \\, \\mathbf{W_1}\\right) \\mathbf{W_2} \\right)\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate some data\n",
    "torch.manual_seed(7) # Set the random seed so things are predictable\n",
    "\n",
    "# Features are 3 random normal variables\n",
    "features = torch.randn((1, 3))\n",
    "\n",
    "# Define the size of each layer in our network\n",
    "n_input = features.shape[1]     # Number of input units, must match number of input features\n",
    "n_hidden = 2                    # Number of hidden units \n",
    "n_output = 1                    # Number of output units\n",
    "\n",
    "# Weights for inputs to hidden layer\n",
    "W1 = torch.randn(n_input, n_hidden)\n",
    "# Weights for hidden layer to output layer\n",
    "W2 = torch.randn(n_hidden, n_output)\n",
    "\n",
    "# and bias terms for hidden and output layers\n",
    "B1 = torch.randn((1, n_hidden))\n",
    "B2 = torch.randn((1, n_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Task:** Calculate and print the output for this multi-layer network using the weights `W1` & `W2`, and the biases, `B1` & `B2`. The correct value should be `tensor([[0.3171]])` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3171]])\n"
     ]
    }
   ],
   "source": [
    "### Solution\n",
    "\n",
    "h = activation(torch.mm(features, W1) + B1)\n",
    "output = activation(torch.mm(h, W2) + B2)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Autograd\n",
    "\n",
    "Automatic differentiation package : **No need to worry about back propagation partial derivatives and chain rule** \n",
    "\n",
    "Tensors track their computational history and support gradient computation\n",
    "\n",
    "`requires_grad=True` : Tells PyTorch that we want to compute gradients for the specific tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(5.0, requires_grad=True)\n",
    "print(x.grad) # no gradient at the initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `backward()` function is responsible for calculation of gradients and accumulate (not apply) them in respective tensors\n",
    "\n",
    "The tensor with `requires_grad=True`: has attribute to check the gradients values : `grad`\n",
    "\n",
    "Let's consider a function of x. \n",
    "$f(x) = x^2 + 2x + 1$\n",
    "\n",
    "\n",
    ">**Question** what is the gradient at the point $x=5$\n",
    "\n",
    "**the derivate of $f$ is $f'(x) = 2x + 2$. for $x=5$ the gradient is 12**\n",
    "\n",
    "The following code will compute and accumulate the gradient w.r.t $x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(36., grad_fn=<AddBackward0>) True\n",
      "tensor(12.)\n"
     ]
    }
   ],
   "source": [
    "# we compute some function f(x) = x^2 + 2x + 1\n",
    "z = x ** 2 + 2*x + 1\n",
    "\n",
    "print(z, z.requires_grad)\n",
    "\n",
    "z.backward() # compute and propagate the gradient\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the previous cell several time and see how the value of the gradient changes\n",
    "\n",
    "**Note**: Because the gradient is accumulated everytime you call `backward()` it is important to zero the accumulated values before any calculations, i.e., `x.grad = None` or `zero_grad()` for optimizers.\n",
    "\n",
    "\n",
    "\n",
    "To stop PyTorch from tracking the history and forming the backward graph, the code can be wrapped inside with torch.no_grad(): It will make the code run faster whenever gradient tracking is not needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(36.) False\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # we compute some function f(x) = x^2 + 2x + 1\n",
    "    z = x ** 2 + 2*x + 1\n",
    "\n",
    "    print(z, z.requires_grad)\n",
    "\n",
    "    # z.backward()  will trigger an eoor no gradient is tracked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward graph is created automatically and dynamically by autograd class during forward pass. Backward() simply calculates the gradients by passing its argument to the already made backward graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient w.r.t x =  tensor([0., 5., 3.])\n",
      "Gradient w.r.t y =  tensor([ 1.,  2., 10.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0, 10.0], requires_grad = True)\n",
    "y = torch.tensor([0.0, 5.0, 3], requires_grad = True)\n",
    "\n",
    "z = x * y\n",
    "\n",
    "z.backward(torch.tensor([1.0, 1.0, 1.0])) # compute the gradient usinng an external gradient\n",
    "print(\"Gradient w.r.t x = \", x.grad)\n",
    "print(\"Gradient w.r.t y = \", y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor to numpy array and vice-versa\n",
    "\n",
    "PyTorch has a great feature for converting between Numpy arrays and Torch tensors. To create a tensor from a Numpy array, use `torch.from_numpy()`. To convert a tensor to a Numpy array, use the `.numpy()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.51380853, 0.98588048, 0.79321233],\n",
       "       [0.17281425, 0.88647339, 0.9228533 ],\n",
       "       [0.82934223, 0.20525904, 0.86651418],\n",
       "       [0.14288894, 0.74958566, 0.7688467 ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.random.rand(4,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5138, 0.9859, 0.7932],\n",
       "        [0.1728, 0.8865, 0.9229],\n",
       "        [0.8293, 0.2053, 0.8665],\n",
       "        [0.1429, 0.7496, 0.7688]], dtype=torch.float64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.51380853, 0.98588048, 0.79321233],\n",
       "       [0.17281425, 0.88647339, 0.9228533 ],\n",
       "       [0.82934223, 0.20525904, 0.86651418],\n",
       "       [0.14288894, 0.74958566, 0.7688467 ]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "## Model Design in Pytorch\n",
    "Now we're going to build a larger network that can solve a (formerly) difficult problem, identifying text in an image. Here we'll use the MNIST dataset which consists of greyscale handwritten digits. Each image is 28x28 pixels, you can see a sample below.:\n",
    " \n",
    "<img src=\"./assets/mnist.png\" width=\"500px\"> \n",
    "\n",
    "Our goal is to build a neural network that can take one of these images and predict the digit in the image.\n",
    "\n",
    "we have three simple parts that we need to build:\n",
    "1. Data Loading process.\n",
    "2. Model building.\n",
    "3. the training loops.\n",
    "\n",
    "### 1. Data Loading\n",
    "\n",
    "Data Loading in pytorch is very easy and broken into 3 steps:\n",
    "1. Data Source.\n",
    "2. Data Transformations.\n",
    "3. Data Loader.\n",
    "\n",
    "\n",
    "\n",
    "#### Loading data\n",
    "\n",
    "Pytorch uses data loading utility which is called `DataLoader` that supports:\n",
    "automatic batching, transformation, single- and multi-process data loading and more.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch. utils.data import DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "test_batch_size = 100\n",
    "\n",
    "data_transformations = transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])\n",
    "\n",
    "mnist_train = datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=data_transformations)\n",
    "mnist_test = datasets.MNIST('../data', train=False,\n",
    "                            transform=data_transformations)\n",
    "\n",
    "train_loader = DataLoader(mnist_train,\n",
    "                          batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test,\n",
    "                         batch_size=test_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label= tensor(5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x20dc359fca0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "print('Label=', labels[0])\n",
    "plt.imshow(images[0].reshape(28,28), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's try to build a simple network for this dataset using weight matrices and matrix multiplications. Then, we'll see how to do it using PyTorch's `nn` module which provides a much more convenient and powerful method for defining network architectures.\n",
    "\n",
    "We will create fully connected layers (Each unit in one layer is connected to each unit in the next layer) with following architecture:\n",
    "\n",
    "- Input layer of size `784 = 28x28`: this correspond to the *flattening* of the input image, i.e., tranform the 2D images into 1D vectors. Having batch size of `32`, the input size will be `(32, 784)` \n",
    "- Two hidden layers with 256 and 100 units\n",
    "- Output layer with 10 units corresponding to number of classes\n",
    "\n",
    "> **Task**: Flatten the batch of images images. Then build a multi-layer network with 784 input units, 100 hidden units, 100 hidden units, and 10 output units using random tensors for the weights and biases. For now, use a sigmoid activation for the hidden layer. Leave the output layer without an activation, we'll add one that gives us a probability distribution next.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Solution\n",
    "def activation(x):\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "# Flatten the input images\n",
    "inputs = images.view(images.shape[0], -1)\n",
    "\n",
    "# Create parameters with random initialization\n",
    "w1 = torch.randn(784, 256)\n",
    "b1 = torch.randn(256)\n",
    "\n",
    "w2 = torch.randn(256, 100)\n",
    "b2 = torch.randn(100)\n",
    "\n",
    "w3 = torch.randn(100, 10)\n",
    "b3 = torch.randn(10)\n",
    "\n",
    "# Write the code to compute the output of network from the image inputs\n",
    "h1 = activation(torch.mm(inputs, w1) + b1)\n",
    "\n",
    "h2 = activation(torch.mm(h1, w2) + b2)\n",
    "\n",
    "out = activation(torch.mm(h2, w3) + b3)\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 10 outputs for our network. We want to pass in an image to our network and get out a probability distribution over the classes that tells us the likely class(es) the image belongs to. Something that looks like this: \n",
    "\n",
    "<image src=\"./assets/image_distribution.png\" width=\"400px\">\n",
    "\n",
    "Here we see that the probability for each class is roughly the same. This is representing an untrained network, it hasn't seen any data yet so it just returns a uniform distribution with equal probabilities for each class.\n",
    "\n",
    "To calculate this probability distribution, we often use the softmax function. Mathematically this looks like:\n",
    "\n",
    "\n",
    "#### $\\sigma(x_i) = \\dfrac{e^{x_i}}{\\sum_{j=1}^{K}e^{x_j}}$\n",
    "\n",
    "\n",
    "What this does is squish each input between 0 and 1 and normalizes the values to give you a proper probability distribution where the probabilites sum up to one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Implement a function softmax that performs the softmax calculation and returns probability distributions for each example in the batch. Note that you'll need to pay attention to the shapes when doing this. If you have a tensor `a` with shape `(32, 10)` and a tensor `b` with shape `(32,)`, doing `a/b` will give you an error because PyTorch will try to do the division across the columns (called broadcasting) but you'll get a size mismatch. The way to think about this is for each of the 32 examples, you only want to divide by one value, the sum in the denominator. So you need b to have a shape of `(32, 1)`. This way PyTorch will divide the 10 values in each row of a by the one value in each row of b. Pay attention to how you take the sum as well. You'll need to define the dim keyword in torch.sum. Setting dim=0 takes the sum across the rows while `dim=1` takes the sum across the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10])\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "## Solution\n",
    "def softmax(x):\n",
    "    # return the softmax\n",
    "    return torch.exp(x)/torch.sum(torch.exp(x), dim=1).view(-1, 1)\n",
    "\n",
    "probabilities = softmax(out)\n",
    "\n",
    "# Does it have the right shape? Should be (64, 10)\n",
    "print(probabilities.shape)\n",
    "# Does it sum to 1?\n",
    "print(probabilities.sum(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Model building\n",
    "\n",
    "PyTorch provides a module `nn` that makes building networks much simpler. Here I'll show you how to build the same one as above with 784 inputs, 256 hidden units,  100 hidden units,, 10 output units and a softmax output.\n",
    "\n",
    "1. Defining components: <br/>\n",
    "This step is done in the constructor, where you will define the layers that will be used accordingly in the next step.\n",
    "2. Network flow: <br/>\n",
    "This step is done in the forward function. Where you will get the input batch as an argument then you will use the defined layers in the previous step to define the flow of the network then you will return the output batch.\n",
    "\n",
    "\n",
    "Pytorch is a dynamic framework, where you can use primitive python keywords with it.\n",
    "You can use if and while statements. Also, it can accepts and returns more than one batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (hidden1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (hidden2): Linear(in_features=256, out_features=100, bias=True)\n",
      "  (output): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden1 = nn.Linear(28*28, 256)        \n",
    "        # Write 2 lines to define 2 more linear layers.\n",
    "        # 1 hidden layers with number of neurons numbers: 250 and 100\n",
    "        # 1 output layer that should output 10 neurons, one for each class.\n",
    "        self.hidden2 = nn.Linear(256, 100) \n",
    "        self.output = nn.Linear(100, 10) \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # the linear layers fc1, fc2, fc3, and fc4\n",
    "        # accepts only flattened input (1D batches)\n",
    "        # while the batch x is of size (batch, 28 * 28)\n",
    "        # define one line to flatten the x to be of size (batch_sz, 28 * 28)\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = self.output(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "model = Net().to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Training loops\n",
    "After that we should define the loops over tha batches and run the training on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "lr = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "criterion = nn.NLLLoss() # negative log loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train( model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test( model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            # Do the same that was done in the previous function.\n",
    "            # But without backprobagating the loss and without running the optimizers\n",
    "            # As this function is only for test.\n",
    "            # write 3 lines to transform the data to the device, get the output and compute the loss\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\rTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.314716\n",
      "Train Epoch: 1 [320/60000 (1%)]\tLoss: 2.294615\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.200132\n",
      "Train Epoch: 1 [960/60000 (2%)]\tLoss: 2.117995\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.047817\n",
      "Train Epoch: 1 [1600/60000 (3%)]\tLoss: 1.972666\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 1.803920\n",
      "Train Epoch: 1 [2240/60000 (4%)]\tLoss: 1.646397\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 1.442107\n",
      "Train Epoch: 1 [2880/60000 (5%)]\tLoss: 1.394910\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 1.128933\n",
      "Train Epoch: 1 [3520/60000 (6%)]\tLoss: 1.220533\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 1.089290\n",
      "Train Epoch: 1 [4160/60000 (7%)]\tLoss: 1.013167\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.820810\n",
      "Train Epoch: 1 [4800/60000 (8%)]\tLoss: 0.626434\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.849811\n",
      "Train Epoch: 1 [5440/60000 (9%)]\tLoss: 0.699650\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.713503\n",
      "Train Epoch: 1 [6080/60000 (10%)]\tLoss: 0.597469\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.626098\n",
      "Train Epoch: 1 [6720/60000 (11%)]\tLoss: 0.692202\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.499127\n",
      "Train Epoch: 1 [7360/60000 (12%)]\tLoss: 0.670500\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.545045\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 0.264014\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.350187\n",
      "Train Epoch: 1 [8640/60000 (14%)]\tLoss: 0.472619\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.415042\n",
      "Train Epoch: 1 [9280/60000 (15%)]\tLoss: 0.403420\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.452328\n",
      "Train Epoch: 1 [9920/60000 (17%)]\tLoss: 0.652192\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.514286\n",
      "Train Epoch: 1 [10560/60000 (18%)]\tLoss: 0.391313\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.311616\n",
      "Train Epoch: 1 [11200/60000 (19%)]\tLoss: 0.357603\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.315414\n",
      "Train Epoch: 1 [11840/60000 (20%)]\tLoss: 0.437110\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.509628\n",
      "Train Epoch: 1 [12480/60000 (21%)]\tLoss: 0.236528\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.595422\n",
      "Train Epoch: 1 [13120/60000 (22%)]\tLoss: 0.507362\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.224542\n",
      "Train Epoch: 1 [13760/60000 (23%)]\tLoss: 0.335499\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.492058\n",
      "Train Epoch: 1 [14400/60000 (24%)]\tLoss: 0.661815\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.244539\n",
      "Train Epoch: 1 [15040/60000 (25%)]\tLoss: 0.329210\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.344126\n",
      "Train Epoch: 1 [15680/60000 (26%)]\tLoss: 0.092692\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.265987\n",
      "Train Epoch: 1 [16320/60000 (27%)]\tLoss: 0.444454\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.159557\n",
      "Train Epoch: 1 [16960/60000 (28%)]\tLoss: 0.565512\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.468222\n",
      "Train Epoch: 1 [17600/60000 (29%)]\tLoss: 0.349769\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.420628\n",
      "Train Epoch: 1 [18240/60000 (30%)]\tLoss: 0.447350\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.259314\n",
      "Train Epoch: 1 [18880/60000 (31%)]\tLoss: 0.499434\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.322636\n",
      "Train Epoch: 1 [19520/60000 (33%)]\tLoss: 0.361557\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.344355\n",
      "Train Epoch: 1 [20160/60000 (34%)]\tLoss: 0.073829\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.500014\n",
      "Train Epoch: 1 [20800/60000 (35%)]\tLoss: 0.313680\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [23], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mSGD(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlr, momentum\u001b[39m=\u001b[39mmomentum)\n\u001b[0;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m----> 7\u001b[0m     train(model, device, train_loader, optimizer, epoch)\n\u001b[0;32m      8\u001b[0m     test(model, device, test_loader)\n\u001b[0;32m     10\u001b[0m torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), \u001b[39m\"\u001b[39m\u001b[39mmnist_model.pt\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn [21], line 3\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m( model, device, train_loader, optimizer, epoch):\n\u001b[0;32m      2\u001b[0m     model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m----> 3\u001b[0m     \u001b[39mfor\u001b[39;00m batch_idx, (data, target) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m      4\u001b[0m         data, target \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device), target\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      5\u001b[0m         optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:652\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    650\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    651\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 652\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    653\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    654\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    655\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    656\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:692\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    691\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 692\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    693\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    694\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\datasets\\mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    142\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img\u001b[39m.\u001b[39mnumpy(), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[0;32m    147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\transforms\\transforms.py:94\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     93\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 94\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     95\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\transforms\\transforms.py:269\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, tensor: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m    262\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[39m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[0;32m    268\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 269\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mnormalize(tensor, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmean, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstd, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\transforms\\functional.py:360\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(tensor, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m    358\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimg should be Tensor Image. Got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(tensor)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 360\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39;49mnormalize(tensor, mean\u001b[39m=\u001b[39;49mmean, std\u001b[39m=\u001b[39;49mstd, inplace\u001b[39m=\u001b[39;49minplace)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\transforms\\functional_tensor.py:953\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    951\u001b[0m mean \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mas_tensor(mean, dtype\u001b[39m=\u001b[39mdtype, device\u001b[39m=\u001b[39mtensor\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    952\u001b[0m std \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mas_tensor(std, dtype\u001b[39m=\u001b[39mdtype, device\u001b[39m=\u001b[39mtensor\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m--> 953\u001b[0m \u001b[39mif\u001b[39;00m (std \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49many():\n\u001b[0;32m    954\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstd evaluated to zero after conversion to \u001b[39m\u001b[39m{\u001b[39;00mdtype\u001b[39m}\u001b[39;00m\u001b[39m, leading to division by zero.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    955\u001b[0m \u001b[39mif\u001b[39;00m mean\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)\n",
    "\n",
    "torch.save(model.state_dict(), \"mnist_model.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question**: How to check if the neural network model is overfitting?v\n",
    "\n",
    "<span style=\"color:blue\"> Track the training and the testing losses during the training, if the loss on test set starts increasing then the model is overfitting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
